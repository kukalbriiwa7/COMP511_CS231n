{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Attentional Networks in Computer Vision\n",
    "Prepared by Comp411 Teaching Unit (TA Can Küçüksözen) in the context of Computer Vision with Deep Learning Course. Do not hesitate to ask in case you have any questions, contact me at: ckucuksozen19@ku.edu.tr\n",
    "\n",
    "Up until this point, we have worked with deep fully-connected networks, convolutional networks and recurrent networks using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are very computationally efficient, on the other hand, most successful image processing methods use convolutional networks. However recent state-of-the-art results on computer vision realm are acquired using Attentional layers and Transformer architectures.\n",
    "\n",
    "First you will implement several layer types that are used in fully attentional networks. You will then use these layers to train an Attentional Image Classification network, specifically a smaller version of Vision Transformer (VIT) on the CIFAR-10 dataset. The original paper can be accessed via the following link: https://arxiv.org/pdf/2010.11929.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. Preparation\n",
    "\n",
    "First, we load the CIFAR-10 dataset. This might take a couple minutes the first time you do it, but the files should stay cached after that.\n",
    "\n",
    "In previous parts of the assignment we had to write our own code to download the CIFAR-10 dataset, preprocess it, and iterate through it in minibatches; PyTorch provides convenient tools to automate this process for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('./comp411/datasets', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./comp411/datasets', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./comp411/datasets', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "You have an option to **use GPU by setting the flag to True below**. It is not necessary to use GPU for this assignment. Note that if your computer does not have CUDA enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode.\n",
    "\n",
    "The global variables `dtype` and `device` will control the data types throughout this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Barebones Transformers: Self-Attentional Layer\n",
    "\n",
    "Here you will complete the implementation of the Pytorch nn.module `SelfAttention`, which will perform the forward pass of a self-attentional layer. Our implementation of the SelfAttentional layer will include three distinct fully connected layers which will be responsible of:\n",
    "\n",
    "1. A fully connected layer, `W_Q`, which will be used to project our input into `queries`\n",
    "2. A fully connected layer, `W_K`, which will be used to project our input into `keys`\n",
    "3. A fully connected layer, `W_V`, which will be used to project our input into `values`\n",
    "\n",
    "After defining such three fully connected layers, and obtain our `queries, keys, and values` variables at the beginning of our forward pass, the following operations should be carried out in order to complete the attentional layer implementation.\n",
    "\n",
    "1. Seperate each of `query, key, and value` projections into their respective heads. In other words, split the feature vector dimension of each matrix into necessarry number of chunks.\n",
    "\n",
    "2. Compute the `Attention Scores` between each pair of sequence elements via conducting a scaled dot product operation between every pair of `queries` and `keys`. Note that `Attention Scores` matrix should have the size of `[# of queries , # of keys]`\n",
    "\n",
    "3. Calculate the `Attention Weights` of each query by applying the non-linear `Softmax` normalization accross the `keys` dimension of the `Attention Scores` matrix.\n",
    "\n",
    "4. Obtain the output combination of `values` by matrix multiplying `Attention Weights` with `values`\n",
    "\n",
    "5. Reassemble heads into one flat vector and return the output.\n",
    "\n",
    "**HINT**: For a more detailed explanation of the self attentional layer, examine the Appendix A of the original ViT manuscript here:  https://arxiv.org/pdf/2010.11929.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dims, head_dims=128, num_heads=2,  bias=False):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        ## initialize module's instance variables\n",
    "        self.input_dims = input_dims\n",
    "        self.head_dims = head_dims\n",
    "        self.num_heads = num_heads\n",
    "        self.proj_dims = head_dims * num_heads\n",
    "        \n",
    "        ## Declare module's parameters\n",
    "        self.W_Q = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        self.W_K = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        self.W_V = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        \n",
    "        self.W_O = nn.Linear(self.proj_dims,self.proj_dims,bias=bias)\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## Input of shape, [B, N, D] where:\n",
    "        ## - B denotes the batch size\n",
    "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
    "        ## - D corresponds to model dimensionality\n",
    "        b,n,d = x.shape\n",
    "        \n",
    "        ## Construct queries,keys,values\n",
    "        q_ = self.W_Q(x)\n",
    "        k_ = self.W_K(x)\n",
    "        v_ = self.W_V(x)\n",
    "        \n",
    "        ## Seperate q,k,v into their corresponding heads,\n",
    "        ## After this operation each q,k,v will have the shape: [B,H,N,D//H] where\n",
    "        ## - B denotes the batch size\n",
    "        ## - H denotes number of heads\n",
    "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
    "        ## - D//H corresponds to per head dimensionality\n",
    "        q, k, v = map(lambda z: torch.reshape(z, (b,n,self.num_heads,self.head_dims)).permute(0,2,1,3), [q_,k_,v_])\n",
    "       \n",
    "        attn_out = None\n",
    "        \n",
    "        #########################################################################################\n",
    "        # TODO: Complete the forward pass of the SelfAttention layer, follow the comments below #\n",
    "        #########################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        ## Compute attention logits. Note that this operation is implemented as a\n",
    "        ## batched matrix multiplication between q and k, the output is scaled by 1/(D//H)^(1/2)\n",
    "        ## inputs are queries and keys that are both of size [B,H,N,D//H]\n",
    "        ## Output Attention logits should have the size: [B,H,N,N]\n",
    "        \n",
    "        intermdeiary = torch.matmul(q,k.transpose(-2,-1)) * (1/((d//self.num_heads)**(1/2)))\n",
    "    \n",
    "        ## Compute attention Weights. Recall that this operation is conducted as a\n",
    "        ## Softmax Normalization across the keys dimension. \n",
    "        ## Hint: You can apply the Softmax operation across the final dimension\n",
    "        \n",
    "        # attn_weights = nn.softmax(intermediary,dim=-1)\n",
    "        attn_weights = intermdeiary.softmax(dim=-1)\n",
    "    \n",
    "        ## Compute attention output values. Bear in mind that this operation is applied as a \n",
    "        ## batched matrix multiplication between the Attention Weights matrix and \n",
    "        ## the values tensor. After computing output values, the output should be reshaped\n",
    "        ## Inputs are Attention Weights with size [B, H, N, N], values with size [B, H, N, D//H]\n",
    "        ## Output should be of size [B, N, D]\n",
    "        ## Hint: you should use torch.matmul, torch.permute, torch.reshape in that order\n",
    "        ##       (or any other equivalent torch operations)\n",
    "        \n",
    "        scores = torch.matmul(attn_weights,v)\n",
    "        \n",
    "        out = scores.reshape(b,n,self.proj_dims)\n",
    "        \n",
    "        ## Compute output feature map. This operation is just passing the concatenated attention \n",
    "        ## output that we have just obtained through a final projection layer W_O.\n",
    "        ## Both the input and the output should be of size [B, N, D]\n",
    "        \n",
    "        attn_out = self.W_O(out)        \n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             \n",
    "        ################################################################################\n",
    "        \n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the forward pass of the Self-Attentional Layer above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 256])\n"
     ]
    }
   ],
   "source": [
    "def test_self_attn_layer():\n",
    "    x = torch.zeros((64, 16, 32), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 32\n",
    "    layer = SelfAttention(32,64,4)\n",
    "    out = layer(x)\n",
    "    print(out.size())  # you should see [64,16,256]\n",
    "test_self_attn_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III. Barebones Transformers: Transformer Encoder Block\n",
    "\n",
    "Here you will complete the implementation of the Pytorch nn.module `TransformerBlock`, which will perform the forward pass of a Transfomer Encoder Block. You can refer to Figure 1 of the original manuscript of ViT from this link: https://arxiv.org/pdf/2010.11929.pdf in order to get yourself familiar with the architecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation of a two layer GELU activated Fully Connected Network is provided for you below:\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(input_dims, hidden_dims, bias=bias)\n",
    "        self.fc_2 = nn.Linear(hidden_dims, output_dims, bias=bias)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        o = F.gelu(self.fc_1(x))\n",
    "        o = self.fc_2(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build from scratch a TransformerBlock Module. Note that the architecture of this\n",
    "## module follows a simple computational pipeline:\n",
    "## input --> layernorm --> SelfAttention --> skip connection \n",
    "##       --> layernorm --> MLP ---> skip connection ---> output\n",
    "## Note that the TransformerBlock module works on a single hidden dimension hidden_dims,\n",
    "## in order to faciliate skip connections with ease. Be careful about the input arguments\n",
    "## to the SelfAttention block.\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_dims, num_heads=4, bias=False):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "###############################################################\n",
    "# TODO: Complete the consturctor of  TransformerBlock module  #\n",
    "###############################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
    "\n",
    "        self.layerNorm_1 = nn.LayerNorm(hidden_dims)\n",
    "        self.selfAttention = SelfAttention(hidden_dims, hidden_dims//num_heads, num_heads, bias=bias)\n",
    "        self.layerNorm_2 = nn.LayerNorm(hidden_dims)\n",
    "        self.fullyConnected = MLP(hidden_dims,hidden_dims,hidden_dims,bias=bias)\n",
    "        \n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "###################################################################\n",
    "#                                 END OF YOUR CODE                #             \n",
    "###################################################################\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "##############################################################\n",
    "# TODO: Complete the forward of TransformerBlock module      #\n",
    "##############################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
    "\n",
    "        normalized_1 = self.layerNorm_1(x)\n",
    "        attn_out = self.selfAttention(normalized_1)\n",
    "        skipped_1 = attn_out + x\n",
    "        normalized_2 = self.layerNorm_2(skipped_1)\n",
    "        fullyConnected_out = self.fullyConnected(normalized_2)\n",
    "        skipped_2 = fullyConnected_out + x\n",
    "        \n",
    "        return skipped_2\n",
    "        \n",
    " # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "###################################################################\n",
    "#                                 END OF YOUR CODE                #             \n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the forward pass of the Transformer Block Layer above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 128])\n"
     ]
    }
   ],
   "source": [
    "def test_transfomerblock_layer():\n",
    "    x = torch.zeros((64, 16, 128), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 128\n",
    "    layer = TransformerBlock(128,4) # hidden dims size 128, heads size 4\n",
    "    out = layer(x)\n",
    "    print(out.size())  # you should see [64,16,128]\n",
    "test_transfomerblock_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV The Vision Transformer (ViT)\n",
    "\n",
    "The final implementation for the Pytorch nn.module `ViT` is given to you below, which will perform the forward pass of the Vision Transformer. Study it and get yourself familiar with the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, hidden_dims, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4, bias=False):\n",
    "        super(ViT, self).__init__()\n",
    "                \n",
    "        ## initialize module's instance variables\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.input_dims = input_dims\n",
    "        self.output_dims = output_dims\n",
    "        self.num_trans_layers = num_trans_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.image_k = image_k\n",
    "        self.patch_k = patch_k\n",
    "        \n",
    "        self.image_height = self.image_width = image_k\n",
    "        self.patch_height = self.patch_width = patch_k\n",
    "        \n",
    "        assert self.image_height % self.patch_height == 0 and self.image_width % self.patch_width == 0,\\\n",
    "                'Image size must be divisible by the patch size.'\n",
    "\n",
    "        self.num_patches = (self.image_height // self.patch_height) * (self.image_width // self.patch_width)\n",
    "        self.patch_flat_len = self.patch_height * self.patch_width\n",
    "        \n",
    "        ## Declare module's parameters\n",
    "        \n",
    "        ## ViT's flattened patch embedding projection:\n",
    "        self.linear_embed = nn.Linear(self.input_dims*self.patch_flat_len, self.hidden_dims)\n",
    "        \n",
    "        ## Learnable positional embeddings, an embedding is learned for each patch location and the class token\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, self.hidden_dims))\n",
    "        \n",
    "        ## Learnable classt token and its index among attention sequence elements.\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,self.hidden_dims))\n",
    "        self.cls_index = torch.LongTensor([0])\n",
    "        \n",
    "        ## Declare cascaded Transformer blocks:\n",
    "        transformer_encoder_list = []\n",
    "        for _ in range(self.num_trans_layers):\n",
    "            transformer_encoder_list.append(TransformerBlock(self.hidden_dims, self.num_heads, bias))\n",
    "        self.transformer_encoder = nn.Sequential(*transformer_encoder_list)\n",
    "        \n",
    "        ## Declare the output mlp:\n",
    "        self.out_mlp = MLP(self.hidden_dims, self.hidden_dims, self.output_dims)\n",
    "         \n",
    "    def unfold(self, x, f = 7, st = 4, p = 0):\n",
    "        ## Create sliding window pathes using nn.Functional.unfold\n",
    "        ## Input dimensions: [B,D,H,W] where\n",
    "        ## --B : input batch size\n",
    "        ## --D : input channels\n",
    "        ## --H, W: input height and width\n",
    "        ## Output dimensions: [B,N,H*W,D]\n",
    "        ## --N : number of patches, decided according to sliding window kernel size (f),\n",
    "        ##      sliding window stride and padding.\n",
    "        b,d,h,w = x.shape\n",
    "        x_unf = F.unfold(x, (f,f), stride=st, padding=p)    \n",
    "        x_unf = torch.reshape(x_unf.permute(0,2,1), (b,-1,d,f*f)).transpose(-1,-2)\n",
    "        n = x_unf.size(1)\n",
    "        return x_unf,n\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b = x.size(0)\n",
    "        ## create sliding window patches from the input image\n",
    "        x_patches,n = self.unfold(x, self.patch_height, self.patch_height, 0)\n",
    "        ## flatten each patch into a 1d vector: i.e. 3x4x4 image patch turned into 1x1x48\n",
    "        x_patch_flat = torch.reshape(x_patches, (b,n,-1))\n",
    "        ## linearly embed each flattened patch\n",
    "        x_embed = self.linear_embed(x_patch_flat)\n",
    "        \n",
    "        ## retrieve class token \n",
    "        cls_tokens = self.cls_token.repeat(b,1,1)\n",
    "        ## concatanate class token to input patches\n",
    "        xcls_embed = torch.cat([cls_tokens, x_embed], dim=-2)\n",
    "        \n",
    "        ## add positional embedding to input patches + class token \n",
    "        xcls_pos_embed = xcls_embed + self.pos_embedding\n",
    "        \n",
    "        ## pass through the transformer encoder\n",
    "        trans_out = self.transformer_encoder(xcls_pos_embed)\n",
    "        \n",
    "        ## select the class token \n",
    "        out_cls_token = torch.index_select(trans_out, -2, self.cls_index.to(trans_out.device))\n",
    "        \n",
    "        ## create output\n",
    "        out = self.out_mlp(out_cls_token)\n",
    "        \n",
    "        return out.squeeze(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the forward pass of the ViT above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "def test_vit():\n",
    "    x = torch.zeros((64, 3, 32, 32), dtype=dtype)  # minibatch size 64, image size 3,32,32\n",
    "    model = ViT(hidden_dims=128, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4)\n",
    "    out = model(x)\n",
    "    print(out.size())  # you should see [64,10]\n",
    "test_vit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V. Train the ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Accuracy\n",
    "Given any minibatch of input data and desired targets, we can check the classification accuracy of a neural network. \n",
    "\n",
    "The check_batch_accuracy function is provided for you below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def check_batch_accuracy(out, target,eps=1e-7):\n",
    "    b, c = out.shape\n",
    "    with torch.no_grad():\n",
    "        _, pred = out.max(-1) \n",
    "        correct = np.sum(np.equal(pred.cpu().numpy(), target.cpu().numpy()))\n",
    "    return correct, np.float(correct) / (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "As we have already seen in the Second Assignment, in our PyTorch based training loops, we use an Optimizer object from the `torch.optim` package, which abstract the notion of an optimization algorithm and provides implementations of most of the algorithms commonly used to optimize neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train(network, optimizer, trainloader):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
    "    \n",
    "    Inputs:\n",
    "    - network: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - trainloader: Iterable DataLoader object that fetches the minibatches\n",
    "    \n",
    "    Returns: overall training accuracy for the epoch\n",
    "    \"\"\"\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    network.train()  # put model to training mode\n",
    "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = Variable(inputs.to(device)), targets.to(device)  # move to device, e.g. GPU\n",
    "            \n",
    "        outputs = network(inputs)\n",
    "        loss =  F.cross_entropy(outputs, targets)\n",
    "            \n",
    "        # Zero out all of the gradients for the variables which the optimizer\n",
    "        # will update.\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # This is the backwards pass: compute the gradient of the loss with\n",
    "        # respect to each  parameter of the model.\n",
    "        loss.backward()\n",
    "            \n",
    "        # Actually update the parameters of the model using the gradients\n",
    "        # computed by the backwards pass.\n",
    "        optimizer.step()\n",
    "            \n",
    "        loss = loss.detach()\n",
    "        train_loss += loss.item()\n",
    "        correct_p, _ = check_batch_accuracy(outputs, targets) \n",
    "        correct += correct_p\n",
    "        total += targets.size(0)\n",
    "\n",
    "        print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "        % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Loop\n",
    "We have also prepared a Evaluation loop in order to determine our networks capabilities in terms of classification accuracy on a given dataset, either the training, or the validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(network, evalloader):\n",
    "    \"\"\"\n",
    "    Evaluate a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
    "    \n",
    "    Inputs:\n",
    "    - network: A PyTorch Module giving the model to train.\n",
    "    - evalloader: Iterable DataLoader object that fetches the minibatches\n",
    "    \n",
    "    Returns: overall evaluation accuracy for the epoch\n",
    "    \"\"\"\n",
    "    network.eval() # put model to evaluation mode\n",
    "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print('\\n---- Evaluation in process ----')\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(evalloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device) # move to device, e.g. GPU\n",
    "            outputs = network(inputs)\n",
    "            loss = F.cross_entropy(outputs, targets)\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "            correct_p, _ = check_batch_accuracy(outputs, targets)\n",
    "            correct += correct_p\n",
    "            total += targets.size(0)\n",
    "            print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (eval_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit a ViT\n",
    "Now we are ready to run the training loop. A nice trick is to train your model with just a few training samples in order to see if your implementation is actually bug free. \n",
    "\n",
    "Simply pass the input size, hidden layer size, and number of classes (i.e. output size) to the constructor of `ViT`. \n",
    "\n",
    "You also need to define an optimizer that tracks all the learnable parameters inside `ViT`. We prefer to use `Adam` optimizer for this part.\n",
    "\n",
    "You should be able to overfit small datasets, which will result in very high training accuracy and comparatively low validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For overfitting experiments, the subset of the dataset that is used has 100 sample images\n",
      "==> Data ready, batchsize = 25\n"
     ]
    }
   ],
   "source": [
    "sample_idx_tr = torch.randperm(len(cifar10_train))[:100]\n",
    "sample_idx_val = torch.randperm(len(cifar10_train))[-100:]\n",
    "\n",
    "trainset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_tr)\n",
    "valset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_val)\n",
    "\n",
    "print(\"For overfitting experiments, the subset of the dataset that is used has {} sample images\".format(len(trainset_sub)))\n",
    "\n",
    "batch_size_sub = 25\n",
    "trainloader_sub = torch.utils.data.DataLoader(trainset_sub, batch_size=batch_size_sub, shuffle=True)\n",
    "valloader_sub = torch.utils.data.DataLoader(valset_sub, batch_size=batch_size_sub, shuffle=False)\n",
    "\n",
    "print('==> Data ready, batchsize = {}'.format(batch_size_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Loss: 3.199 | Acc: 4.000% (1/25)\n",
      "Loss: 3.611 | Acc: 8.000% (4/50)\n",
      "Loss: 3.512 | Acc: 6.667% (5/75)\n",
      "Loss: 3.601 | Acc: 6.000% (6/100)\n",
      "Epoch 0 of training is completed, Training accuracy for this epoch is 6.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.147 | Acc: 4.000% (1/25)\n",
      "Loss: 2.938 | Acc: 6.000% (3/50)\n",
      "Loss: 2.956 | Acc: 5.333% (4/75)\n",
      "Loss: 3.060 | Acc: 6.000% (6/100)\n",
      "Evaluation of Epoch 0 is completed, Validation accuracy for this epoch is 6.0\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 2.659 | Acc: 8.000% (2/25)\n",
      "Loss: 2.564 | Acc: 12.000% (6/50)\n",
      "Loss: 2.683 | Acc: 9.333% (7/75)\n",
      "Loss: 2.564 | Acc: 10.000% (10/100)\n",
      "Epoch 1 of training is completed, Training accuracy for this epoch is 10.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.796 | Acc: 8.000% (2/25)\n",
      "Loss: 2.596 | Acc: 14.000% (7/50)\n",
      "Loss: 2.539 | Acc: 14.667% (11/75)\n",
      "Loss: 2.508 | Acc: 18.000% (18/100)\n",
      "Evaluation of Epoch 1 is completed, Validation accuracy for this epoch is 18.0\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 2.277 | Acc: 20.000% (5/25)\n",
      "Loss: 2.337 | Acc: 20.000% (10/50)\n",
      "Loss: 2.327 | Acc: 26.667% (20/75)\n",
      "Loss: 2.267 | Acc: 28.000% (28/100)\n",
      "Epoch 2 of training is completed, Training accuracy for this epoch is 28.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.174 | Acc: 20.000% (5/25)\n",
      "Loss: 2.211 | Acc: 20.000% (10/50)\n",
      "Loss: 2.302 | Acc: 17.333% (13/75)\n",
      "Loss: 2.317 | Acc: 16.000% (16/100)\n",
      "Evaluation of Epoch 2 is completed, Validation accuracy for this epoch is 16.0\n",
      "\n",
      "Epoch: 3\n",
      "Loss: 2.004 | Acc: 32.000% (8/25)\n",
      "Loss: 1.985 | Acc: 40.000% (20/50)\n",
      "Loss: 2.070 | Acc: 32.000% (24/75)\n",
      "Loss: 2.075 | Acc: 30.000% (30/100)\n",
      "Epoch 3 of training is completed, Training accuracy for this epoch is 30.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.418 | Acc: 12.000% (3/25)\n",
      "Loss: 2.255 | Acc: 16.000% (8/50)\n",
      "Loss: 2.305 | Acc: 17.333% (13/75)\n",
      "Loss: 2.278 | Acc: 21.000% (21/100)\n",
      "Evaluation of Epoch 3 is completed, Validation accuracy for this epoch is 21.0\n",
      "\n",
      "Epoch: 4\n",
      "Loss: 1.910 | Acc: 40.000% (10/25)\n",
      "Loss: 1.774 | Acc: 42.000% (21/50)\n",
      "Loss: 1.785 | Acc: 40.000% (30/75)\n",
      "Loss: 1.797 | Acc: 39.000% (39/100)\n",
      "Epoch 4 of training is completed, Training accuracy for this epoch is 39.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.539 | Acc: 20.000% (5/25)\n",
      "Loss: 2.443 | Acc: 20.000% (10/50)\n",
      "Loss: 2.389 | Acc: 22.667% (17/75)\n",
      "Loss: 2.325 | Acc: 23.000% (23/100)\n",
      "Evaluation of Epoch 4 is completed, Validation accuracy for this epoch is 23.0\n",
      "\n",
      "Epoch: 5\n",
      "Loss: 1.645 | Acc: 48.000% (12/25)\n",
      "Loss: 1.662 | Acc: 50.000% (25/50)\n",
      "Loss: 1.647 | Acc: 46.667% (35/75)\n",
      "Loss: 1.595 | Acc: 47.000% (47/100)\n",
      "Epoch 5 of training is completed, Training accuracy for this epoch is 47.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.398 | Acc: 16.000% (4/25)\n",
      "Loss: 2.297 | Acc: 24.000% (12/50)\n",
      "Loss: 2.282 | Acc: 24.000% (18/75)\n",
      "Loss: 2.266 | Acc: 23.000% (23/100)\n",
      "Evaluation of Epoch 5 is completed, Validation accuracy for this epoch is 23.0\n",
      "\n",
      "Epoch: 6\n",
      "Loss: 1.323 | Acc: 60.000% (15/25)\n",
      "Loss: 1.319 | Acc: 64.000% (32/50)\n",
      "Loss: 1.416 | Acc: 54.667% (41/75)\n",
      "Loss: 1.417 | Acc: 55.000% (55/100)\n",
      "Epoch 6 of training is completed, Training accuracy for this epoch is 55.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.428 | Acc: 12.000% (3/25)\n",
      "Loss: 2.381 | Acc: 18.000% (9/50)\n",
      "Loss: 2.369 | Acc: 18.667% (14/75)\n",
      "Loss: 2.309 | Acc: 22.000% (22/100)\n",
      "Evaluation of Epoch 6 is completed, Validation accuracy for this epoch is 22.0\n",
      "\n",
      "Epoch: 7\n",
      "Loss: 1.263 | Acc: 56.000% (14/25)\n",
      "Loss: 1.125 | Acc: 62.000% (31/50)\n",
      "Loss: 1.194 | Acc: 60.000% (45/75)\n",
      "Loss: 1.236 | Acc: 57.000% (57/100)\n",
      "Epoch 7 of training is completed, Training accuracy for this epoch is 57.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.428 | Acc: 16.000% (4/25)\n",
      "Loss: 2.400 | Acc: 18.000% (9/50)\n",
      "Loss: 2.356 | Acc: 22.667% (17/75)\n",
      "Loss: 2.326 | Acc: 25.000% (25/100)\n",
      "Evaluation of Epoch 7 is completed, Validation accuracy for this epoch is 25.0\n",
      "\n",
      "Epoch: 8\n",
      "Loss: 1.242 | Acc: 52.000% (13/25)\n",
      "Loss: 1.048 | Acc: 62.000% (31/50)\n",
      "Loss: 1.093 | Acc: 61.333% (46/75)\n",
      "Loss: 1.027 | Acc: 64.000% (64/100)\n",
      "Epoch 8 of training is completed, Training accuracy for this epoch is 64.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.627 | Acc: 20.000% (5/25)\n",
      "Loss: 2.541 | Acc: 22.000% (11/50)\n",
      "Loss: 2.509 | Acc: 21.333% (16/75)\n",
      "Loss: 2.493 | Acc: 21.000% (21/100)\n",
      "Evaluation of Epoch 8 is completed, Validation accuracy for this epoch is 21.0\n",
      "\n",
      "Epoch: 9\n",
      "Loss: 0.891 | Acc: 68.000% (17/25)\n",
      "Loss: 0.768 | Acc: 80.000% (40/50)\n",
      "Loss: 0.687 | Acc: 82.667% (62/75)\n",
      "Loss: 0.704 | Acc: 82.000% (82/100)\n",
      "Epoch 9 of training is completed, Training accuracy for this epoch is 82.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.667 | Acc: 12.000% (3/25)\n",
      "Loss: 2.535 | Acc: 18.000% (9/50)\n",
      "Loss: 2.558 | Acc: 20.000% (15/75)\n",
      "Loss: 2.493 | Acc: 22.000% (22/100)\n",
      "Evaluation of Epoch 9 is completed, Validation accuracy for this epoch is 22.0\n",
      "\n",
      "Epoch: 10\n",
      "Loss: 0.452 | Acc: 96.000% (24/25)\n",
      "Loss: 0.481 | Acc: 94.000% (47/50)\n",
      "Loss: 0.468 | Acc: 92.000% (69/75)\n",
      "Loss: 0.505 | Acc: 88.000% (88/100)\n",
      "Epoch 10 of training is completed, Training accuracy for this epoch is 88.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.996 | Acc: 8.000% (2/25)\n",
      "Loss: 2.804 | Acc: 14.000% (7/50)\n",
      "Loss: 2.695 | Acc: 21.333% (16/75)\n",
      "Loss: 2.693 | Acc: 22.000% (22/100)\n",
      "Evaluation of Epoch 10 is completed, Validation accuracy for this epoch is 22.0\n",
      "\n",
      "Epoch: 11\n",
      "Loss: 0.312 | Acc: 100.000% (25/25)\n",
      "Loss: 0.320 | Acc: 98.000% (49/50)\n",
      "Loss: 0.327 | Acc: 97.333% (73/75)\n",
      "Loss: 0.305 | Acc: 97.000% (97/100)\n",
      "Epoch 11 of training is completed, Training accuracy for this epoch is 97.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.359 | Acc: 12.000% (3/25)\n",
      "Loss: 3.128 | Acc: 18.000% (9/50)\n",
      "Loss: 3.036 | Acc: 20.000% (15/75)\n",
      "Loss: 3.032 | Acc: 22.000% (22/100)\n",
      "Evaluation of Epoch 11 is completed, Validation accuracy for this epoch is 22.0\n",
      "\n",
      "Epoch: 12\n",
      "Loss: 0.213 | Acc: 100.000% (25/25)\n",
      "Loss: 0.191 | Acc: 100.000% (50/50)\n",
      "Loss: 0.184 | Acc: 100.000% (75/75)\n",
      "Loss: 0.181 | Acc: 100.000% (100/100)\n",
      "Epoch 12 of training is completed, Training accuracy for this epoch is 100.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.514 | Acc: 12.000% (3/25)\n",
      "Loss: 3.156 | Acc: 18.000% (9/50)\n",
      "Loss: 3.054 | Acc: 22.667% (17/75)\n",
      "Loss: 3.118 | Acc: 22.000% (22/100)\n",
      "Evaluation of Epoch 12 is completed, Validation accuracy for this epoch is 22.0\n",
      "\n",
      "Epoch: 13\n",
      "Loss: 0.121 | Acc: 100.000% (25/25)\n",
      "Loss: 0.110 | Acc: 100.000% (50/50)\n",
      "Loss: 0.104 | Acc: 100.000% (75/75)\n",
      "Loss: 0.098 | Acc: 100.000% (100/100)\n",
      "Epoch 13 of training is completed, Training accuracy for this epoch is 100.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.882 | Acc: 12.000% (3/25)\n",
      "Loss: 3.452 | Acc: 18.000% (9/50)\n",
      "Loss: 3.305 | Acc: 24.000% (18/75)\n",
      "Loss: 3.373 | Acc: 23.000% (23/100)\n",
      "Evaluation of Epoch 13 is completed, Validation accuracy for this epoch is 23.0\n",
      "\n",
      "Epoch: 14\n",
      "Loss: 0.071 | Acc: 100.000% (25/25)\n",
      "Loss: 0.061 | Acc: 100.000% (50/50)\n",
      "Loss: 0.062 | Acc: 100.000% (75/75)\n",
      "Loss: 0.054 | Acc: 100.000% (100/100)\n",
      "Epoch 14 of training is completed, Training accuracy for this epoch is 100.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.904 | Acc: 12.000% (3/25)\n",
      "Loss: 3.546 | Acc: 18.000% (9/50)\n",
      "Loss: 3.364 | Acc: 22.667% (17/75)\n",
      "Loss: 3.458 | Acc: 22.000% (22/100)\n",
      "Evaluation of Epoch 14 is completed, Validation accuracy for this epoch is 22.0\n",
      "\n",
      "Final train set accuracy is 100.0\n",
      "Final val set accuracy is 22.0\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "input_dims = 3\n",
    "hidden_dims = 128\n",
    "output_dims=10\n",
    "num_trans_layers = 4\n",
    "num_heads=4\n",
    "image_k=32\n",
    "patch_k=4\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "model = ViT(hidden_dims, input_dims, output_dims, num_trans_layers, num_heads, image_k, patch_k)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "tr_accs=[]\n",
    "eval_accs=[]\n",
    "for epoch in range(15):\n",
    "    tr_acc = train(model, optimizer, trainloader_sub)\n",
    "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
    "              .format(epoch, tr_acc))  \n",
    "    \n",
    "    eval_acc = evaluate(model, valloader_sub)\n",
    "    print('Evaluation of Epoch {} is completed, Validation accuracy for this epoch is {}'\\\n",
    "              .format(epoch, eval_acc))  \n",
    "    tr_accs.append(tr_acc)\n",
    "    eval_accs.append(eval_acc)\n",
    "    \n",
    "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
    "print(\"Final val set accuracy is {}\".format(eval_accs[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the net\n",
    "By training the four-layer ViT network for three epochs, with untuned hyperparameters that are initialized as below,  you should achieve greater than 50% accuracy both on the training set and the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Loss: 4.245 | Acc: 6.250% (4/64)\n",
      "Loss: 3.621 | Acc: 8.594% (11/128)\n",
      "Loss: 3.599 | Acc: 7.812% (15/192)\n",
      "Loss: 3.415 | Acc: 9.766% (25/256)\n",
      "Loss: 3.330 | Acc: 10.625% (34/320)\n",
      "Loss: 3.258 | Acc: 10.417% (40/384)\n",
      "Loss: 3.155 | Acc: 10.938% (49/448)\n",
      "Loss: 3.053 | Acc: 12.891% (66/512)\n",
      "Loss: 2.980 | Acc: 13.715% (79/576)\n",
      "Loss: 2.922 | Acc: 14.062% (90/640)\n",
      "Loss: 2.877 | Acc: 13.636% (96/704)\n",
      "Loss: 2.821 | Acc: 13.932% (107/768)\n",
      "Loss: 2.768 | Acc: 14.062% (117/832)\n",
      "Loss: 2.746 | Acc: 13.839% (124/896)\n",
      "Loss: 2.716 | Acc: 13.646% (131/960)\n",
      "Loss: 2.681 | Acc: 13.965% (143/1024)\n",
      "Loss: 2.646 | Acc: 14.154% (154/1088)\n",
      "Loss: 2.617 | Acc: 14.410% (166/1152)\n",
      "Loss: 2.588 | Acc: 14.885% (181/1216)\n",
      "Loss: 2.571 | Acc: 14.844% (190/1280)\n",
      "Loss: 2.547 | Acc: 15.030% (202/1344)\n",
      "Loss: 2.541 | Acc: 15.128% (213/1408)\n",
      "Loss: 2.529 | Acc: 15.285% (225/1472)\n",
      "Loss: 2.516 | Acc: 15.560% (239/1536)\n",
      "Loss: 2.502 | Acc: 15.812% (253/1600)\n",
      "Loss: 2.486 | Acc: 15.745% (262/1664)\n",
      "Loss: 2.472 | Acc: 15.914% (275/1728)\n",
      "Loss: 2.463 | Acc: 16.071% (288/1792)\n",
      "Loss: 2.456 | Acc: 16.164% (300/1856)\n",
      "Loss: 2.445 | Acc: 16.406% (315/1920)\n",
      "Loss: 2.434 | Acc: 16.482% (327/1984)\n",
      "Loss: 2.423 | Acc: 16.797% (344/2048)\n",
      "Loss: 2.414 | Acc: 16.951% (358/2112)\n",
      "Loss: 2.401 | Acc: 17.371% (378/2176)\n",
      "Loss: 2.388 | Acc: 17.634% (395/2240)\n",
      "Loss: 2.375 | Acc: 17.839% (411/2304)\n",
      "Loss: 2.367 | Acc: 18.117% (429/2368)\n",
      "Loss: 2.359 | Acc: 18.010% (438/2432)\n",
      "Loss: 2.346 | Acc: 18.309% (457/2496)\n",
      "Loss: 2.342 | Acc: 18.281% (468/2560)\n",
      "Loss: 2.335 | Acc: 18.674% (490/2624)\n",
      "Loss: 2.324 | Acc: 18.973% (510/2688)\n",
      "Loss: 2.318 | Acc: 19.222% (529/2752)\n",
      "Loss: 2.311 | Acc: 19.247% (542/2816)\n",
      "Loss: 2.304 | Acc: 19.340% (557/2880)\n",
      "Loss: 2.300 | Acc: 19.531% (575/2944)\n",
      "Loss: 2.289 | Acc: 19.880% (598/3008)\n",
      "Loss: 2.283 | Acc: 19.954% (613/3072)\n",
      "Loss: 2.274 | Acc: 20.249% (635/3136)\n",
      "Loss: 2.271 | Acc: 20.312% (650/3200)\n",
      "Loss: 2.266 | Acc: 20.466% (668/3264)\n",
      "Loss: 2.260 | Acc: 20.733% (690/3328)\n",
      "Loss: 2.255 | Acc: 20.843% (707/3392)\n",
      "Loss: 2.247 | Acc: 21.036% (727/3456)\n",
      "Loss: 2.243 | Acc: 21.136% (744/3520)\n",
      "Loss: 2.235 | Acc: 21.317% (764/3584)\n",
      "Loss: 2.232 | Acc: 21.299% (777/3648)\n",
      "Loss: 2.227 | Acc: 21.309% (791/3712)\n",
      "Loss: 2.222 | Acc: 21.319% (805/3776)\n",
      "Loss: 2.215 | Acc: 21.562% (828/3840)\n",
      "Loss: 2.211 | Acc: 21.593% (843/3904)\n",
      "Loss: 2.204 | Acc: 21.825% (866/3968)\n",
      "Loss: 2.198 | Acc: 22.024% (888/4032)\n",
      "Loss: 2.193 | Acc: 22.241% (911/4096)\n",
      "Loss: 2.188 | Acc: 22.476% (935/4160)\n",
      "Loss: 2.185 | Acc: 22.562% (953/4224)\n",
      "Loss: 2.181 | Acc: 22.505% (965/4288)\n",
      "Loss: 2.176 | Acc: 22.587% (983/4352)\n",
      "Loss: 2.173 | Acc: 22.736% (1004/4416)\n",
      "Loss: 2.169 | Acc: 22.723% (1018/4480)\n",
      "Loss: 2.165 | Acc: 22.909% (1041/4544)\n",
      "Loss: 2.162 | Acc: 23.003% (1060/4608)\n",
      "Loss: 2.159 | Acc: 23.138% (1081/4672)\n",
      "Loss: 2.155 | Acc: 23.247% (1101/4736)\n",
      "Loss: 2.152 | Acc: 23.417% (1124/4800)\n",
      "Loss: 2.147 | Acc: 23.623% (1149/4864)\n",
      "Loss: 2.143 | Acc: 23.620% (1164/4928)\n",
      "Loss: 2.139 | Acc: 23.698% (1183/4992)\n",
      "Loss: 2.133 | Acc: 23.873% (1207/5056)\n",
      "Loss: 2.130 | Acc: 23.887% (1223/5120)\n",
      "Loss: 2.130 | Acc: 23.920% (1240/5184)\n",
      "Loss: 2.126 | Acc: 23.971% (1258/5248)\n",
      "Loss: 2.122 | Acc: 24.096% (1280/5312)\n",
      "Loss: 2.117 | Acc: 24.200% (1301/5376)\n",
      "Loss: 2.116 | Acc: 24.099% (1311/5440)\n",
      "Loss: 2.112 | Acc: 24.201% (1332/5504)\n",
      "Loss: 2.113 | Acc: 24.138% (1344/5568)\n",
      "Loss: 2.111 | Acc: 24.165% (1361/5632)\n",
      "Loss: 2.110 | Acc: 24.105% (1373/5696)\n",
      "Loss: 2.110 | Acc: 24.080% (1387/5760)\n",
      "Loss: 2.108 | Acc: 24.124% (1405/5824)\n",
      "Loss: 2.105 | Acc: 24.151% (1422/5888)\n",
      "Loss: 2.102 | Acc: 24.294% (1446/5952)\n",
      "Loss: 2.101 | Acc: 24.335% (1464/6016)\n",
      "Loss: 2.098 | Acc: 24.424% (1485/6080)\n",
      "Loss: 2.095 | Acc: 24.512% (1506/6144)\n",
      "Loss: 2.092 | Acc: 24.565% (1525/6208)\n",
      "Loss: 2.087 | Acc: 24.713% (1550/6272)\n",
      "Loss: 2.086 | Acc: 24.716% (1566/6336)\n",
      "Loss: 2.082 | Acc: 24.781% (1586/6400)\n",
      "Loss: 2.079 | Acc: 24.907% (1610/6464)\n",
      "Loss: 2.078 | Acc: 25.000% (1632/6528)\n",
      "Loss: 2.077 | Acc: 25.015% (1649/6592)\n",
      "Loss: 2.075 | Acc: 25.015% (1665/6656)\n",
      "Loss: 2.074 | Acc: 25.045% (1683/6720)\n",
      "Loss: 2.070 | Acc: 25.162% (1707/6784)\n",
      "Loss: 2.068 | Acc: 25.248% (1729/6848)\n",
      "Loss: 2.067 | Acc: 25.275% (1747/6912)\n",
      "Loss: 2.066 | Acc: 25.258% (1762/6976)\n",
      "Loss: 2.062 | Acc: 25.412% (1789/7040)\n",
      "Loss: 2.060 | Acc: 25.479% (1810/7104)\n",
      "Loss: 2.058 | Acc: 25.530% (1830/7168)\n",
      "Loss: 2.055 | Acc: 25.636% (1854/7232)\n",
      "Loss: 2.051 | Acc: 25.754% (1879/7296)\n",
      "Loss: 2.050 | Acc: 25.761% (1896/7360)\n",
      "Loss: 2.048 | Acc: 25.822% (1917/7424)\n",
      "Loss: 2.045 | Acc: 25.948% (1943/7488)\n",
      "Loss: 2.042 | Acc: 26.033% (1966/7552)\n",
      "Loss: 2.040 | Acc: 26.037% (1983/7616)\n",
      "Loss: 2.036 | Acc: 26.185% (2011/7680)\n",
      "Loss: 2.034 | Acc: 26.227% (2031/7744)\n",
      "Loss: 2.032 | Acc: 26.306% (2054/7808)\n",
      "Loss: 2.029 | Acc: 26.385% (2077/7872)\n",
      "Loss: 2.027 | Acc: 26.499% (2103/7936)\n",
      "Loss: 2.025 | Acc: 26.562% (2125/8000)\n",
      "Loss: 2.024 | Acc: 26.587% (2144/8064)\n",
      "Loss: 2.021 | Acc: 26.575% (2160/8128)\n",
      "Loss: 2.020 | Acc: 26.611% (2180/8192)\n",
      "Loss: 2.018 | Acc: 26.672% (2202/8256)\n",
      "Loss: 2.015 | Acc: 26.731% (2224/8320)\n",
      "Loss: 2.013 | Acc: 26.801% (2247/8384)\n",
      "Loss: 2.011 | Acc: 26.870% (2270/8448)\n",
      "Loss: 2.009 | Acc: 27.009% (2299/8512)\n",
      "Loss: 2.007 | Acc: 27.134% (2327/8576)\n",
      "Loss: 2.004 | Acc: 27.211% (2351/8640)\n",
      "Loss: 2.002 | Acc: 27.309% (2377/8704)\n",
      "Loss: 2.001 | Acc: 27.349% (2398/8768)\n",
      "Loss: 2.002 | Acc: 27.264% (2408/8832)\n",
      "Loss: 1.999 | Acc: 27.316% (2430/8896)\n",
      "Loss: 1.997 | Acc: 27.388% (2454/8960)\n",
      "Loss: 1.995 | Acc: 27.405% (2473/9024)\n",
      "Loss: 1.994 | Acc: 27.443% (2494/9088)\n",
      "Loss: 1.993 | Acc: 27.480% (2515/9152)\n",
      "Loss: 1.992 | Acc: 27.550% (2539/9216)\n",
      "Loss: 1.988 | Acc: 27.705% (2571/9280)\n",
      "Loss: 1.987 | Acc: 27.750% (2593/9344)\n",
      "Loss: 1.985 | Acc: 27.817% (2617/9408)\n",
      "Loss: 1.984 | Acc: 27.798% (2633/9472)\n",
      "Loss: 1.981 | Acc: 27.905% (2661/9536)\n",
      "Loss: 1.980 | Acc: 27.990% (2687/9600)\n",
      "Loss: 1.980 | Acc: 28.001% (2706/9664)\n",
      "Loss: 1.977 | Acc: 28.074% (2731/9728)\n",
      "Loss: 1.977 | Acc: 28.074% (2749/9792)\n",
      "Loss: 1.977 | Acc: 28.105% (2770/9856)\n",
      "Loss: 1.975 | Acc: 28.135% (2791/9920)\n",
      "Loss: 1.974 | Acc: 28.185% (2814/9984)\n",
      "Loss: 1.973 | Acc: 28.225% (2836/10048)\n",
      "Loss: 1.972 | Acc: 28.244% (2856/10112)\n",
      "Loss: 1.971 | Acc: 28.272% (2877/10176)\n",
      "Loss: 1.970 | Acc: 28.271% (2895/10240)\n",
      "Loss: 1.970 | Acc: 28.290% (2915/10304)\n",
      "Loss: 1.969 | Acc: 28.337% (2938/10368)\n",
      "Loss: 1.968 | Acc: 28.393% (2962/10432)\n",
      "Loss: 1.967 | Acc: 28.439% (2985/10496)\n",
      "Loss: 1.964 | Acc: 28.580% (3018/10560)\n",
      "Loss: 1.962 | Acc: 28.643% (3043/10624)\n",
      "Loss: 1.960 | Acc: 28.686% (3066/10688)\n",
      "Loss: 1.958 | Acc: 28.711% (3087/10752)\n",
      "Loss: 1.957 | Acc: 28.735% (3108/10816)\n",
      "Loss: 1.956 | Acc: 28.750% (3128/10880)\n",
      "Loss: 1.954 | Acc: 28.810% (3153/10944)\n",
      "Loss: 1.953 | Acc: 28.870% (3178/11008)\n",
      "Loss: 1.951 | Acc: 28.938% (3204/11072)\n",
      "Loss: 1.950 | Acc: 28.951% (3224/11136)\n",
      "Loss: 1.950 | Acc: 28.955% (3243/11200)\n",
      "Loss: 1.948 | Acc: 29.004% (3267/11264)\n",
      "Loss: 1.948 | Acc: 29.008% (3286/11328)\n",
      "Loss: 1.948 | Acc: 29.020% (3306/11392)\n",
      "Loss: 1.947 | Acc: 29.076% (3331/11456)\n",
      "Loss: 1.946 | Acc: 29.141% (3357/11520)\n",
      "Loss: 1.945 | Acc: 29.195% (3382/11584)\n",
      "Loss: 1.944 | Acc: 29.241% (3406/11648)\n",
      "Loss: 1.943 | Acc: 29.261% (3427/11712)\n",
      "Loss: 1.942 | Acc: 29.322% (3453/11776)\n",
      "Loss: 1.940 | Acc: 29.392% (3480/11840)\n",
      "Loss: 1.939 | Acc: 29.435% (3504/11904)\n",
      "Loss: 1.938 | Acc: 29.462% (3526/11968)\n",
      "Loss: 1.936 | Acc: 29.463% (3545/12032)\n",
      "Loss: 1.934 | Acc: 29.506% (3569/12096)\n",
      "Loss: 1.932 | Acc: 29.572% (3596/12160)\n",
      "Loss: 1.931 | Acc: 29.638% (3623/12224)\n",
      "Loss: 1.928 | Acc: 29.720% (3652/12288)\n",
      "Loss: 1.926 | Acc: 29.801% (3681/12352)\n",
      "Loss: 1.926 | Acc: 29.841% (3705/12416)\n",
      "Loss: 1.923 | Acc: 29.928% (3735/12480)\n",
      "Loss: 1.922 | Acc: 29.951% (3757/12544)\n",
      "Loss: 1.919 | Acc: 30.005% (3783/12608)\n",
      "Loss: 1.917 | Acc: 30.082% (3812/12672)\n",
      "Loss: 1.915 | Acc: 30.151% (3840/12736)\n",
      "Loss: 1.914 | Acc: 30.180% (3863/12800)\n",
      "Loss: 1.914 | Acc: 30.208% (3886/12864)\n",
      "Loss: 1.913 | Acc: 30.237% (3909/12928)\n",
      "Loss: 1.912 | Acc: 30.272% (3933/12992)\n",
      "Loss: 1.911 | Acc: 30.323% (3959/13056)\n",
      "Loss: 1.910 | Acc: 30.351% (3982/13120)\n",
      "Loss: 1.909 | Acc: 30.431% (4012/13184)\n",
      "Loss: 1.908 | Acc: 30.457% (4035/13248)\n",
      "Loss: 1.906 | Acc: 30.484% (4058/13312)\n",
      "Loss: 1.905 | Acc: 30.570% (4089/13376)\n",
      "Loss: 1.903 | Acc: 30.647% (4119/13440)\n",
      "Loss: 1.903 | Acc: 30.650% (4139/13504)\n",
      "Loss: 1.902 | Acc: 30.638% (4157/13568)\n",
      "Loss: 1.901 | Acc: 30.707% (4186/13632)\n",
      "Loss: 1.900 | Acc: 30.680% (4202/13696)\n",
      "Loss: 1.899 | Acc: 30.749% (4231/13760)\n",
      "Loss: 1.898 | Acc: 30.773% (4254/13824)\n",
      "Loss: 1.897 | Acc: 30.811% (4279/13888)\n",
      "Loss: 1.896 | Acc: 30.849% (4304/13952)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.896 | Acc: 30.886% (4329/14016)\n",
      "Loss: 1.895 | Acc: 30.966% (4360/14080)\n",
      "Loss: 1.894 | Acc: 31.003% (4385/14144)\n",
      "Loss: 1.893 | Acc: 31.053% (4412/14208)\n",
      "Loss: 1.892 | Acc: 31.075% (4435/14272)\n",
      "Loss: 1.891 | Acc: 31.124% (4462/14336)\n",
      "Loss: 1.890 | Acc: 31.160% (4487/14400)\n",
      "Loss: 1.889 | Acc: 31.188% (4511/14464)\n",
      "Loss: 1.889 | Acc: 31.209% (4534/14528)\n",
      "Loss: 1.888 | Acc: 31.223% (4556/14592)\n",
      "Loss: 1.887 | Acc: 31.277% (4584/14656)\n",
      "Loss: 1.887 | Acc: 31.298% (4607/14720)\n",
      "Loss: 1.885 | Acc: 31.399% (4642/14784)\n",
      "Loss: 1.884 | Acc: 31.466% (4672/14848)\n",
      "Loss: 1.883 | Acc: 31.532% (4702/14912)\n",
      "Loss: 1.882 | Acc: 31.557% (4726/14976)\n",
      "Loss: 1.881 | Acc: 31.543% (4744/15040)\n",
      "Loss: 1.880 | Acc: 31.574% (4769/15104)\n",
      "Loss: 1.879 | Acc: 31.606% (4794/15168)\n",
      "Loss: 1.879 | Acc: 31.611% (4815/15232)\n",
      "Loss: 1.878 | Acc: 31.616% (4836/15296)\n",
      "Loss: 1.876 | Acc: 31.686% (4867/15360)\n",
      "Loss: 1.875 | Acc: 31.723% (4893/15424)\n",
      "Loss: 1.874 | Acc: 31.754% (4918/15488)\n",
      "Loss: 1.872 | Acc: 31.790% (4944/15552)\n",
      "Loss: 1.871 | Acc: 31.884% (4979/15616)\n",
      "Loss: 1.870 | Acc: 31.901% (5002/15680)\n",
      "Loss: 1.869 | Acc: 31.955% (5031/15744)\n",
      "Loss: 1.869 | Acc: 31.959% (5052/15808)\n",
      "Loss: 1.868 | Acc: 31.993% (5078/15872)\n",
      "Loss: 1.867 | Acc: 32.003% (5100/15936)\n",
      "Loss: 1.866 | Acc: 32.031% (5125/16000)\n",
      "Loss: 1.866 | Acc: 32.078% (5153/16064)\n",
      "Loss: 1.865 | Acc: 32.118% (5180/16128)\n",
      "Loss: 1.864 | Acc: 32.102% (5198/16192)\n",
      "Loss: 1.863 | Acc: 32.136% (5224/16256)\n",
      "Loss: 1.862 | Acc: 32.212% (5257/16320)\n",
      "Loss: 1.861 | Acc: 32.214% (5278/16384)\n",
      "Loss: 1.860 | Acc: 32.241% (5303/16448)\n",
      "Loss: 1.859 | Acc: 32.310% (5335/16512)\n",
      "Loss: 1.859 | Acc: 32.324% (5358/16576)\n",
      "Loss: 1.857 | Acc: 32.368% (5386/16640)\n",
      "Loss: 1.856 | Acc: 32.411% (5414/16704)\n",
      "Loss: 1.856 | Acc: 32.443% (5440/16768)\n",
      "Loss: 1.856 | Acc: 32.444% (5461/16832)\n",
      "Loss: 1.854 | Acc: 32.505% (5492/16896)\n",
      "Loss: 1.853 | Acc: 32.524% (5516/16960)\n",
      "Loss: 1.852 | Acc: 32.554% (5542/17024)\n",
      "Loss: 1.851 | Acc: 32.596% (5570/17088)\n",
      "Loss: 1.850 | Acc: 32.643% (5599/17152)\n",
      "Loss: 1.849 | Acc: 32.679% (5626/17216)\n",
      "Loss: 1.848 | Acc: 32.731% (5656/17280)\n",
      "Loss: 1.847 | Acc: 32.778% (5685/17344)\n",
      "Loss: 1.847 | Acc: 32.812% (5712/17408)\n",
      "Loss: 1.845 | Acc: 32.870% (5743/17472)\n",
      "Loss: 1.844 | Acc: 32.927% (5774/17536)\n",
      "Loss: 1.844 | Acc: 32.926% (5795/17600)\n",
      "Loss: 1.843 | Acc: 32.943% (5819/17664)\n",
      "Loss: 1.842 | Acc: 32.993% (5849/17728)\n",
      "Loss: 1.841 | Acc: 33.026% (5876/17792)\n",
      "Loss: 1.841 | Acc: 33.020% (5896/17856)\n",
      "Loss: 1.839 | Acc: 33.052% (5923/17920)\n",
      "Loss: 1.838 | Acc: 33.107% (5954/17984)\n",
      "Loss: 1.836 | Acc: 33.162% (5985/18048)\n",
      "Loss: 1.836 | Acc: 33.182% (6010/18112)\n",
      "Loss: 1.835 | Acc: 33.231% (6040/18176)\n",
      "Loss: 1.834 | Acc: 33.262% (6067/18240)\n",
      "Loss: 1.833 | Acc: 33.277% (6091/18304)\n",
      "Loss: 1.832 | Acc: 33.319% (6120/18368)\n",
      "Loss: 1.831 | Acc: 33.350% (6147/18432)\n",
      "Loss: 1.830 | Acc: 33.369% (6172/18496)\n",
      "Loss: 1.830 | Acc: 33.378% (6195/18560)\n",
      "Loss: 1.829 | Acc: 33.419% (6224/18624)\n",
      "Loss: 1.829 | Acc: 33.449% (6251/18688)\n",
      "Loss: 1.828 | Acc: 33.484% (6279/18752)\n",
      "Loss: 1.827 | Acc: 33.493% (6302/18816)\n",
      "Loss: 1.827 | Acc: 33.512% (6327/18880)\n",
      "Loss: 1.826 | Acc: 33.536% (6353/18944)\n",
      "Loss: 1.825 | Acc: 33.581% (6383/19008)\n",
      "Loss: 1.825 | Acc: 33.636% (6415/19072)\n",
      "Loss: 1.824 | Acc: 33.670% (6443/19136)\n",
      "Loss: 1.823 | Acc: 33.693% (6469/19200)\n",
      "Loss: 1.822 | Acc: 33.716% (6495/19264)\n",
      "Loss: 1.822 | Acc: 33.749% (6523/19328)\n",
      "Loss: 1.820 | Acc: 33.787% (6552/19392)\n",
      "Loss: 1.819 | Acc: 33.815% (6579/19456)\n",
      "Loss: 1.819 | Acc: 33.863% (6610/19520)\n",
      "Loss: 1.818 | Acc: 33.885% (6636/19584)\n",
      "Loss: 1.817 | Acc: 33.907% (6662/19648)\n",
      "Loss: 1.817 | Acc: 33.934% (6689/19712)\n",
      "Loss: 1.816 | Acc: 33.940% (6712/19776)\n",
      "Loss: 1.815 | Acc: 33.957% (6737/19840)\n",
      "Loss: 1.814 | Acc: 34.018% (6771/19904)\n",
      "Loss: 1.814 | Acc: 34.034% (6796/19968)\n",
      "Loss: 1.813 | Acc: 34.031% (6817/20032)\n",
      "Loss: 1.813 | Acc: 34.057% (6844/20096)\n",
      "Loss: 1.812 | Acc: 34.097% (6874/20160)\n",
      "Loss: 1.811 | Acc: 34.133% (6903/20224)\n",
      "Loss: 1.810 | Acc: 34.178% (6934/20288)\n",
      "Loss: 1.809 | Acc: 34.203% (6961/20352)\n",
      "Loss: 1.809 | Acc: 34.228% (6988/20416)\n",
      "Loss: 1.808 | Acc: 34.258% (7016/20480)\n",
      "Loss: 1.807 | Acc: 34.292% (7045/20544)\n",
      "Loss: 1.807 | Acc: 34.312% (7071/20608)\n",
      "Loss: 1.806 | Acc: 34.375% (7106/20672)\n",
      "Loss: 1.805 | Acc: 34.409% (7135/20736)\n",
      "Loss: 1.805 | Acc: 34.433% (7162/20800)\n",
      "Loss: 1.805 | Acc: 34.433% (7184/20864)\n",
      "Loss: 1.804 | Acc: 34.461% (7212/20928)\n",
      "Loss: 1.803 | Acc: 34.480% (7238/20992)\n",
      "Loss: 1.802 | Acc: 34.508% (7266/21056)\n",
      "Loss: 1.801 | Acc: 34.555% (7298/21120)\n",
      "Loss: 1.801 | Acc: 34.517% (7312/21184)\n",
      "Loss: 1.801 | Acc: 34.540% (7339/21248)\n",
      "Loss: 1.800 | Acc: 34.605% (7375/21312)\n",
      "Loss: 1.799 | Acc: 34.623% (7401/21376)\n",
      "Loss: 1.798 | Acc: 34.660% (7431/21440)\n",
      "Loss: 1.798 | Acc: 34.663% (7454/21504)\n",
      "Loss: 1.797 | Acc: 34.686% (7481/21568)\n",
      "Loss: 1.797 | Acc: 34.680% (7502/21632)\n",
      "Loss: 1.796 | Acc: 34.735% (7536/21696)\n",
      "Loss: 1.796 | Acc: 34.747% (7561/21760)\n",
      "Loss: 1.795 | Acc: 34.792% (7593/21824)\n",
      "Loss: 1.795 | Acc: 34.791% (7615/21888)\n",
      "Loss: 1.794 | Acc: 34.844% (7649/21952)\n",
      "Loss: 1.793 | Acc: 34.875% (7678/22016)\n",
      "Loss: 1.792 | Acc: 34.905% (7707/22080)\n",
      "Loss: 1.791 | Acc: 34.903% (7729/22144)\n",
      "Loss: 1.791 | Acc: 34.911% (7753/22208)\n",
      "Loss: 1.791 | Acc: 34.909% (7775/22272)\n",
      "Loss: 1.791 | Acc: 34.917% (7799/22336)\n",
      "Loss: 1.790 | Acc: 34.960% (7831/22400)\n",
      "Loss: 1.790 | Acc: 34.980% (7858/22464)\n",
      "Loss: 1.789 | Acc: 35.028% (7891/22528)\n",
      "Loss: 1.788 | Acc: 35.061% (7921/22592)\n",
      "Loss: 1.788 | Acc: 35.072% (7946/22656)\n",
      "Loss: 1.787 | Acc: 35.079% (7970/22720)\n",
      "Loss: 1.788 | Acc: 35.064% (7989/22784)\n",
      "Loss: 1.787 | Acc: 35.067% (8012/22848)\n",
      "Loss: 1.787 | Acc: 35.113% (8045/22912)\n",
      "Loss: 1.786 | Acc: 35.141% (8074/22976)\n",
      "Loss: 1.785 | Acc: 35.143% (8097/23040)\n",
      "Loss: 1.785 | Acc: 35.145% (8120/23104)\n",
      "Loss: 1.784 | Acc: 35.191% (8153/23168)\n",
      "Loss: 1.784 | Acc: 35.223% (8183/23232)\n",
      "Loss: 1.783 | Acc: 35.268% (8216/23296)\n",
      "Loss: 1.782 | Acc: 35.278% (8241/23360)\n",
      "Loss: 1.782 | Acc: 35.289% (8266/23424)\n",
      "Loss: 1.781 | Acc: 35.337% (8300/23488)\n",
      "Loss: 1.780 | Acc: 35.356% (8327/23552)\n",
      "Loss: 1.780 | Acc: 35.383% (8356/23616)\n",
      "Loss: 1.780 | Acc: 35.393% (8381/23680)\n",
      "Loss: 1.779 | Acc: 35.407% (8407/23744)\n",
      "Loss: 1.779 | Acc: 35.433% (8436/23808)\n",
      "Loss: 1.778 | Acc: 35.426% (8457/23872)\n",
      "Loss: 1.778 | Acc: 35.424% (8479/23936)\n",
      "Loss: 1.778 | Acc: 35.438% (8505/24000)\n",
      "Loss: 1.777 | Acc: 35.468% (8535/24064)\n",
      "Loss: 1.776 | Acc: 35.490% (8563/24128)\n",
      "Loss: 1.776 | Acc: 35.512% (8591/24192)\n",
      "Loss: 1.775 | Acc: 35.517% (8615/24256)\n",
      "Loss: 1.775 | Acc: 35.535% (8642/24320)\n",
      "Loss: 1.774 | Acc: 35.560% (8671/24384)\n",
      "Loss: 1.774 | Acc: 35.573% (8697/24448)\n",
      "Loss: 1.773 | Acc: 35.603% (8727/24512)\n",
      "Loss: 1.773 | Acc: 35.620% (8754/24576)\n",
      "Loss: 1.772 | Acc: 35.641% (8782/24640)\n",
      "Loss: 1.771 | Acc: 35.646% (8806/24704)\n",
      "Loss: 1.771 | Acc: 35.651% (8830/24768)\n",
      "Loss: 1.770 | Acc: 35.672% (8858/24832)\n",
      "Loss: 1.770 | Acc: 35.717% (8892/24896)\n",
      "Loss: 1.769 | Acc: 35.733% (8919/24960)\n",
      "Loss: 1.769 | Acc: 35.750% (8946/25024)\n",
      "Loss: 1.769 | Acc: 35.766% (8973/25088)\n",
      "Loss: 1.769 | Acc: 35.755% (8993/25152)\n",
      "Loss: 1.768 | Acc: 35.767% (9019/25216)\n",
      "Loss: 1.768 | Acc: 35.791% (9048/25280)\n",
      "Loss: 1.767 | Acc: 35.815% (9077/25344)\n",
      "Loss: 1.767 | Acc: 35.823% (9102/25408)\n",
      "Loss: 1.767 | Acc: 35.847% (9131/25472)\n",
      "Loss: 1.766 | Acc: 35.871% (9160/25536)\n",
      "Loss: 1.765 | Acc: 35.906% (9192/25600)\n",
      "Loss: 1.764 | Acc: 35.910% (9216/25664)\n",
      "Loss: 1.764 | Acc: 35.938% (9246/25728)\n",
      "Loss: 1.764 | Acc: 35.945% (9271/25792)\n",
      "Loss: 1.763 | Acc: 35.957% (9297/25856)\n",
      "Loss: 1.763 | Acc: 35.980% (9326/25920)\n",
      "Loss: 1.762 | Acc: 35.984% (9350/25984)\n",
      "Loss: 1.761 | Acc: 36.010% (9380/26048)\n",
      "Loss: 1.761 | Acc: 36.033% (9409/26112)\n",
      "Loss: 1.760 | Acc: 36.048% (9436/26176)\n",
      "Loss: 1.760 | Acc: 36.086% (9469/26240)\n",
      "Loss: 1.759 | Acc: 36.112% (9499/26304)\n",
      "Loss: 1.759 | Acc: 36.120% (9524/26368)\n",
      "Loss: 1.759 | Acc: 36.119% (9547/26432)\n",
      "Loss: 1.758 | Acc: 36.149% (9578/26496)\n",
      "Loss: 1.757 | Acc: 36.197% (9614/26560)\n",
      "Loss: 1.756 | Acc: 36.223% (9644/26624)\n",
      "Loss: 1.755 | Acc: 36.260% (9677/26688)\n",
      "Loss: 1.754 | Acc: 36.281% (9706/26752)\n",
      "Loss: 1.754 | Acc: 36.322% (9740/26816)\n",
      "Loss: 1.753 | Acc: 36.313% (9761/26880)\n",
      "Loss: 1.754 | Acc: 36.323% (9787/26944)\n",
      "Loss: 1.753 | Acc: 36.345% (9816/27008)\n",
      "Loss: 1.752 | Acc: 36.366% (9845/27072)\n",
      "Loss: 1.751 | Acc: 36.387% (9874/27136)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.750 | Acc: 36.419% (9906/27200)\n",
      "Loss: 1.750 | Acc: 36.447% (9937/27264)\n",
      "Loss: 1.749 | Acc: 36.472% (9967/27328)\n",
      "Loss: 1.749 | Acc: 36.492% (9996/27392)\n",
      "Loss: 1.749 | Acc: 36.517% (10026/27456)\n",
      "Loss: 1.747 | Acc: 36.552% (10059/27520)\n",
      "Loss: 1.747 | Acc: 36.561% (10085/27584)\n",
      "Loss: 1.747 | Acc: 36.574% (10112/27648)\n",
      "Loss: 1.746 | Acc: 36.580% (10137/27712)\n",
      "Loss: 1.745 | Acc: 36.611% (10169/27776)\n",
      "Loss: 1.745 | Acc: 36.627% (10197/27840)\n",
      "Loss: 1.745 | Acc: 36.622% (10219/27904)\n",
      "Loss: 1.745 | Acc: 36.624% (10243/27968)\n",
      "Loss: 1.744 | Acc: 36.644% (10272/28032)\n",
      "Loss: 1.744 | Acc: 36.671% (10303/28096)\n",
      "Loss: 1.743 | Acc: 36.694% (10333/28160)\n",
      "Loss: 1.743 | Acc: 36.678% (10352/28224)\n",
      "Loss: 1.742 | Acc: 36.694% (10380/28288)\n",
      "Loss: 1.742 | Acc: 36.696% (10404/28352)\n",
      "Loss: 1.742 | Acc: 36.701% (10429/28416)\n",
      "Loss: 1.742 | Acc: 36.706% (10454/28480)\n",
      "Loss: 1.742 | Acc: 36.705% (10477/28544)\n",
      "Loss: 1.742 | Acc: 36.682% (10494/28608)\n",
      "Loss: 1.741 | Acc: 36.698% (10522/28672)\n",
      "Loss: 1.741 | Acc: 36.714% (10550/28736)\n",
      "Loss: 1.740 | Acc: 36.708% (10572/28800)\n",
      "Loss: 1.740 | Acc: 36.738% (10604/28864)\n",
      "Loss: 1.740 | Acc: 36.760% (10634/28928)\n",
      "Loss: 1.739 | Acc: 36.783% (10664/28992)\n",
      "Loss: 1.739 | Acc: 36.784% (10688/29056)\n",
      "Loss: 1.739 | Acc: 36.789% (10713/29120)\n",
      "Loss: 1.738 | Acc: 36.808% (10742/29184)\n",
      "Loss: 1.738 | Acc: 36.806% (10765/29248)\n",
      "Loss: 1.737 | Acc: 36.821% (10793/29312)\n",
      "Loss: 1.737 | Acc: 36.846% (10824/29376)\n",
      "Loss: 1.736 | Acc: 36.844% (10847/29440)\n",
      "Loss: 1.736 | Acc: 36.859% (10875/29504)\n",
      "Loss: 1.735 | Acc: 36.851% (10896/29568)\n",
      "Loss: 1.735 | Acc: 36.876% (10927/29632)\n",
      "Loss: 1.735 | Acc: 36.864% (10947/29696)\n",
      "Loss: 1.734 | Acc: 36.878% (10975/29760)\n",
      "Loss: 1.734 | Acc: 36.873% (10997/29824)\n",
      "Loss: 1.734 | Acc: 36.894% (11027/29888)\n",
      "Loss: 1.733 | Acc: 36.942% (11065/29952)\n",
      "Loss: 1.732 | Acc: 36.974% (11098/30016)\n",
      "Loss: 1.732 | Acc: 36.978% (11123/30080)\n",
      "Loss: 1.732 | Acc: 37.006% (11155/30144)\n",
      "Loss: 1.732 | Acc: 37.033% (11187/30208)\n",
      "Loss: 1.731 | Acc: 37.051% (11216/30272)\n",
      "Loss: 1.730 | Acc: 37.058% (11242/30336)\n",
      "Loss: 1.730 | Acc: 37.072% (11270/30400)\n",
      "Loss: 1.730 | Acc: 37.063% (11291/30464)\n",
      "Loss: 1.729 | Acc: 37.084% (11321/30528)\n",
      "Loss: 1.729 | Acc: 37.078% (11343/30592)\n",
      "Loss: 1.729 | Acc: 37.086% (11369/30656)\n",
      "Loss: 1.729 | Acc: 37.103% (11398/30720)\n",
      "Loss: 1.728 | Acc: 37.100% (11421/30784)\n",
      "Loss: 1.728 | Acc: 37.114% (11449/30848)\n",
      "Loss: 1.728 | Acc: 37.128% (11477/30912)\n",
      "Loss: 1.727 | Acc: 37.151% (11508/30976)\n",
      "Loss: 1.727 | Acc: 37.165% (11536/31040)\n",
      "Loss: 1.727 | Acc: 37.182% (11565/31104)\n",
      "Loss: 1.727 | Acc: 37.179% (11588/31168)\n",
      "Loss: 1.726 | Acc: 37.231% (11628/31232)\n",
      "Loss: 1.725 | Acc: 37.264% (11662/31296)\n",
      "Loss: 1.724 | Acc: 37.283% (11692/31360)\n",
      "Loss: 1.724 | Acc: 37.303% (11722/31424)\n",
      "Loss: 1.724 | Acc: 37.306% (11747/31488)\n",
      "Loss: 1.724 | Acc: 37.329% (11778/31552)\n",
      "Loss: 1.724 | Acc: 37.348% (11808/31616)\n",
      "Loss: 1.724 | Acc: 37.371% (11839/31680)\n",
      "Loss: 1.723 | Acc: 37.405% (11874/31744)\n",
      "Loss: 1.723 | Acc: 37.415% (11901/31808)\n",
      "Loss: 1.723 | Acc: 37.400% (11920/31872)\n",
      "Loss: 1.723 | Acc: 37.419% (11950/31936)\n",
      "Loss: 1.723 | Acc: 37.431% (11978/32000)\n",
      "Loss: 1.723 | Acc: 37.444% (12006/32064)\n",
      "Loss: 1.722 | Acc: 37.463% (12036/32128)\n",
      "Loss: 1.721 | Acc: 37.475% (12064/32192)\n",
      "Loss: 1.721 | Acc: 37.488% (12092/32256)\n",
      "Loss: 1.721 | Acc: 37.488% (12116/32320)\n",
      "Loss: 1.720 | Acc: 37.506% (12146/32384)\n",
      "Loss: 1.720 | Acc: 37.518% (12174/32448)\n",
      "Loss: 1.720 | Acc: 37.543% (12206/32512)\n",
      "Loss: 1.719 | Acc: 37.571% (12239/32576)\n",
      "Loss: 1.719 | Acc: 37.567% (12262/32640)\n",
      "Loss: 1.719 | Acc: 37.589% (12293/32704)\n",
      "Loss: 1.718 | Acc: 37.598% (12320/32768)\n",
      "Loss: 1.718 | Acc: 37.622% (12352/32832)\n",
      "Loss: 1.717 | Acc: 37.646% (12384/32896)\n",
      "Loss: 1.717 | Acc: 37.661% (12413/32960)\n",
      "Loss: 1.716 | Acc: 37.682% (12444/33024)\n",
      "Loss: 1.715 | Acc: 37.696% (12473/33088)\n",
      "Loss: 1.714 | Acc: 37.723% (12506/33152)\n",
      "Loss: 1.713 | Acc: 37.762% (12543/33216)\n",
      "Loss: 1.713 | Acc: 37.788% (12576/33280)\n",
      "Loss: 1.713 | Acc: 37.797% (12603/33344)\n",
      "Loss: 1.712 | Acc: 37.838% (12641/33408)\n",
      "Loss: 1.711 | Acc: 37.844% (12667/33472)\n",
      "Loss: 1.711 | Acc: 37.849% (12693/33536)\n",
      "Loss: 1.710 | Acc: 37.848% (12717/33600)\n",
      "Loss: 1.710 | Acc: 37.845% (12740/33664)\n",
      "Loss: 1.710 | Acc: 37.859% (12769/33728)\n",
      "Loss: 1.710 | Acc: 37.852% (12791/33792)\n",
      "Loss: 1.710 | Acc: 37.860% (12818/33856)\n",
      "Loss: 1.709 | Acc: 37.886% (12851/33920)\n",
      "Loss: 1.709 | Acc: 37.906% (12882/33984)\n",
      "Loss: 1.708 | Acc: 37.914% (12909/34048)\n",
      "Loss: 1.708 | Acc: 37.943% (12943/34112)\n",
      "Loss: 1.707 | Acc: 37.954% (12971/34176)\n",
      "Loss: 1.706 | Acc: 37.982% (13005/34240)\n",
      "Loss: 1.706 | Acc: 37.990% (13032/34304)\n",
      "Loss: 1.706 | Acc: 38.018% (13066/34368)\n",
      "Loss: 1.705 | Acc: 38.026% (13093/34432)\n",
      "Loss: 1.705 | Acc: 38.033% (13120/34496)\n",
      "Loss: 1.705 | Acc: 38.061% (13154/34560)\n",
      "Loss: 1.704 | Acc: 38.081% (13185/34624)\n",
      "Loss: 1.703 | Acc: 38.100% (13216/34688)\n",
      "Loss: 1.703 | Acc: 38.113% (13245/34752)\n",
      "Loss: 1.702 | Acc: 38.135% (13277/34816)\n",
      "Loss: 1.702 | Acc: 38.159% (13310/34880)\n",
      "Loss: 1.702 | Acc: 38.175% (13340/34944)\n",
      "Loss: 1.701 | Acc: 38.194% (13371/35008)\n",
      "Loss: 1.701 | Acc: 38.213% (13402/35072)\n",
      "Loss: 1.701 | Acc: 38.220% (13429/35136)\n",
      "Loss: 1.700 | Acc: 38.241% (13461/35200)\n",
      "Loss: 1.700 | Acc: 38.251% (13489/35264)\n",
      "Loss: 1.700 | Acc: 38.256% (13515/35328)\n",
      "Loss: 1.700 | Acc: 38.274% (13546/35392)\n",
      "Loss: 1.699 | Acc: 38.284% (13574/35456)\n",
      "Loss: 1.699 | Acc: 38.305% (13606/35520)\n",
      "Loss: 1.698 | Acc: 38.323% (13637/35584)\n",
      "Loss: 1.698 | Acc: 38.330% (13664/35648)\n",
      "Loss: 1.698 | Acc: 38.357% (13698/35712)\n",
      "Loss: 1.697 | Acc: 38.378% (13730/35776)\n",
      "Loss: 1.697 | Acc: 38.393% (13760/35840)\n",
      "Loss: 1.697 | Acc: 38.400% (13787/35904)\n",
      "Loss: 1.697 | Acc: 38.404% (13813/35968)\n",
      "Loss: 1.696 | Acc: 38.424% (13845/36032)\n",
      "Loss: 1.696 | Acc: 38.439% (13875/36096)\n",
      "Loss: 1.695 | Acc: 38.465% (13909/36160)\n",
      "Loss: 1.695 | Acc: 38.477% (13938/36224)\n",
      "Loss: 1.695 | Acc: 38.487% (13966/36288)\n",
      "Loss: 1.694 | Acc: 38.496% (13994/36352)\n",
      "Loss: 1.694 | Acc: 38.505% (14022/36416)\n",
      "Loss: 1.693 | Acc: 38.533% (14057/36480)\n",
      "Loss: 1.693 | Acc: 38.567% (14094/36544)\n",
      "Loss: 1.693 | Acc: 38.563% (14117/36608)\n",
      "Loss: 1.692 | Acc: 38.580% (14148/36672)\n",
      "Loss: 1.692 | Acc: 38.600% (14180/36736)\n",
      "Loss: 1.692 | Acc: 38.622% (14213/36800)\n",
      "Loss: 1.691 | Acc: 38.637% (14243/36864)\n",
      "Loss: 1.691 | Acc: 38.637% (14268/36928)\n",
      "Loss: 1.691 | Acc: 38.633% (14291/36992)\n",
      "Loss: 1.690 | Acc: 38.658% (14325/37056)\n",
      "Loss: 1.690 | Acc: 38.664% (14352/37120)\n",
      "Loss: 1.690 | Acc: 38.662% (14376/37184)\n",
      "Loss: 1.690 | Acc: 38.660% (14400/37248)\n",
      "Loss: 1.689 | Acc: 38.669% (14428/37312)\n",
      "Loss: 1.689 | Acc: 38.683% (14458/37376)\n",
      "Loss: 1.688 | Acc: 38.705% (14491/37440)\n",
      "Loss: 1.687 | Acc: 38.727% (14524/37504)\n",
      "Loss: 1.687 | Acc: 38.732% (14551/37568)\n",
      "Loss: 1.687 | Acc: 38.741% (14579/37632)\n",
      "Loss: 1.687 | Acc: 38.765% (14613/37696)\n",
      "Loss: 1.686 | Acc: 38.774% (14641/37760)\n",
      "Loss: 1.686 | Acc: 38.766% (14663/37824)\n",
      "Loss: 1.686 | Acc: 38.785% (14695/37888)\n",
      "Loss: 1.686 | Acc: 38.796% (14724/37952)\n",
      "Loss: 1.685 | Acc: 38.823% (14759/38016)\n",
      "Loss: 1.685 | Acc: 38.824% (14784/38080)\n",
      "Loss: 1.685 | Acc: 38.848% (14818/38144)\n",
      "Loss: 1.684 | Acc: 38.869% (14851/38208)\n",
      "Loss: 1.685 | Acc: 38.867% (14875/38272)\n",
      "Loss: 1.684 | Acc: 38.877% (14904/38336)\n",
      "Loss: 1.684 | Acc: 38.901% (14938/38400)\n",
      "Loss: 1.683 | Acc: 38.914% (14968/38464)\n",
      "Loss: 1.683 | Acc: 38.922% (14996/38528)\n",
      "Loss: 1.682 | Acc: 38.954% (15033/38592)\n",
      "Loss: 1.682 | Acc: 38.972% (15065/38656)\n",
      "Loss: 1.681 | Acc: 38.998% (15100/38720)\n",
      "Loss: 1.681 | Acc: 39.001% (15126/38784)\n",
      "Loss: 1.681 | Acc: 39.024% (15160/38848)\n",
      "Loss: 1.680 | Acc: 39.052% (15196/38912)\n",
      "Loss: 1.680 | Acc: 39.062% (15225/38976)\n",
      "Loss: 1.680 | Acc: 39.045% (15243/39040)\n",
      "Loss: 1.680 | Acc: 39.057% (15273/39104)\n",
      "Loss: 1.679 | Acc: 39.088% (15310/39168)\n",
      "Loss: 1.679 | Acc: 39.083% (15333/39232)\n",
      "Loss: 1.679 | Acc: 39.088% (15360/39296)\n",
      "Loss: 1.679 | Acc: 39.106% (15392/39360)\n",
      "Loss: 1.679 | Acc: 39.121% (15423/39424)\n",
      "Loss: 1.679 | Acc: 39.118% (15447/39488)\n",
      "Loss: 1.678 | Acc: 39.126% (15475/39552)\n",
      "Loss: 1.678 | Acc: 39.126% (15500/39616)\n",
      "Loss: 1.678 | Acc: 39.153% (15536/39680)\n",
      "Loss: 1.677 | Acc: 39.161% (15564/39744)\n",
      "Loss: 1.677 | Acc: 39.165% (15591/39808)\n",
      "Loss: 1.677 | Acc: 39.175% (15620/39872)\n",
      "Loss: 1.676 | Acc: 39.195% (15653/39936)\n",
      "Loss: 1.676 | Acc: 39.195% (15678/40000)\n",
      "Loss: 1.676 | Acc: 39.217% (15712/40064)\n",
      "Loss: 1.676 | Acc: 39.222% (15739/40128)\n",
      "Loss: 1.675 | Acc: 39.229% (15767/40192)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.675 | Acc: 39.259% (15804/40256)\n",
      "Loss: 1.674 | Acc: 39.281% (15838/40320)\n",
      "Loss: 1.674 | Acc: 39.263% (15856/40384)\n",
      "Loss: 1.674 | Acc: 39.265% (15882/40448)\n",
      "Loss: 1.674 | Acc: 39.280% (15913/40512)\n",
      "Loss: 1.674 | Acc: 39.282% (15939/40576)\n",
      "Loss: 1.674 | Acc: 39.289% (15967/40640)\n",
      "Loss: 1.674 | Acc: 39.291% (15993/40704)\n",
      "Loss: 1.674 | Acc: 39.288% (16017/40768)\n",
      "Loss: 1.673 | Acc: 39.300% (16047/40832)\n",
      "Loss: 1.673 | Acc: 39.329% (16084/40896)\n",
      "Loss: 1.672 | Acc: 39.348% (16117/40960)\n",
      "Loss: 1.671 | Acc: 39.377% (16154/41024)\n",
      "Loss: 1.671 | Acc: 39.389% (16184/41088)\n",
      "Loss: 1.671 | Acc: 39.403% (16215/41152)\n",
      "Loss: 1.671 | Acc: 39.402% (16240/41216)\n",
      "Loss: 1.670 | Acc: 39.421% (16273/41280)\n",
      "Loss: 1.670 | Acc: 39.418% (16297/41344)\n",
      "Loss: 1.670 | Acc: 39.434% (16329/41408)\n",
      "Loss: 1.669 | Acc: 39.456% (16363/41472)\n",
      "Loss: 1.669 | Acc: 39.474% (16396/41536)\n",
      "Loss: 1.668 | Acc: 39.481% (16424/41600)\n",
      "Loss: 1.668 | Acc: 39.485% (16451/41664)\n",
      "Loss: 1.668 | Acc: 39.491% (16479/41728)\n",
      "Loss: 1.667 | Acc: 39.488% (16503/41792)\n",
      "Loss: 1.667 | Acc: 39.497% (16532/41856)\n",
      "Loss: 1.666 | Acc: 39.518% (16566/41920)\n",
      "Loss: 1.666 | Acc: 39.534% (16598/41984)\n",
      "Loss: 1.666 | Acc: 39.545% (16628/42048)\n",
      "Loss: 1.666 | Acc: 39.554% (16657/42112)\n",
      "Loss: 1.665 | Acc: 39.553% (16682/42176)\n",
      "Loss: 1.665 | Acc: 39.564% (16712/42240)\n",
      "Loss: 1.665 | Acc: 39.559% (16735/42304)\n",
      "Loss: 1.665 | Acc: 39.582% (16770/42368)\n",
      "Loss: 1.664 | Acc: 39.607% (16806/42432)\n",
      "Loss: 1.664 | Acc: 39.620% (16837/42496)\n",
      "Loss: 1.664 | Acc: 39.633% (16868/42560)\n",
      "Loss: 1.663 | Acc: 39.654% (16902/42624)\n",
      "Loss: 1.663 | Acc: 39.669% (16934/42688)\n",
      "Loss: 1.662 | Acc: 39.689% (16968/42752)\n",
      "Loss: 1.662 | Acc: 39.705% (17000/42816)\n",
      "Loss: 1.661 | Acc: 39.727% (17035/42880)\n",
      "Loss: 1.661 | Acc: 39.742% (17067/42944)\n",
      "Loss: 1.660 | Acc: 39.751% (17096/43008)\n",
      "Loss: 1.660 | Acc: 39.759% (17125/43072)\n",
      "Loss: 1.660 | Acc: 39.763% (17152/43136)\n",
      "Loss: 1.660 | Acc: 39.771% (17181/43200)\n",
      "Loss: 1.659 | Acc: 39.765% (17204/43264)\n",
      "Loss: 1.659 | Acc: 39.783% (17237/43328)\n",
      "Loss: 1.659 | Acc: 39.798% (17269/43392)\n",
      "Loss: 1.659 | Acc: 39.806% (17298/43456)\n",
      "Loss: 1.658 | Acc: 39.818% (17329/43520)\n",
      "Loss: 1.658 | Acc: 39.833% (17361/43584)\n",
      "Loss: 1.658 | Acc: 39.835% (17387/43648)\n",
      "Loss: 1.658 | Acc: 39.843% (17416/43712)\n",
      "Loss: 1.657 | Acc: 39.867% (17452/43776)\n",
      "Loss: 1.657 | Acc: 39.877% (17482/43840)\n",
      "Loss: 1.657 | Acc: 39.894% (17515/43904)\n",
      "Loss: 1.657 | Acc: 39.897% (17542/43968)\n",
      "Loss: 1.656 | Acc: 39.923% (17579/44032)\n",
      "Loss: 1.655 | Acc: 39.949% (17616/44096)\n",
      "Loss: 1.655 | Acc: 39.948% (17641/44160)\n",
      "Loss: 1.655 | Acc: 39.962% (17673/44224)\n",
      "Loss: 1.655 | Acc: 39.977% (17705/44288)\n",
      "Loss: 1.654 | Acc: 39.982% (17733/44352)\n",
      "Loss: 1.654 | Acc: 39.977% (17756/44416)\n",
      "Loss: 1.654 | Acc: 40.004% (17794/44480)\n",
      "Loss: 1.654 | Acc: 40.014% (17824/44544)\n",
      "Loss: 1.653 | Acc: 40.026% (17855/44608)\n",
      "Loss: 1.653 | Acc: 40.052% (17892/44672)\n",
      "Loss: 1.652 | Acc: 40.071% (17926/44736)\n",
      "Loss: 1.652 | Acc: 40.085% (17958/44800)\n",
      "Loss: 1.652 | Acc: 40.090% (17986/44864)\n",
      "Loss: 1.651 | Acc: 40.109% (18020/44928)\n",
      "Loss: 1.650 | Acc: 40.116% (18049/44992)\n",
      "Loss: 1.650 | Acc: 40.115% (18074/45056)\n",
      "Loss: 1.650 | Acc: 40.122% (18103/45120)\n",
      "Loss: 1.649 | Acc: 40.151% (18142/45184)\n",
      "Loss: 1.649 | Acc: 40.170% (18176/45248)\n",
      "Loss: 1.649 | Acc: 40.175% (18204/45312)\n",
      "Loss: 1.649 | Acc: 40.160% (18223/45376)\n",
      "Loss: 1.648 | Acc: 40.180% (18258/45440)\n",
      "Loss: 1.648 | Acc: 40.192% (18289/45504)\n",
      "Loss: 1.648 | Acc: 40.210% (18323/45568)\n",
      "Loss: 1.647 | Acc: 40.222% (18354/45632)\n",
      "Loss: 1.647 | Acc: 40.244% (18390/45696)\n",
      "Loss: 1.647 | Acc: 40.258% (18422/45760)\n",
      "Loss: 1.646 | Acc: 40.278% (18457/45824)\n",
      "Loss: 1.646 | Acc: 40.283% (18485/45888)\n",
      "Loss: 1.646 | Acc: 40.301% (18519/45952)\n",
      "Loss: 1.645 | Acc: 40.316% (18552/46016)\n",
      "Loss: 1.645 | Acc: 40.317% (18578/46080)\n",
      "Loss: 1.645 | Acc: 40.335% (18612/46144)\n",
      "Loss: 1.645 | Acc: 40.344% (18642/46208)\n",
      "Loss: 1.644 | Acc: 40.344% (18668/46272)\n",
      "Loss: 1.645 | Acc: 40.334% (18689/46336)\n",
      "Loss: 1.644 | Acc: 40.345% (18720/46400)\n",
      "Loss: 1.644 | Acc: 40.350% (18748/46464)\n",
      "Loss: 1.644 | Acc: 40.361% (18779/46528)\n",
      "Loss: 1.643 | Acc: 40.385% (18816/46592)\n",
      "Loss: 1.643 | Acc: 40.387% (18843/46656)\n",
      "Loss: 1.643 | Acc: 40.390% (18870/46720)\n",
      "Loss: 1.643 | Acc: 40.416% (18908/46784)\n",
      "Loss: 1.642 | Acc: 40.435% (18943/46848)\n",
      "Loss: 1.642 | Acc: 40.444% (18973/46912)\n",
      "Loss: 1.642 | Acc: 40.448% (19001/46976)\n",
      "Loss: 1.641 | Acc: 40.472% (19038/47040)\n",
      "Loss: 1.641 | Acc: 40.485% (19070/47104)\n",
      "Loss: 1.641 | Acc: 40.496% (19101/47168)\n",
      "Loss: 1.640 | Acc: 40.509% (19133/47232)\n",
      "Loss: 1.640 | Acc: 40.526% (19167/47296)\n",
      "Loss: 1.640 | Acc: 40.545% (19202/47360)\n",
      "Loss: 1.640 | Acc: 40.551% (19231/47424)\n",
      "Loss: 1.640 | Acc: 40.534% (19249/47488)\n",
      "Loss: 1.639 | Acc: 40.545% (19280/47552)\n",
      "Loss: 1.639 | Acc: 40.562% (19314/47616)\n",
      "Loss: 1.639 | Acc: 40.573% (19345/47680)\n",
      "Loss: 1.639 | Acc: 40.558% (19364/47744)\n",
      "Loss: 1.639 | Acc: 40.552% (19387/47808)\n",
      "Loss: 1.639 | Acc: 40.556% (19415/47872)\n",
      "Loss: 1.639 | Acc: 40.544% (19435/47936)\n",
      "Loss: 1.639 | Acc: 40.556% (19467/48000)\n",
      "Loss: 1.638 | Acc: 40.592% (19510/48064)\n",
      "Loss: 1.638 | Acc: 40.604% (19542/48128)\n",
      "Loss: 1.638 | Acc: 40.604% (19568/48192)\n",
      "Loss: 1.637 | Acc: 40.615% (19599/48256)\n",
      "Loss: 1.637 | Acc: 40.608% (19622/48320)\n",
      "Loss: 1.637 | Acc: 40.613% (19650/48384)\n",
      "Loss: 1.637 | Acc: 40.619% (19679/48448)\n",
      "Loss: 1.637 | Acc: 40.613% (19702/48512)\n",
      "Loss: 1.637 | Acc: 40.613% (19728/48576)\n",
      "Loss: 1.637 | Acc: 40.631% (19763/48640)\n",
      "Loss: 1.636 | Acc: 40.650% (19798/48704)\n",
      "Loss: 1.636 | Acc: 40.664% (19831/48768)\n",
      "Loss: 1.636 | Acc: 40.660% (19855/48832)\n",
      "Loss: 1.635 | Acc: 40.674% (19888/48896)\n",
      "Loss: 1.635 | Acc: 40.682% (19918/48960)\n",
      "Loss: 1.635 | Acc: 40.690% (19938/49000)\n",
      "Epoch 0 of training is completed, Training accuracy for this epoch is 40.689795918367345\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.412 | Acc: 53.125% (34/64)\n",
      "Loss: 1.355 | Acc: 54.688% (70/128)\n",
      "Loss: 1.446 | Acc: 48.438% (93/192)\n",
      "Loss: 1.517 | Acc: 45.703% (117/256)\n",
      "Loss: 1.468 | Acc: 45.938% (147/320)\n",
      "Loss: 1.476 | Acc: 46.354% (178/384)\n",
      "Loss: 1.489 | Acc: 45.312% (203/448)\n",
      "Loss: 1.478 | Acc: 45.898% (235/512)\n",
      "Loss: 1.455 | Acc: 46.181% (266/576)\n",
      "Loss: 1.444 | Acc: 45.938% (294/640)\n",
      "Loss: 1.451 | Acc: 45.739% (322/704)\n",
      "Loss: 1.465 | Acc: 45.182% (347/768)\n",
      "Loss: 1.471 | Acc: 44.952% (374/832)\n",
      "Loss: 1.464 | Acc: 45.647% (409/896)\n",
      "Loss: 1.449 | Acc: 46.354% (445/960)\n",
      "Loss: 1.445 | Acc: 46.973% (481/1024)\n",
      "Loss: 1.446 | Acc: 47.059% (512/1088)\n",
      "Loss: 1.444 | Acc: 47.396% (546/1152)\n",
      "Loss: 1.441 | Acc: 47.780% (581/1216)\n",
      "Loss: 1.452 | Acc: 47.422% (607/1280)\n",
      "Loss: 1.454 | Acc: 47.247% (635/1344)\n",
      "Loss: 1.454 | Acc: 47.017% (662/1408)\n",
      "Loss: 1.451 | Acc: 47.147% (694/1472)\n",
      "Loss: 1.459 | Acc: 46.810% (719/1536)\n",
      "Loss: 1.458 | Acc: 46.938% (751/1600)\n",
      "Loss: 1.459 | Acc: 46.875% (780/1664)\n",
      "Loss: 1.460 | Acc: 46.991% (812/1728)\n",
      "Loss: 1.458 | Acc: 46.819% (839/1792)\n",
      "Loss: 1.456 | Acc: 46.875% (870/1856)\n",
      "Loss: 1.456 | Acc: 47.135% (905/1920)\n",
      "Loss: 1.456 | Acc: 47.026% (933/1984)\n",
      "Loss: 1.456 | Acc: 46.826% (959/2048)\n",
      "Loss: 1.450 | Acc: 46.922% (991/2112)\n",
      "Loss: 1.452 | Acc: 46.921% (1021/2176)\n",
      "Loss: 1.452 | Acc: 46.875% (1050/2240)\n",
      "Loss: 1.455 | Acc: 46.962% (1082/2304)\n",
      "Loss: 1.458 | Acc: 46.959% (1112/2368)\n",
      "Loss: 1.463 | Acc: 47.039% (1144/2432)\n",
      "Loss: 1.462 | Acc: 47.155% (1177/2496)\n",
      "Loss: 1.470 | Acc: 46.992% (1203/2560)\n",
      "Loss: 1.471 | Acc: 46.951% (1232/2624)\n",
      "Loss: 1.471 | Acc: 46.912% (1261/2688)\n",
      "Loss: 1.469 | Acc: 46.766% (1287/2752)\n",
      "Loss: 1.469 | Acc: 46.697% (1315/2816)\n",
      "Loss: 1.468 | Acc: 46.597% (1342/2880)\n",
      "Loss: 1.467 | Acc: 46.637% (1373/2944)\n",
      "Loss: 1.463 | Acc: 46.975% (1413/3008)\n",
      "Loss: 1.464 | Acc: 46.810% (1438/3072)\n",
      "Loss: 1.464 | Acc: 46.779% (1467/3136)\n",
      "Loss: 1.464 | Acc: 46.875% (1500/3200)\n",
      "Loss: 1.463 | Acc: 47.028% (1535/3264)\n",
      "Loss: 1.467 | Acc: 46.995% (1564/3328)\n",
      "Loss: 1.467 | Acc: 46.993% (1594/3392)\n",
      "Loss: 1.469 | Acc: 46.875% (1620/3456)\n",
      "Loss: 1.470 | Acc: 46.790% (1647/3520)\n",
      "Loss: 1.468 | Acc: 46.847% (1679/3584)\n",
      "Loss: 1.469 | Acc: 46.848% (1709/3648)\n",
      "Loss: 1.467 | Acc: 46.848% (1739/3712)\n",
      "Loss: 1.469 | Acc: 46.822% (1768/3776)\n",
      "Loss: 1.468 | Acc: 46.823% (1798/3840)\n",
      "Loss: 1.469 | Acc: 46.824% (1828/3904)\n",
      "Loss: 1.467 | Acc: 46.925% (1862/3968)\n",
      "Loss: 1.470 | Acc: 46.900% (1891/4032)\n",
      "Loss: 1.471 | Acc: 46.899% (1921/4096)\n",
      "Loss: 1.475 | Acc: 46.827% (1948/4160)\n",
      "Loss: 1.473 | Acc: 46.993% (1985/4224)\n",
      "Loss: 1.474 | Acc: 46.922% (2012/4288)\n",
      "Loss: 1.474 | Acc: 47.013% (2046/4352)\n",
      "Loss: 1.472 | Acc: 47.169% (2083/4416)\n",
      "Loss: 1.471 | Acc: 47.277% (2118/4480)\n",
      "Loss: 1.468 | Acc: 47.403% (2154/4544)\n",
      "Loss: 1.469 | Acc: 47.418% (2185/4608)\n",
      "Loss: 1.468 | Acc: 47.474% (2218/4672)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.467 | Acc: 47.466% (2248/4736)\n",
      "Loss: 1.467 | Acc: 47.542% (2282/4800)\n",
      "Loss: 1.464 | Acc: 47.615% (2316/4864)\n",
      "Loss: 1.464 | Acc: 47.687% (2350/4928)\n",
      "Loss: 1.464 | Acc: 47.676% (2380/4992)\n",
      "Loss: 1.462 | Acc: 47.666% (2410/5056)\n",
      "Loss: 1.463 | Acc: 47.617% (2438/5120)\n",
      "Loss: 1.462 | Acc: 47.589% (2467/5184)\n",
      "Loss: 1.464 | Acc: 47.542% (2495/5248)\n",
      "Loss: 1.465 | Acc: 47.459% (2521/5312)\n",
      "Loss: 1.469 | Acc: 47.340% (2545/5376)\n",
      "Loss: 1.469 | Acc: 47.279% (2572/5440)\n",
      "Loss: 1.469 | Acc: 47.347% (2606/5504)\n",
      "Loss: 1.469 | Acc: 47.306% (2634/5568)\n",
      "Loss: 1.469 | Acc: 47.390% (2669/5632)\n",
      "Loss: 1.470 | Acc: 47.419% (2701/5696)\n",
      "Loss: 1.467 | Acc: 47.465% (2734/5760)\n",
      "Loss: 1.466 | Acc: 47.527% (2768/5824)\n",
      "Loss: 1.468 | Acc: 47.418% (2792/5888)\n",
      "Loss: 1.469 | Acc: 47.379% (2820/5952)\n",
      "Loss: 1.470 | Acc: 47.291% (2845/6016)\n",
      "Loss: 1.471 | Acc: 47.220% (2871/6080)\n",
      "Loss: 1.470 | Acc: 47.282% (2905/6144)\n",
      "Loss: 1.471 | Acc: 47.197% (2930/6208)\n",
      "Loss: 1.473 | Acc: 47.066% (2952/6272)\n",
      "Loss: 1.473 | Acc: 47.143% (2987/6336)\n",
      "Loss: 1.473 | Acc: 47.078% (3013/6400)\n",
      "Loss: 1.474 | Acc: 47.061% (3042/6464)\n",
      "Loss: 1.475 | Acc: 47.059% (3072/6528)\n",
      "Loss: 1.476 | Acc: 47.027% (3100/6592)\n",
      "Loss: 1.477 | Acc: 46.950% (3125/6656)\n",
      "Loss: 1.477 | Acc: 46.920% (3153/6720)\n",
      "Loss: 1.477 | Acc: 46.875% (3180/6784)\n",
      "Loss: 1.475 | Acc: 46.933% (3214/6848)\n",
      "Loss: 1.476 | Acc: 46.861% (3239/6912)\n",
      "Loss: 1.478 | Acc: 46.846% (3268/6976)\n",
      "Loss: 1.479 | Acc: 46.861% (3299/7040)\n",
      "Loss: 1.478 | Acc: 46.875% (3330/7104)\n",
      "Loss: 1.478 | Acc: 46.833% (3357/7168)\n",
      "Loss: 1.480 | Acc: 46.764% (3382/7232)\n",
      "Loss: 1.478 | Acc: 46.806% (3415/7296)\n",
      "Loss: 1.476 | Acc: 46.902% (3452/7360)\n",
      "Loss: 1.476 | Acc: 46.821% (3476/7424)\n",
      "Loss: 1.475 | Acc: 46.808% (3505/7488)\n",
      "Loss: 1.475 | Acc: 46.835% (3537/7552)\n",
      "Loss: 1.476 | Acc: 46.822% (3566/7616)\n",
      "Loss: 1.475 | Acc: 46.836% (3597/7680)\n",
      "Loss: 1.473 | Acc: 46.888% (3631/7744)\n",
      "Loss: 1.472 | Acc: 46.888% (3661/7808)\n",
      "Loss: 1.474 | Acc: 46.811% (3685/7872)\n",
      "Loss: 1.475 | Acc: 46.787% (3713/7936)\n",
      "Loss: 1.475 | Acc: 46.775% (3742/8000)\n",
      "Loss: 1.475 | Acc: 46.801% (3774/8064)\n",
      "Loss: 1.476 | Acc: 46.777% (3802/8128)\n",
      "Loss: 1.475 | Acc: 46.814% (3835/8192)\n",
      "Loss: 1.476 | Acc: 46.742% (3859/8256)\n",
      "Loss: 1.477 | Acc: 46.755% (3890/8320)\n",
      "Loss: 1.477 | Acc: 46.744% (3919/8384)\n",
      "Loss: 1.476 | Acc: 46.757% (3950/8448)\n",
      "Loss: 1.477 | Acc: 46.769% (3981/8512)\n",
      "Loss: 1.478 | Acc: 46.735% (4008/8576)\n",
      "Loss: 1.478 | Acc: 46.771% (4041/8640)\n",
      "Loss: 1.478 | Acc: 46.795% (4073/8704)\n",
      "Loss: 1.479 | Acc: 46.738% (4098/8768)\n",
      "Loss: 1.479 | Acc: 46.750% (4129/8832)\n",
      "Loss: 1.478 | Acc: 46.796% (4163/8896)\n",
      "Loss: 1.477 | Acc: 46.864% (4199/8960)\n",
      "Loss: 1.477 | Acc: 46.897% (4232/9024)\n",
      "Loss: 1.478 | Acc: 46.809% (4254/9088)\n",
      "Loss: 1.477 | Acc: 46.799% (4283/9152)\n",
      "Loss: 1.476 | Acc: 46.875% (4320/9216)\n",
      "Loss: 1.476 | Acc: 46.875% (4350/9280)\n",
      "Loss: 1.476 | Acc: 46.800% (4373/9344)\n",
      "Loss: 1.477 | Acc: 46.822% (4405/9408)\n",
      "Loss: 1.477 | Acc: 46.854% (4438/9472)\n",
      "Loss: 1.477 | Acc: 46.854% (4468/9536)\n",
      "Loss: 1.475 | Acc: 46.865% (4499/9600)\n",
      "Loss: 1.475 | Acc: 46.844% (4527/9664)\n",
      "Loss: 1.475 | Acc: 46.844% (4557/9728)\n",
      "Loss: 1.475 | Acc: 46.824% (4585/9792)\n",
      "Loss: 1.475 | Acc: 46.855% (4618/9856)\n",
      "Loss: 1.476 | Acc: 46.774% (4640/9920)\n",
      "Loss: 1.475 | Acc: 46.775% (4670/9984)\n",
      "Loss: 1.475 | Acc: 46.760% (4676/10000)\n",
      "Evaluation of Epoch 0 is completed, Test accuracy for this epoch is 46.76\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 1.573 | Acc: 46.875% (30/64)\n",
      "Loss: 1.511 | Acc: 48.438% (62/128)\n",
      "Loss: 1.517 | Acc: 46.354% (89/192)\n",
      "Loss: 1.473 | Acc: 49.219% (126/256)\n",
      "Loss: 1.448 | Acc: 49.375% (158/320)\n",
      "Loss: 1.398 | Acc: 51.302% (197/384)\n",
      "Loss: 1.363 | Acc: 52.455% (235/448)\n",
      "Loss: 1.378 | Acc: 50.977% (261/512)\n",
      "Loss: 1.379 | Acc: 51.389% (296/576)\n",
      "Loss: 1.387 | Acc: 51.250% (328/640)\n",
      "Loss: 1.395 | Acc: 51.278% (361/704)\n",
      "Loss: 1.386 | Acc: 51.172% (393/768)\n",
      "Loss: 1.383 | Acc: 50.601% (421/832)\n",
      "Loss: 1.386 | Acc: 50.446% (452/896)\n",
      "Loss: 1.378 | Acc: 50.938% (489/960)\n",
      "Loss: 1.357 | Acc: 51.367% (526/1024)\n",
      "Loss: 1.355 | Acc: 52.022% (566/1088)\n",
      "Loss: 1.359 | Acc: 51.997% (599/1152)\n",
      "Loss: 1.356 | Acc: 52.220% (635/1216)\n",
      "Loss: 1.365 | Acc: 51.484% (659/1280)\n",
      "Loss: 1.362 | Acc: 51.414% (691/1344)\n",
      "Loss: 1.365 | Acc: 51.136% (720/1408)\n",
      "Loss: 1.379 | Acc: 50.815% (748/1472)\n",
      "Loss: 1.380 | Acc: 50.716% (779/1536)\n",
      "Loss: 1.379 | Acc: 50.875% (814/1600)\n",
      "Loss: 1.374 | Acc: 51.142% (851/1664)\n",
      "Loss: 1.374 | Acc: 51.215% (885/1728)\n",
      "Loss: 1.376 | Acc: 51.507% (923/1792)\n",
      "Loss: 1.377 | Acc: 51.562% (957/1856)\n",
      "Loss: 1.374 | Acc: 51.615% (991/1920)\n",
      "Loss: 1.381 | Acc: 51.411% (1020/1984)\n",
      "Loss: 1.385 | Acc: 50.977% (1044/2048)\n",
      "Loss: 1.386 | Acc: 50.994% (1077/2112)\n",
      "Loss: 1.395 | Acc: 50.643% (1102/2176)\n",
      "Loss: 1.396 | Acc: 50.670% (1135/2240)\n",
      "Loss: 1.398 | Acc: 50.521% (1164/2304)\n",
      "Loss: 1.397 | Acc: 50.338% (1192/2368)\n",
      "Loss: 1.389 | Acc: 50.699% (1233/2432)\n",
      "Loss: 1.389 | Acc: 50.641% (1264/2496)\n",
      "Loss: 1.387 | Acc: 50.625% (1296/2560)\n",
      "Loss: 1.386 | Acc: 50.724% (1331/2624)\n",
      "Loss: 1.387 | Acc: 50.818% (1366/2688)\n",
      "Loss: 1.390 | Acc: 50.618% (1393/2752)\n",
      "Loss: 1.390 | Acc: 50.462% (1421/2816)\n",
      "Loss: 1.384 | Acc: 50.521% (1455/2880)\n",
      "Loss: 1.386 | Acc: 50.408% (1484/2944)\n",
      "Loss: 1.383 | Acc: 50.465% (1518/3008)\n",
      "Loss: 1.380 | Acc: 50.456% (1550/3072)\n",
      "Loss: 1.382 | Acc: 50.415% (1581/3136)\n",
      "Loss: 1.376 | Acc: 50.594% (1619/3200)\n",
      "Loss: 1.376 | Acc: 50.582% (1651/3264)\n",
      "Loss: 1.384 | Acc: 50.331% (1675/3328)\n",
      "Loss: 1.381 | Acc: 50.442% (1711/3392)\n",
      "Loss: 1.380 | Acc: 50.289% (1738/3456)\n",
      "Loss: 1.381 | Acc: 50.284% (1770/3520)\n",
      "Loss: 1.380 | Acc: 50.391% (1806/3584)\n",
      "Loss: 1.380 | Acc: 50.411% (1839/3648)\n",
      "Loss: 1.382 | Acc: 50.350% (1869/3712)\n",
      "Loss: 1.379 | Acc: 50.477% (1906/3776)\n",
      "Loss: 1.376 | Acc: 50.599% (1943/3840)\n",
      "Loss: 1.375 | Acc: 50.820% (1984/3904)\n",
      "Loss: 1.374 | Acc: 50.832% (2017/3968)\n",
      "Loss: 1.375 | Acc: 50.843% (2050/4032)\n",
      "Loss: 1.376 | Acc: 50.806% (2081/4096)\n",
      "Loss: 1.377 | Acc: 50.721% (2110/4160)\n",
      "Loss: 1.377 | Acc: 50.687% (2141/4224)\n",
      "Loss: 1.382 | Acc: 50.606% (2170/4288)\n",
      "Loss: 1.382 | Acc: 50.551% (2200/4352)\n",
      "Loss: 1.380 | Acc: 50.476% (2229/4416)\n",
      "Loss: 1.378 | Acc: 50.491% (2262/4480)\n",
      "Loss: 1.378 | Acc: 50.440% (2292/4544)\n",
      "Loss: 1.376 | Acc: 50.412% (2323/4608)\n",
      "Loss: 1.376 | Acc: 50.449% (2357/4672)\n",
      "Loss: 1.378 | Acc: 50.443% (2389/4736)\n",
      "Loss: 1.377 | Acc: 50.500% (2424/4800)\n",
      "Loss: 1.375 | Acc: 50.637% (2463/4864)\n",
      "Loss: 1.376 | Acc: 50.670% (2497/4928)\n",
      "Loss: 1.376 | Acc: 50.661% (2529/4992)\n",
      "Loss: 1.378 | Acc: 50.653% (2561/5056)\n",
      "Loss: 1.376 | Acc: 50.723% (2597/5120)\n",
      "Loss: 1.375 | Acc: 50.772% (2632/5184)\n",
      "Loss: 1.374 | Acc: 50.743% (2663/5248)\n",
      "Loss: 1.377 | Acc: 50.678% (2692/5312)\n",
      "Loss: 1.376 | Acc: 50.558% (2718/5376)\n",
      "Loss: 1.379 | Acc: 50.515% (2748/5440)\n",
      "Loss: 1.381 | Acc: 50.545% (2782/5504)\n",
      "Loss: 1.382 | Acc: 50.449% (2809/5568)\n",
      "Loss: 1.381 | Acc: 50.497% (2844/5632)\n",
      "Loss: 1.382 | Acc: 50.386% (2870/5696)\n",
      "Loss: 1.382 | Acc: 50.295% (2897/5760)\n",
      "Loss: 1.382 | Acc: 50.223% (2925/5824)\n",
      "Loss: 1.381 | Acc: 50.272% (2960/5888)\n",
      "Loss: 1.380 | Acc: 50.319% (2995/5952)\n",
      "Loss: 1.378 | Acc: 50.382% (3031/6016)\n",
      "Loss: 1.375 | Acc: 50.411% (3065/6080)\n",
      "Loss: 1.373 | Acc: 50.521% (3104/6144)\n",
      "Loss: 1.372 | Acc: 50.548% (3138/6208)\n",
      "Loss: 1.371 | Acc: 50.558% (3171/6272)\n",
      "Loss: 1.369 | Acc: 50.600% (3206/6336)\n",
      "Loss: 1.372 | Acc: 50.500% (3232/6400)\n",
      "Loss: 1.372 | Acc: 50.495% (3264/6464)\n",
      "Loss: 1.375 | Acc: 50.368% (3288/6528)\n",
      "Loss: 1.376 | Acc: 50.288% (3315/6592)\n",
      "Loss: 1.376 | Acc: 50.346% (3351/6656)\n",
      "Loss: 1.376 | Acc: 50.357% (3384/6720)\n",
      "Loss: 1.374 | Acc: 50.398% (3419/6784)\n",
      "Loss: 1.375 | Acc: 50.350% (3448/6848)\n",
      "Loss: 1.376 | Acc: 50.275% (3475/6912)\n",
      "Loss: 1.376 | Acc: 50.287% (3508/6976)\n",
      "Loss: 1.376 | Acc: 50.298% (3541/7040)\n",
      "Loss: 1.375 | Acc: 50.267% (3571/7104)\n",
      "Loss: 1.375 | Acc: 50.195% (3598/7168)\n",
      "Loss: 1.377 | Acc: 50.152% (3627/7232)\n",
      "Loss: 1.375 | Acc: 50.164% (3660/7296)\n",
      "Loss: 1.375 | Acc: 50.163% (3692/7360)\n",
      "Loss: 1.377 | Acc: 50.148% (3723/7424)\n",
      "Loss: 1.378 | Acc: 50.067% (3749/7488)\n",
      "Loss: 1.378 | Acc: 50.040% (3779/7552)\n",
      "Loss: 1.379 | Acc: 50.000% (3808/7616)\n",
      "Loss: 1.379 | Acc: 50.013% (3841/7680)\n",
      "Loss: 1.380 | Acc: 49.987% (3871/7744)\n",
      "Loss: 1.379 | Acc: 50.026% (3906/7808)\n",
      "Loss: 1.378 | Acc: 50.089% (3943/7872)\n",
      "Loss: 1.379 | Acc: 50.000% (3968/7936)\n",
      "Loss: 1.379 | Acc: 50.038% (4003/8000)\n",
      "Loss: 1.378 | Acc: 50.062% (4037/8064)\n",
      "Loss: 1.379 | Acc: 50.098% (4072/8128)\n",
      "Loss: 1.378 | Acc: 50.159% (4109/8192)\n",
      "Loss: 1.379 | Acc: 50.121% (4138/8256)\n",
      "Loss: 1.379 | Acc: 50.168% (4174/8320)\n",
      "Loss: 1.379 | Acc: 50.179% (4207/8384)\n",
      "Loss: 1.378 | Acc: 50.213% (4242/8448)\n",
      "Loss: 1.377 | Acc: 50.294% (4281/8512)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.376 | Acc: 50.292% (4313/8576)\n",
      "Loss: 1.376 | Acc: 50.312% (4347/8640)\n",
      "Loss: 1.378 | Acc: 50.253% (4374/8704)\n",
      "Loss: 1.379 | Acc: 50.228% (4404/8768)\n",
      "Loss: 1.377 | Acc: 50.215% (4435/8832)\n",
      "Loss: 1.379 | Acc: 50.191% (4465/8896)\n",
      "Loss: 1.379 | Acc: 50.156% (4494/8960)\n",
      "Loss: 1.383 | Acc: 50.011% (4513/9024)\n",
      "Loss: 1.383 | Acc: 49.978% (4542/9088)\n",
      "Loss: 1.383 | Acc: 49.967% (4573/9152)\n",
      "Loss: 1.382 | Acc: 50.033% (4611/9216)\n",
      "Loss: 1.383 | Acc: 49.978% (4638/9280)\n",
      "Loss: 1.383 | Acc: 49.968% (4669/9344)\n",
      "Loss: 1.385 | Acc: 49.904% (4695/9408)\n",
      "Loss: 1.388 | Acc: 49.799% (4717/9472)\n",
      "Loss: 1.388 | Acc: 49.801% (4749/9536)\n",
      "Loss: 1.388 | Acc: 49.802% (4781/9600)\n",
      "Loss: 1.388 | Acc: 49.855% (4818/9664)\n",
      "Loss: 1.388 | Acc: 49.836% (4848/9728)\n",
      "Loss: 1.388 | Acc: 49.786% (4875/9792)\n",
      "Loss: 1.387 | Acc: 49.756% (4904/9856)\n",
      "Loss: 1.387 | Acc: 49.798% (4940/9920)\n",
      "Loss: 1.387 | Acc: 49.770% (4969/9984)\n",
      "Loss: 1.387 | Acc: 49.751% (4999/10048)\n",
      "Loss: 1.387 | Acc: 49.812% (5037/10112)\n",
      "Loss: 1.387 | Acc: 49.813% (5069/10176)\n",
      "Loss: 1.386 | Acc: 49.873% (5107/10240)\n",
      "Loss: 1.385 | Acc: 49.932% (5145/10304)\n",
      "Loss: 1.387 | Acc: 49.884% (5172/10368)\n",
      "Loss: 1.387 | Acc: 49.837% (5199/10432)\n",
      "Loss: 1.387 | Acc: 49.819% (5229/10496)\n",
      "Loss: 1.387 | Acc: 49.830% (5262/10560)\n",
      "Loss: 1.387 | Acc: 49.868% (5298/10624)\n",
      "Loss: 1.388 | Acc: 49.813% (5324/10688)\n",
      "Loss: 1.387 | Acc: 49.823% (5357/10752)\n",
      "Loss: 1.386 | Acc: 49.871% (5394/10816)\n",
      "Loss: 1.387 | Acc: 49.816% (5420/10880)\n",
      "Loss: 1.387 | Acc: 49.808% (5451/10944)\n",
      "Loss: 1.386 | Acc: 49.800% (5482/11008)\n",
      "Loss: 1.385 | Acc: 49.846% (5519/11072)\n",
      "Loss: 1.385 | Acc: 49.874% (5554/11136)\n",
      "Loss: 1.384 | Acc: 49.911% (5590/11200)\n",
      "Loss: 1.384 | Acc: 49.911% (5622/11264)\n",
      "Loss: 1.383 | Acc: 49.929% (5656/11328)\n",
      "Loss: 1.385 | Acc: 49.842% (5678/11392)\n",
      "Loss: 1.383 | Acc: 49.904% (5717/11456)\n",
      "Loss: 1.382 | Acc: 49.957% (5755/11520)\n",
      "Loss: 1.383 | Acc: 49.905% (5781/11584)\n",
      "Loss: 1.383 | Acc: 49.948% (5818/11648)\n",
      "Loss: 1.384 | Acc: 49.940% (5849/11712)\n",
      "Loss: 1.384 | Acc: 49.958% (5883/11776)\n",
      "Loss: 1.383 | Acc: 49.975% (5917/11840)\n",
      "Loss: 1.383 | Acc: 49.975% (5949/11904)\n",
      "Loss: 1.384 | Acc: 49.967% (5980/11968)\n",
      "Loss: 1.383 | Acc: 49.992% (6015/12032)\n",
      "Loss: 1.383 | Acc: 50.008% (6049/12096)\n",
      "Loss: 1.381 | Acc: 50.074% (6089/12160)\n",
      "Loss: 1.381 | Acc: 50.057% (6119/12224)\n",
      "Loss: 1.381 | Acc: 50.033% (6148/12288)\n",
      "Loss: 1.380 | Acc: 50.089% (6187/12352)\n",
      "Loss: 1.380 | Acc: 50.056% (6215/12416)\n",
      "Loss: 1.382 | Acc: 50.008% (6241/12480)\n",
      "Loss: 1.383 | Acc: 49.984% (6270/12544)\n",
      "Loss: 1.384 | Acc: 49.968% (6300/12608)\n",
      "Loss: 1.384 | Acc: 49.929% (6327/12672)\n",
      "Loss: 1.384 | Acc: 49.890% (6354/12736)\n",
      "Loss: 1.383 | Acc: 49.953% (6394/12800)\n",
      "Loss: 1.384 | Acc: 49.930% (6423/12864)\n",
      "Loss: 1.384 | Acc: 49.954% (6458/12928)\n",
      "Loss: 1.384 | Acc: 49.969% (6492/12992)\n",
      "Loss: 1.382 | Acc: 50.054% (6535/13056)\n",
      "Loss: 1.382 | Acc: 50.069% (6569/13120)\n",
      "Loss: 1.382 | Acc: 50.121% (6608/13184)\n",
      "Loss: 1.381 | Acc: 50.136% (6642/13248)\n",
      "Loss: 1.381 | Acc: 50.158% (6677/13312)\n",
      "Loss: 1.381 | Acc: 50.135% (6706/13376)\n",
      "Loss: 1.382 | Acc: 50.089% (6732/13440)\n",
      "Loss: 1.382 | Acc: 50.096% (6765/13504)\n",
      "Loss: 1.383 | Acc: 50.066% (6793/13568)\n",
      "Loss: 1.383 | Acc: 50.066% (6825/13632)\n",
      "Loss: 1.381 | Acc: 50.095% (6861/13696)\n",
      "Loss: 1.382 | Acc: 50.116% (6896/13760)\n",
      "Loss: 1.382 | Acc: 50.123% (6929/13824)\n",
      "Loss: 1.382 | Acc: 50.108% (6959/13888)\n",
      "Loss: 1.381 | Acc: 50.122% (6993/13952)\n",
      "Loss: 1.381 | Acc: 50.100% (7022/14016)\n",
      "Loss: 1.381 | Acc: 50.085% (7052/14080)\n",
      "Loss: 1.380 | Acc: 50.099% (7086/14144)\n",
      "Loss: 1.381 | Acc: 50.099% (7118/14208)\n",
      "Loss: 1.381 | Acc: 50.091% (7149/14272)\n",
      "Loss: 1.380 | Acc: 50.119% (7185/14336)\n",
      "Loss: 1.380 | Acc: 50.111% (7216/14400)\n",
      "Loss: 1.381 | Acc: 50.104% (7247/14464)\n",
      "Loss: 1.382 | Acc: 50.096% (7278/14528)\n",
      "Loss: 1.382 | Acc: 50.110% (7312/14592)\n",
      "Loss: 1.382 | Acc: 50.082% (7340/14656)\n",
      "Loss: 1.382 | Acc: 50.061% (7369/14720)\n",
      "Loss: 1.383 | Acc: 50.047% (7399/14784)\n",
      "Loss: 1.383 | Acc: 50.013% (7426/14848)\n",
      "Loss: 1.383 | Acc: 50.047% (7463/14912)\n",
      "Loss: 1.383 | Acc: 50.020% (7491/14976)\n",
      "Loss: 1.382 | Acc: 50.053% (7528/15040)\n",
      "Loss: 1.383 | Acc: 50.020% (7555/15104)\n",
      "Loss: 1.383 | Acc: 50.026% (7588/15168)\n",
      "Loss: 1.383 | Acc: 50.046% (7623/15232)\n",
      "Loss: 1.384 | Acc: 50.039% (7654/15296)\n",
      "Loss: 1.383 | Acc: 50.065% (7690/15360)\n",
      "Loss: 1.384 | Acc: 50.058% (7721/15424)\n",
      "Loss: 1.384 | Acc: 50.071% (7755/15488)\n",
      "Loss: 1.384 | Acc: 50.051% (7784/15552)\n",
      "Loss: 1.384 | Acc: 50.032% (7813/15616)\n",
      "Loss: 1.384 | Acc: 50.070% (7851/15680)\n",
      "Loss: 1.384 | Acc: 50.076% (7884/15744)\n",
      "Loss: 1.384 | Acc: 50.076% (7916/15808)\n",
      "Loss: 1.384 | Acc: 50.057% (7945/15872)\n",
      "Loss: 1.384 | Acc: 50.075% (7980/15936)\n",
      "Loss: 1.383 | Acc: 50.056% (8009/16000)\n",
      "Loss: 1.384 | Acc: 50.050% (8040/16064)\n",
      "Loss: 1.383 | Acc: 50.112% (8082/16128)\n",
      "Loss: 1.383 | Acc: 50.080% (8109/16192)\n",
      "Loss: 1.382 | Acc: 50.123% (8148/16256)\n",
      "Loss: 1.382 | Acc: 50.116% (8179/16320)\n",
      "Loss: 1.382 | Acc: 50.140% (8215/16384)\n",
      "Loss: 1.382 | Acc: 50.134% (8246/16448)\n",
      "Loss: 1.382 | Acc: 50.139% (8279/16512)\n",
      "Loss: 1.382 | Acc: 50.115% (8307/16576)\n",
      "Loss: 1.381 | Acc: 50.144% (8344/16640)\n",
      "Loss: 1.382 | Acc: 50.114% (8371/16704)\n",
      "Loss: 1.382 | Acc: 50.107% (8402/16768)\n",
      "Loss: 1.381 | Acc: 50.107% (8434/16832)\n",
      "Loss: 1.381 | Acc: 50.136% (8471/16896)\n",
      "Loss: 1.382 | Acc: 50.094% (8496/16960)\n",
      "Loss: 1.382 | Acc: 50.094% (8528/17024)\n",
      "Loss: 1.381 | Acc: 50.135% (8567/17088)\n",
      "Loss: 1.380 | Acc: 50.169% (8605/17152)\n",
      "Loss: 1.381 | Acc: 50.157% (8635/17216)\n",
      "Loss: 1.380 | Acc: 50.168% (8669/17280)\n",
      "Loss: 1.381 | Acc: 50.173% (8702/17344)\n",
      "Loss: 1.379 | Acc: 50.218% (8742/17408)\n",
      "Loss: 1.380 | Acc: 50.235% (8777/17472)\n",
      "Loss: 1.381 | Acc: 50.205% (8804/17536)\n",
      "Loss: 1.381 | Acc: 50.216% (8838/17600)\n",
      "Loss: 1.380 | Acc: 50.221% (8871/17664)\n",
      "Loss: 1.381 | Acc: 50.209% (8901/17728)\n",
      "Loss: 1.381 | Acc: 50.197% (8931/17792)\n",
      "Loss: 1.380 | Acc: 50.218% (8967/17856)\n",
      "Loss: 1.381 | Acc: 50.184% (8993/17920)\n",
      "Loss: 1.381 | Acc: 50.183% (9025/17984)\n",
      "Loss: 1.381 | Acc: 50.183% (9057/18048)\n",
      "Loss: 1.381 | Acc: 50.193% (9091/18112)\n",
      "Loss: 1.382 | Acc: 50.182% (9121/18176)\n",
      "Loss: 1.383 | Acc: 50.148% (9147/18240)\n",
      "Loss: 1.383 | Acc: 50.164% (9182/18304)\n",
      "Loss: 1.383 | Acc: 50.158% (9213/18368)\n",
      "Loss: 1.383 | Acc: 50.179% (9249/18432)\n",
      "Loss: 1.384 | Acc: 50.130% (9272/18496)\n",
      "Loss: 1.385 | Acc: 50.113% (9301/18560)\n",
      "Loss: 1.384 | Acc: 50.123% (9335/18624)\n",
      "Loss: 1.385 | Acc: 50.102% (9363/18688)\n",
      "Loss: 1.386 | Acc: 50.037% (9383/18752)\n",
      "Loss: 1.385 | Acc: 50.053% (9418/18816)\n",
      "Loss: 1.385 | Acc: 50.074% (9454/18880)\n",
      "Loss: 1.385 | Acc: 50.074% (9486/18944)\n",
      "Loss: 1.385 | Acc: 50.063% (9516/19008)\n",
      "Loss: 1.385 | Acc: 50.047% (9545/19072)\n",
      "Loss: 1.386 | Acc: 50.042% (9576/19136)\n",
      "Loss: 1.386 | Acc: 50.047% (9609/19200)\n",
      "Loss: 1.385 | Acc: 50.031% (9638/19264)\n",
      "Loss: 1.386 | Acc: 50.005% (9665/19328)\n",
      "Loss: 1.386 | Acc: 50.036% (9703/19392)\n",
      "Loss: 1.385 | Acc: 50.036% (9735/19456)\n",
      "Loss: 1.385 | Acc: 50.020% (9764/19520)\n",
      "Loss: 1.385 | Acc: 50.036% (9799/19584)\n",
      "Loss: 1.385 | Acc: 50.025% (9829/19648)\n",
      "Loss: 1.385 | Acc: 50.020% (9860/19712)\n",
      "Loss: 1.385 | Acc: 50.010% (9890/19776)\n",
      "Loss: 1.385 | Acc: 50.025% (9925/19840)\n",
      "Loss: 1.385 | Acc: 50.030% (9958/19904)\n",
      "Loss: 1.385 | Acc: 50.040% (9992/19968)\n",
      "Loss: 1.384 | Acc: 50.080% (10032/20032)\n",
      "Loss: 1.384 | Acc: 50.080% (10064/20096)\n",
      "Loss: 1.383 | Acc: 50.099% (10100/20160)\n",
      "Loss: 1.383 | Acc: 50.129% (10138/20224)\n",
      "Loss: 1.383 | Acc: 50.133% (10171/20288)\n",
      "Loss: 1.382 | Acc: 50.162% (10209/20352)\n",
      "Loss: 1.382 | Acc: 50.152% (10239/20416)\n",
      "Loss: 1.382 | Acc: 50.151% (10271/20480)\n",
      "Loss: 1.382 | Acc: 50.165% (10306/20544)\n",
      "Loss: 1.381 | Acc: 50.189% (10343/20608)\n",
      "Loss: 1.382 | Acc: 50.174% (10372/20672)\n",
      "Loss: 1.382 | Acc: 50.169% (10403/20736)\n",
      "Loss: 1.382 | Acc: 50.173% (10436/20800)\n",
      "Loss: 1.382 | Acc: 50.197% (10473/20864)\n",
      "Loss: 1.381 | Acc: 50.186% (10503/20928)\n",
      "Loss: 1.381 | Acc: 50.167% (10531/20992)\n",
      "Loss: 1.381 | Acc: 50.166% (10563/21056)\n",
      "Loss: 1.381 | Acc: 50.118% (10585/21120)\n",
      "Loss: 1.381 | Acc: 50.165% (10627/21184)\n",
      "Loss: 1.380 | Acc: 50.198% (10666/21248)\n",
      "Loss: 1.380 | Acc: 50.197% (10698/21312)\n",
      "Loss: 1.381 | Acc: 50.192% (10729/21376)\n",
      "Loss: 1.381 | Acc: 50.191% (10761/21440)\n",
      "Loss: 1.381 | Acc: 50.195% (10794/21504)\n",
      "Loss: 1.381 | Acc: 50.176% (10822/21568)\n",
      "Loss: 1.381 | Acc: 50.176% (10854/21632)\n",
      "Loss: 1.381 | Acc: 50.198% (10891/21696)\n",
      "Loss: 1.381 | Acc: 50.179% (10919/21760)\n",
      "Loss: 1.380 | Acc: 50.220% (10960/21824)\n",
      "Loss: 1.379 | Acc: 50.233% (10995/21888)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.379 | Acc: 50.232% (11027/21952)\n",
      "Loss: 1.379 | Acc: 50.245% (11062/22016)\n",
      "Loss: 1.379 | Acc: 50.240% (11093/22080)\n",
      "Loss: 1.380 | Acc: 50.221% (11121/22144)\n",
      "Loss: 1.380 | Acc: 50.198% (11148/22208)\n",
      "Loss: 1.380 | Acc: 50.233% (11188/22272)\n",
      "Loss: 1.381 | Acc: 50.224% (11218/22336)\n",
      "Loss: 1.380 | Acc: 50.228% (11251/22400)\n",
      "Loss: 1.381 | Acc: 50.200% (11277/22464)\n",
      "Loss: 1.380 | Acc: 50.218% (11313/22528)\n",
      "Loss: 1.380 | Acc: 50.235% (11349/22592)\n",
      "Loss: 1.380 | Acc: 50.225% (11379/22656)\n",
      "Loss: 1.380 | Acc: 50.238% (11414/22720)\n",
      "Loss: 1.381 | Acc: 50.224% (11443/22784)\n",
      "Loss: 1.380 | Acc: 50.232% (11477/22848)\n",
      "Loss: 1.381 | Acc: 50.231% (11509/22912)\n",
      "Loss: 1.381 | Acc: 50.218% (11538/22976)\n",
      "Loss: 1.380 | Acc: 50.256% (11579/23040)\n",
      "Loss: 1.381 | Acc: 50.268% (11614/23104)\n",
      "Loss: 1.380 | Acc: 50.281% (11649/23168)\n",
      "Loss: 1.380 | Acc: 50.310% (11688/23232)\n",
      "Loss: 1.380 | Acc: 50.326% (11724/23296)\n",
      "Loss: 1.380 | Acc: 50.342% (11760/23360)\n",
      "Loss: 1.380 | Acc: 50.354% (11795/23424)\n",
      "Loss: 1.379 | Acc: 50.358% (11828/23488)\n",
      "Loss: 1.379 | Acc: 50.365% (11862/23552)\n",
      "Loss: 1.379 | Acc: 50.373% (11896/23616)\n",
      "Loss: 1.379 | Acc: 50.410% (11937/23680)\n",
      "Loss: 1.378 | Acc: 50.425% (11973/23744)\n",
      "Loss: 1.378 | Acc: 50.449% (12011/23808)\n",
      "Loss: 1.379 | Acc: 50.423% (12037/23872)\n",
      "Loss: 1.378 | Acc: 50.447% (12075/23936)\n",
      "Loss: 1.378 | Acc: 50.458% (12110/24000)\n",
      "Loss: 1.378 | Acc: 50.461% (12143/24064)\n",
      "Loss: 1.378 | Acc: 50.443% (12171/24128)\n",
      "Loss: 1.378 | Acc: 50.446% (12204/24192)\n",
      "Loss: 1.378 | Acc: 50.453% (12238/24256)\n",
      "Loss: 1.378 | Acc: 50.473% (12275/24320)\n",
      "Loss: 1.378 | Acc: 50.488% (12311/24384)\n",
      "Loss: 1.378 | Acc: 50.466% (12338/24448)\n",
      "Loss: 1.379 | Acc: 50.465% (12370/24512)\n",
      "Loss: 1.379 | Acc: 50.464% (12402/24576)\n",
      "Loss: 1.379 | Acc: 50.434% (12427/24640)\n",
      "Loss: 1.379 | Acc: 50.425% (12457/24704)\n",
      "Loss: 1.380 | Acc: 50.412% (12486/24768)\n",
      "Loss: 1.379 | Acc: 50.411% (12518/24832)\n",
      "Loss: 1.379 | Acc: 50.426% (12554/24896)\n",
      "Loss: 1.379 | Acc: 50.413% (12583/24960)\n",
      "Loss: 1.379 | Acc: 50.436% (12621/25024)\n",
      "Loss: 1.379 | Acc: 50.450% (12657/25088)\n",
      "Loss: 1.379 | Acc: 50.477% (12696/25152)\n",
      "Loss: 1.379 | Acc: 50.452% (12722/25216)\n",
      "Loss: 1.379 | Acc: 50.459% (12756/25280)\n",
      "Loss: 1.378 | Acc: 50.446% (12785/25344)\n",
      "Loss: 1.378 | Acc: 50.472% (12824/25408)\n",
      "Loss: 1.378 | Acc: 50.444% (12849/25472)\n",
      "Loss: 1.378 | Acc: 50.443% (12881/25536)\n",
      "Loss: 1.378 | Acc: 50.461% (12918/25600)\n",
      "Loss: 1.378 | Acc: 50.448% (12947/25664)\n",
      "Loss: 1.378 | Acc: 50.443% (12978/25728)\n",
      "Loss: 1.379 | Acc: 50.446% (13011/25792)\n",
      "Loss: 1.378 | Acc: 50.445% (13043/25856)\n",
      "Loss: 1.378 | Acc: 50.444% (13075/25920)\n",
      "Loss: 1.378 | Acc: 50.446% (13108/25984)\n",
      "Loss: 1.378 | Acc: 50.449% (13141/26048)\n",
      "Loss: 1.378 | Acc: 50.456% (13175/26112)\n",
      "Loss: 1.378 | Acc: 50.436% (13202/26176)\n",
      "Loss: 1.377 | Acc: 50.446% (13237/26240)\n",
      "Loss: 1.378 | Acc: 50.449% (13270/26304)\n",
      "Loss: 1.378 | Acc: 50.451% (13303/26368)\n",
      "Loss: 1.378 | Acc: 50.465% (13339/26432)\n",
      "Loss: 1.377 | Acc: 50.502% (13381/26496)\n",
      "Loss: 1.377 | Acc: 50.493% (13411/26560)\n",
      "Loss: 1.378 | Acc: 50.466% (13436/26624)\n",
      "Loss: 1.378 | Acc: 50.465% (13468/26688)\n",
      "Loss: 1.378 | Acc: 50.452% (13497/26752)\n",
      "Loss: 1.378 | Acc: 50.440% (13526/26816)\n",
      "Loss: 1.378 | Acc: 50.432% (13556/26880)\n",
      "Loss: 1.377 | Acc: 50.449% (13593/26944)\n",
      "Loss: 1.377 | Acc: 50.463% (13629/27008)\n",
      "Loss: 1.377 | Acc: 50.465% (13662/27072)\n",
      "Loss: 1.377 | Acc: 50.450% (13690/27136)\n",
      "Loss: 1.377 | Acc: 50.452% (13723/27200)\n",
      "Loss: 1.376 | Acc: 50.458% (13757/27264)\n",
      "Loss: 1.376 | Acc: 50.450% (13787/27328)\n",
      "Loss: 1.376 | Acc: 50.431% (13814/27392)\n",
      "Loss: 1.376 | Acc: 50.422% (13844/27456)\n",
      "Loss: 1.376 | Acc: 50.425% (13877/27520)\n",
      "Loss: 1.376 | Acc: 50.410% (13905/27584)\n",
      "Loss: 1.376 | Acc: 50.420% (13940/27648)\n",
      "Loss: 1.376 | Acc: 50.440% (13978/27712)\n",
      "Loss: 1.375 | Acc: 50.436% (14009/27776)\n",
      "Loss: 1.375 | Acc: 50.456% (14047/27840)\n",
      "Loss: 1.375 | Acc: 50.459% (14080/27904)\n",
      "Loss: 1.374 | Acc: 50.497% (14123/27968)\n",
      "Loss: 1.374 | Acc: 50.507% (14158/28032)\n",
      "Loss: 1.373 | Acc: 50.505% (14190/28096)\n",
      "Loss: 1.373 | Acc: 50.533% (14230/28160)\n",
      "Loss: 1.373 | Acc: 50.528% (14261/28224)\n",
      "Loss: 1.373 | Acc: 50.530% (14294/28288)\n",
      "Loss: 1.373 | Acc: 50.529% (14326/28352)\n",
      "Loss: 1.373 | Acc: 50.524% (14357/28416)\n",
      "Loss: 1.373 | Acc: 50.537% (14393/28480)\n",
      "Loss: 1.373 | Acc: 50.529% (14423/28544)\n",
      "Loss: 1.373 | Acc: 50.517% (14452/28608)\n",
      "Loss: 1.373 | Acc: 50.534% (14489/28672)\n",
      "Loss: 1.373 | Acc: 50.539% (14523/28736)\n",
      "Loss: 1.373 | Acc: 50.559% (14561/28800)\n",
      "Loss: 1.373 | Acc: 50.554% (14592/28864)\n",
      "Loss: 1.373 | Acc: 50.550% (14623/28928)\n",
      "Loss: 1.373 | Acc: 50.555% (14657/28992)\n",
      "Loss: 1.373 | Acc: 50.575% (14695/29056)\n",
      "Loss: 1.373 | Acc: 50.584% (14730/29120)\n",
      "Loss: 1.372 | Acc: 50.593% (14765/29184)\n",
      "Loss: 1.372 | Acc: 50.578% (14793/29248)\n",
      "Loss: 1.372 | Acc: 50.587% (14828/29312)\n",
      "Loss: 1.372 | Acc: 50.589% (14861/29376)\n",
      "Loss: 1.373 | Acc: 50.567% (14887/29440)\n",
      "Loss: 1.373 | Acc: 50.556% (14916/29504)\n",
      "Loss: 1.373 | Acc: 50.558% (14949/29568)\n",
      "Loss: 1.373 | Acc: 50.553% (14980/29632)\n",
      "Loss: 1.373 | Acc: 50.552% (15012/29696)\n",
      "Loss: 1.373 | Acc: 50.548% (15043/29760)\n",
      "Loss: 1.373 | Acc: 50.526% (15069/29824)\n",
      "Loss: 1.373 | Acc: 50.539% (15105/29888)\n",
      "Loss: 1.373 | Acc: 50.507% (15128/29952)\n",
      "Loss: 1.373 | Acc: 50.520% (15164/30016)\n",
      "Loss: 1.372 | Acc: 50.549% (15205/30080)\n",
      "Loss: 1.373 | Acc: 50.557% (15240/30144)\n",
      "Loss: 1.372 | Acc: 50.566% (15275/30208)\n",
      "Loss: 1.372 | Acc: 50.588% (15314/30272)\n",
      "Loss: 1.371 | Acc: 50.607% (15352/30336)\n",
      "Loss: 1.371 | Acc: 50.622% (15389/30400)\n",
      "Loss: 1.371 | Acc: 50.624% (15422/30464)\n",
      "Loss: 1.371 | Acc: 50.635% (15458/30528)\n",
      "Loss: 1.371 | Acc: 50.647% (15494/30592)\n",
      "Loss: 1.370 | Acc: 50.646% (15526/30656)\n",
      "Loss: 1.370 | Acc: 50.654% (15561/30720)\n",
      "Loss: 1.370 | Acc: 50.646% (15591/30784)\n",
      "Loss: 1.370 | Acc: 50.613% (15613/30848)\n",
      "Loss: 1.371 | Acc: 50.611% (15645/30912)\n",
      "Loss: 1.371 | Acc: 50.600% (15674/30976)\n",
      "Loss: 1.371 | Acc: 50.615% (15711/31040)\n",
      "Loss: 1.370 | Acc: 50.627% (15747/31104)\n",
      "Loss: 1.370 | Acc: 50.645% (15785/31168)\n",
      "Loss: 1.370 | Acc: 50.628% (15812/31232)\n",
      "Loss: 1.370 | Acc: 50.620% (15842/31296)\n",
      "Loss: 1.370 | Acc: 50.622% (15875/31360)\n",
      "Loss: 1.370 | Acc: 50.624% (15908/31424)\n",
      "Loss: 1.370 | Acc: 50.654% (15950/31488)\n",
      "Loss: 1.370 | Acc: 50.650% (15981/31552)\n",
      "Loss: 1.369 | Acc: 50.648% (16013/31616)\n",
      "Loss: 1.369 | Acc: 50.638% (16042/31680)\n",
      "Loss: 1.370 | Acc: 50.627% (16071/31744)\n",
      "Loss: 1.370 | Acc: 50.622% (16102/31808)\n",
      "Loss: 1.370 | Acc: 50.628% (16136/31872)\n",
      "Loss: 1.370 | Acc: 50.626% (16168/31936)\n",
      "Loss: 1.370 | Acc: 50.606% (16194/32000)\n",
      "Loss: 1.370 | Acc: 50.614% (16229/32064)\n",
      "Loss: 1.370 | Acc: 50.604% (16258/32128)\n",
      "Loss: 1.370 | Acc: 50.596% (16288/32192)\n",
      "Loss: 1.370 | Acc: 50.601% (16322/32256)\n",
      "Loss: 1.370 | Acc: 50.603% (16355/32320)\n",
      "Loss: 1.370 | Acc: 50.593% (16384/32384)\n",
      "Loss: 1.370 | Acc: 50.579% (16412/32448)\n",
      "Loss: 1.370 | Acc: 50.563% (16439/32512)\n",
      "Loss: 1.370 | Acc: 50.571% (16474/32576)\n",
      "Loss: 1.370 | Acc: 50.585% (16511/32640)\n",
      "Loss: 1.371 | Acc: 50.563% (16536/32704)\n",
      "Loss: 1.370 | Acc: 50.580% (16574/32768)\n",
      "Loss: 1.370 | Acc: 50.591% (16610/32832)\n",
      "Loss: 1.371 | Acc: 50.565% (16634/32896)\n",
      "Loss: 1.371 | Acc: 50.564% (16666/32960)\n",
      "Loss: 1.371 | Acc: 50.566% (16699/33024)\n",
      "Loss: 1.370 | Acc: 50.586% (16738/33088)\n",
      "Loss: 1.370 | Acc: 50.582% (16769/33152)\n",
      "Loss: 1.370 | Acc: 50.569% (16797/33216)\n",
      "Loss: 1.370 | Acc: 50.598% (16839/33280)\n",
      "Loss: 1.370 | Acc: 50.597% (16871/33344)\n",
      "Loss: 1.370 | Acc: 50.590% (16901/33408)\n",
      "Loss: 1.370 | Acc: 50.601% (16937/33472)\n",
      "Loss: 1.370 | Acc: 50.587% (16965/33536)\n",
      "Loss: 1.370 | Acc: 50.583% (16996/33600)\n",
      "Loss: 1.370 | Acc: 50.597% (17033/33664)\n",
      "Loss: 1.370 | Acc: 50.587% (17062/33728)\n",
      "Loss: 1.369 | Acc: 50.601% (17099/33792)\n",
      "Loss: 1.369 | Acc: 50.597% (17130/33856)\n",
      "Loss: 1.369 | Acc: 50.601% (17164/33920)\n",
      "Loss: 1.369 | Acc: 50.609% (17199/33984)\n",
      "Loss: 1.369 | Acc: 50.614% (17233/34048)\n",
      "Loss: 1.369 | Acc: 50.613% (17265/34112)\n",
      "Loss: 1.368 | Acc: 50.623% (17301/34176)\n",
      "Loss: 1.368 | Acc: 50.628% (17335/34240)\n",
      "Loss: 1.368 | Acc: 50.641% (17372/34304)\n",
      "Loss: 1.368 | Acc: 50.652% (17408/34368)\n",
      "Loss: 1.368 | Acc: 50.665% (17445/34432)\n",
      "Loss: 1.367 | Acc: 50.670% (17479/34496)\n",
      "Loss: 1.367 | Acc: 50.660% (17508/34560)\n",
      "Loss: 1.367 | Acc: 50.670% (17544/34624)\n",
      "Loss: 1.367 | Acc: 50.663% (17574/34688)\n",
      "Loss: 1.367 | Acc: 50.670% (17609/34752)\n",
      "Loss: 1.367 | Acc: 50.663% (17639/34816)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.367 | Acc: 50.668% (17673/34880)\n",
      "Loss: 1.367 | Acc: 50.667% (17705/34944)\n",
      "Loss: 1.368 | Acc: 50.668% (17738/35008)\n",
      "Loss: 1.368 | Acc: 50.653% (17765/35072)\n",
      "Loss: 1.367 | Acc: 50.655% (17798/35136)\n",
      "Loss: 1.368 | Acc: 50.645% (17827/35200)\n",
      "Loss: 1.367 | Acc: 50.649% (17861/35264)\n",
      "Loss: 1.368 | Acc: 50.628% (17886/35328)\n",
      "Loss: 1.367 | Acc: 50.658% (17929/35392)\n",
      "Loss: 1.368 | Acc: 50.649% (17958/35456)\n",
      "Loss: 1.368 | Acc: 50.648% (17990/35520)\n",
      "Loss: 1.368 | Acc: 50.646% (18022/35584)\n",
      "Loss: 1.368 | Acc: 50.648% (18055/35648)\n",
      "Loss: 1.368 | Acc: 50.638% (18084/35712)\n",
      "Loss: 1.368 | Acc: 50.648% (18120/35776)\n",
      "Loss: 1.368 | Acc: 50.653% (18154/35840)\n",
      "Loss: 1.368 | Acc: 50.657% (18188/35904)\n",
      "Loss: 1.367 | Acc: 50.670% (18225/35968)\n",
      "Loss: 1.368 | Acc: 50.663% (18255/36032)\n",
      "Loss: 1.368 | Acc: 50.640% (18279/36096)\n",
      "Loss: 1.368 | Acc: 50.622% (18305/36160)\n",
      "Loss: 1.368 | Acc: 50.599% (18329/36224)\n",
      "Loss: 1.369 | Acc: 50.587% (18357/36288)\n",
      "Loss: 1.368 | Acc: 50.575% (18385/36352)\n",
      "Loss: 1.368 | Acc: 50.574% (18417/36416)\n",
      "Loss: 1.368 | Acc: 50.573% (18449/36480)\n",
      "Loss: 1.369 | Acc: 50.550% (18473/36544)\n",
      "Loss: 1.369 | Acc: 50.552% (18506/36608)\n",
      "Loss: 1.369 | Acc: 50.545% (18536/36672)\n",
      "Loss: 1.369 | Acc: 50.553% (18571/36736)\n",
      "Loss: 1.369 | Acc: 50.549% (18602/36800)\n",
      "Loss: 1.369 | Acc: 50.556% (18637/36864)\n",
      "Loss: 1.369 | Acc: 50.558% (18670/36928)\n",
      "Loss: 1.369 | Acc: 50.549% (18699/36992)\n",
      "Loss: 1.369 | Acc: 50.561% (18736/37056)\n",
      "Loss: 1.369 | Acc: 50.563% (18769/37120)\n",
      "Loss: 1.370 | Acc: 50.567% (18803/37184)\n",
      "Loss: 1.370 | Acc: 50.564% (18834/37248)\n",
      "Loss: 1.369 | Acc: 50.587% (18875/37312)\n",
      "Loss: 1.369 | Acc: 50.602% (18913/37376)\n",
      "Loss: 1.369 | Acc: 50.590% (18941/37440)\n",
      "Loss: 1.369 | Acc: 50.592% (18974/37504)\n",
      "Loss: 1.369 | Acc: 50.604% (19011/37568)\n",
      "Loss: 1.369 | Acc: 50.622% (19050/37632)\n",
      "Loss: 1.369 | Acc: 50.634% (19087/37696)\n",
      "Loss: 1.369 | Acc: 50.633% (19119/37760)\n",
      "Loss: 1.369 | Acc: 50.637% (19153/37824)\n",
      "Loss: 1.369 | Acc: 50.652% (19191/37888)\n",
      "Loss: 1.369 | Acc: 50.661% (19227/37952)\n",
      "Loss: 1.369 | Acc: 50.663% (19260/38016)\n",
      "Loss: 1.369 | Acc: 50.678% (19298/38080)\n",
      "Loss: 1.369 | Acc: 50.676% (19330/38144)\n",
      "Loss: 1.369 | Acc: 50.694% (19369/38208)\n",
      "Loss: 1.368 | Acc: 50.687% (19399/38272)\n",
      "Loss: 1.368 | Acc: 50.694% (19434/38336)\n",
      "Loss: 1.368 | Acc: 50.708% (19472/38400)\n",
      "Loss: 1.368 | Acc: 50.710% (19505/38464)\n",
      "Loss: 1.368 | Acc: 50.703% (19535/38528)\n",
      "Loss: 1.368 | Acc: 50.702% (19567/38592)\n",
      "Loss: 1.368 | Acc: 50.709% (19602/38656)\n",
      "Loss: 1.368 | Acc: 50.705% (19633/38720)\n",
      "Loss: 1.368 | Acc: 50.714% (19669/38784)\n",
      "Loss: 1.368 | Acc: 50.716% (19702/38848)\n",
      "Loss: 1.368 | Acc: 50.717% (19735/38912)\n",
      "Loss: 1.368 | Acc: 50.713% (19766/38976)\n",
      "Loss: 1.368 | Acc: 50.717% (19800/39040)\n",
      "Loss: 1.367 | Acc: 50.724% (19835/39104)\n",
      "Loss: 1.367 | Acc: 50.712% (19863/39168)\n",
      "Loss: 1.367 | Acc: 50.716% (19897/39232)\n",
      "Loss: 1.368 | Acc: 50.720% (19931/39296)\n",
      "Loss: 1.367 | Acc: 50.719% (19963/39360)\n",
      "Loss: 1.367 | Acc: 50.731% (20000/39424)\n",
      "Loss: 1.368 | Acc: 50.717% (20027/39488)\n",
      "Loss: 1.368 | Acc: 50.708% (20056/39552)\n",
      "Loss: 1.368 | Acc: 50.702% (20086/39616)\n",
      "Loss: 1.368 | Acc: 50.698% (20117/39680)\n",
      "Loss: 1.368 | Acc: 50.689% (20146/39744)\n",
      "Loss: 1.368 | Acc: 50.683% (20176/39808)\n",
      "Loss: 1.368 | Acc: 50.690% (20211/39872)\n",
      "Loss: 1.368 | Acc: 50.701% (20248/39936)\n",
      "Loss: 1.368 | Acc: 50.708% (20283/40000)\n",
      "Loss: 1.368 | Acc: 50.704% (20314/40064)\n",
      "Loss: 1.368 | Acc: 50.705% (20347/40128)\n",
      "Loss: 1.368 | Acc: 50.727% (20388/40192)\n",
      "Loss: 1.368 | Acc: 50.735% (20424/40256)\n",
      "Loss: 1.368 | Acc: 50.727% (20453/40320)\n",
      "Loss: 1.368 | Acc: 50.716% (20481/40384)\n",
      "Loss: 1.368 | Acc: 50.717% (20514/40448)\n",
      "Loss: 1.367 | Acc: 50.731% (20552/40512)\n",
      "Loss: 1.367 | Acc: 50.734% (20586/40576)\n",
      "Loss: 1.367 | Acc: 50.716% (20611/40640)\n",
      "Loss: 1.367 | Acc: 50.712% (20642/40704)\n",
      "Loss: 1.367 | Acc: 50.724% (20679/40768)\n",
      "Loss: 1.367 | Acc: 50.725% (20712/40832)\n",
      "Loss: 1.367 | Acc: 50.721% (20743/40896)\n",
      "Loss: 1.367 | Acc: 50.730% (20779/40960)\n",
      "Loss: 1.367 | Acc: 50.739% (20815/41024)\n",
      "Loss: 1.367 | Acc: 50.742% (20849/41088)\n",
      "Loss: 1.366 | Acc: 50.751% (20885/41152)\n",
      "Loss: 1.366 | Acc: 50.750% (20917/41216)\n",
      "Loss: 1.366 | Acc: 50.753% (20951/41280)\n",
      "Loss: 1.366 | Acc: 50.750% (20982/41344)\n",
      "Loss: 1.366 | Acc: 50.744% (21012/41408)\n",
      "Loss: 1.366 | Acc: 50.745% (21045/41472)\n",
      "Loss: 1.366 | Acc: 50.737% (21074/41536)\n",
      "Loss: 1.366 | Acc: 50.748% (21111/41600)\n",
      "Loss: 1.366 | Acc: 50.746% (21143/41664)\n",
      "Loss: 1.366 | Acc: 50.752% (21178/41728)\n",
      "Loss: 1.366 | Acc: 50.747% (21208/41792)\n",
      "Loss: 1.365 | Acc: 50.748% (21241/41856)\n",
      "Loss: 1.365 | Acc: 50.756% (21277/41920)\n",
      "Loss: 1.365 | Acc: 50.776% (21318/41984)\n",
      "Loss: 1.365 | Acc: 50.763% (21345/42048)\n",
      "Loss: 1.365 | Acc: 50.762% (21377/42112)\n",
      "Loss: 1.365 | Acc: 50.752% (21405/42176)\n",
      "Loss: 1.365 | Acc: 50.743% (21434/42240)\n",
      "Loss: 1.365 | Acc: 50.747% (21468/42304)\n",
      "Loss: 1.365 | Acc: 50.758% (21505/42368)\n",
      "Loss: 1.365 | Acc: 50.757% (21537/42432)\n",
      "Loss: 1.365 | Acc: 50.753% (21568/42496)\n",
      "Loss: 1.365 | Acc: 50.752% (21600/42560)\n",
      "Loss: 1.365 | Acc: 50.744% (21629/42624)\n",
      "Loss: 1.365 | Acc: 50.754% (21666/42688)\n",
      "Loss: 1.365 | Acc: 50.753% (21698/42752)\n",
      "Loss: 1.365 | Acc: 50.752% (21730/42816)\n",
      "Loss: 1.365 | Acc: 50.758% (21765/42880)\n",
      "Loss: 1.365 | Acc: 50.764% (21800/42944)\n",
      "Loss: 1.365 | Acc: 50.774% (21837/43008)\n",
      "Loss: 1.365 | Acc: 50.768% (21867/43072)\n",
      "Loss: 1.365 | Acc: 50.749% (21891/43136)\n",
      "Loss: 1.365 | Acc: 50.762% (21929/43200)\n",
      "Loss: 1.365 | Acc: 50.737% (21951/43264)\n",
      "Loss: 1.365 | Acc: 50.725% (21978/43328)\n",
      "Loss: 1.365 | Acc: 50.726% (22011/43392)\n",
      "Loss: 1.365 | Acc: 50.739% (22049/43456)\n",
      "Loss: 1.366 | Acc: 50.724% (22075/43520)\n",
      "Loss: 1.366 | Acc: 50.718% (22105/43584)\n",
      "Loss: 1.366 | Acc: 50.719% (22138/43648)\n",
      "Loss: 1.366 | Acc: 50.718% (22170/43712)\n",
      "Loss: 1.365 | Acc: 50.713% (22200/43776)\n",
      "Loss: 1.365 | Acc: 50.719% (22235/43840)\n",
      "Loss: 1.365 | Acc: 50.713% (22265/43904)\n",
      "Loss: 1.365 | Acc: 50.705% (22294/43968)\n",
      "Loss: 1.365 | Acc: 50.724% (22335/44032)\n",
      "Loss: 1.365 | Acc: 50.726% (22368/44096)\n",
      "Loss: 1.366 | Acc: 50.722% (22399/44160)\n",
      "Loss: 1.365 | Acc: 50.735% (22437/44224)\n",
      "Loss: 1.365 | Acc: 50.754% (22478/44288)\n",
      "Loss: 1.365 | Acc: 50.758% (22512/44352)\n",
      "Loss: 1.365 | Acc: 50.763% (22547/44416)\n",
      "Loss: 1.365 | Acc: 50.771% (22583/44480)\n",
      "Loss: 1.364 | Acc: 50.779% (22619/44544)\n",
      "Loss: 1.364 | Acc: 50.787% (22655/44608)\n",
      "Loss: 1.364 | Acc: 50.795% (22691/44672)\n",
      "Loss: 1.364 | Acc: 50.802% (22727/44736)\n",
      "Loss: 1.364 | Acc: 50.799% (22758/44800)\n",
      "Loss: 1.364 | Acc: 50.789% (22786/44864)\n",
      "Loss: 1.364 | Acc: 50.783% (22816/44928)\n",
      "Loss: 1.364 | Acc: 50.771% (22843/44992)\n",
      "Loss: 1.364 | Acc: 50.763% (22872/45056)\n",
      "Loss: 1.364 | Acc: 50.769% (22907/45120)\n",
      "Loss: 1.364 | Acc: 50.761% (22936/45184)\n",
      "Loss: 1.364 | Acc: 50.769% (22972/45248)\n",
      "Loss: 1.364 | Acc: 50.772% (23006/45312)\n",
      "Loss: 1.364 | Acc: 50.767% (23036/45376)\n",
      "Loss: 1.364 | Acc: 50.779% (23074/45440)\n",
      "Loss: 1.364 | Acc: 50.800% (23116/45504)\n",
      "Loss: 1.363 | Acc: 50.801% (23149/45568)\n",
      "Loss: 1.363 | Acc: 50.798% (23180/45632)\n",
      "Loss: 1.363 | Acc: 50.808% (23217/45696)\n",
      "Loss: 1.363 | Acc: 50.800% (23246/45760)\n",
      "Loss: 1.363 | Acc: 50.803% (23280/45824)\n",
      "Loss: 1.363 | Acc: 50.798% (23310/45888)\n",
      "Loss: 1.363 | Acc: 50.792% (23340/45952)\n",
      "Loss: 1.363 | Acc: 50.791% (23372/46016)\n",
      "Loss: 1.363 | Acc: 50.807% (23412/46080)\n",
      "Loss: 1.363 | Acc: 50.811% (23446/46144)\n",
      "Loss: 1.363 | Acc: 50.803% (23475/46208)\n",
      "Loss: 1.363 | Acc: 50.802% (23507/46272)\n",
      "Loss: 1.363 | Acc: 50.794% (23536/46336)\n",
      "Loss: 1.363 | Acc: 50.802% (23572/46400)\n",
      "Loss: 1.363 | Acc: 50.809% (23608/46464)\n",
      "Loss: 1.362 | Acc: 50.812% (23642/46528)\n",
      "Loss: 1.362 | Acc: 50.811% (23674/46592)\n",
      "Loss: 1.362 | Acc: 50.817% (23709/46656)\n",
      "Loss: 1.362 | Acc: 50.815% (23741/46720)\n",
      "Loss: 1.362 | Acc: 50.834% (23782/46784)\n",
      "Loss: 1.362 | Acc: 50.843% (23819/46848)\n",
      "Loss: 1.362 | Acc: 50.838% (23849/46912)\n",
      "Loss: 1.361 | Acc: 50.837% (23881/46976)\n",
      "Loss: 1.361 | Acc: 50.840% (23915/47040)\n",
      "Loss: 1.361 | Acc: 50.851% (23953/47104)\n",
      "Loss: 1.361 | Acc: 50.857% (23988/47168)\n",
      "Loss: 1.361 | Acc: 50.862% (24023/47232)\n",
      "Loss: 1.361 | Acc: 50.854% (24052/47296)\n",
      "Loss: 1.361 | Acc: 50.859% (24087/47360)\n",
      "Loss: 1.361 | Acc: 50.856% (24118/47424)\n",
      "Loss: 1.361 | Acc: 50.872% (24158/47488)\n",
      "Loss: 1.361 | Acc: 50.862% (24186/47552)\n",
      "Loss: 1.361 | Acc: 50.863% (24219/47616)\n",
      "Loss: 1.360 | Acc: 50.872% (24256/47680)\n",
      "Loss: 1.360 | Acc: 50.878% (24291/47744)\n",
      "Loss: 1.360 | Acc: 50.876% (24323/47808)\n",
      "Loss: 1.360 | Acc: 50.879% (24357/47872)\n",
      "Loss: 1.360 | Acc: 50.868% (24384/47936)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.360 | Acc: 50.881% (24423/48000)\n",
      "Loss: 1.360 | Acc: 50.878% (24454/48064)\n",
      "Loss: 1.360 | Acc: 50.889% (24492/48128)\n",
      "Loss: 1.360 | Acc: 50.882% (24521/48192)\n",
      "Loss: 1.360 | Acc: 50.889% (24557/48256)\n",
      "Loss: 1.359 | Acc: 50.890% (24590/48320)\n",
      "Loss: 1.359 | Acc: 50.893% (24624/48384)\n",
      "Loss: 1.359 | Acc: 50.888% (24654/48448)\n",
      "Loss: 1.359 | Acc: 50.888% (24687/48512)\n",
      "Loss: 1.359 | Acc: 50.896% (24723/48576)\n",
      "Loss: 1.359 | Acc: 50.894% (24755/48640)\n",
      "Loss: 1.359 | Acc: 50.914% (24797/48704)\n",
      "Loss: 1.359 | Acc: 50.919% (24832/48768)\n",
      "Loss: 1.359 | Acc: 50.928% (24869/48832)\n",
      "Loss: 1.359 | Acc: 50.941% (24908/48896)\n",
      "Loss: 1.359 | Acc: 50.942% (24941/48960)\n",
      "Loss: 1.358 | Acc: 50.943% (24962/49000)\n",
      "Epoch 1 of training is completed, Training accuracy for this epoch is 50.94285714285714\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.263 | Acc: 51.562% (33/64)\n",
      "Loss: 1.233 | Acc: 55.469% (71/128)\n",
      "Loss: 1.270 | Acc: 55.729% (107/192)\n",
      "Loss: 1.380 | Acc: 51.562% (132/256)\n",
      "Loss: 1.348 | Acc: 51.250% (164/320)\n",
      "Loss: 1.366 | Acc: 51.042% (196/384)\n",
      "Loss: 1.382 | Acc: 50.000% (224/448)\n",
      "Loss: 1.356 | Acc: 50.391% (258/512)\n",
      "Loss: 1.330 | Acc: 51.389% (296/576)\n",
      "Loss: 1.312 | Acc: 52.188% (334/640)\n",
      "Loss: 1.314 | Acc: 52.415% (369/704)\n",
      "Loss: 1.323 | Acc: 52.214% (401/768)\n",
      "Loss: 1.319 | Acc: 52.764% (439/832)\n",
      "Loss: 1.312 | Acc: 52.902% (474/896)\n",
      "Loss: 1.296 | Acc: 53.438% (513/960)\n",
      "Loss: 1.284 | Acc: 54.004% (553/1024)\n",
      "Loss: 1.288 | Acc: 53.676% (584/1088)\n",
      "Loss: 1.280 | Acc: 53.819% (620/1152)\n",
      "Loss: 1.280 | Acc: 54.030% (657/1216)\n",
      "Loss: 1.301 | Acc: 53.281% (682/1280)\n",
      "Loss: 1.300 | Acc: 53.125% (714/1344)\n",
      "Loss: 1.301 | Acc: 52.841% (744/1408)\n",
      "Loss: 1.292 | Acc: 52.989% (780/1472)\n",
      "Loss: 1.298 | Acc: 52.669% (809/1536)\n",
      "Loss: 1.298 | Acc: 52.812% (845/1600)\n",
      "Loss: 1.301 | Acc: 52.704% (877/1664)\n",
      "Loss: 1.302 | Acc: 52.720% (911/1728)\n",
      "Loss: 1.300 | Acc: 52.511% (941/1792)\n",
      "Loss: 1.300 | Acc: 52.586% (976/1856)\n",
      "Loss: 1.307 | Acc: 52.812% (1014/1920)\n",
      "Loss: 1.307 | Acc: 52.873% (1049/1984)\n",
      "Loss: 1.307 | Acc: 52.979% (1085/2048)\n",
      "Loss: 1.296 | Acc: 53.314% (1126/2112)\n",
      "Loss: 1.299 | Acc: 53.033% (1154/2176)\n",
      "Loss: 1.296 | Acc: 53.348% (1195/2240)\n",
      "Loss: 1.296 | Acc: 53.385% (1230/2304)\n",
      "Loss: 1.300 | Acc: 53.336% (1263/2368)\n",
      "Loss: 1.302 | Acc: 53.207% (1294/2432)\n",
      "Loss: 1.306 | Acc: 53.125% (1326/2496)\n",
      "Loss: 1.317 | Acc: 52.930% (1355/2560)\n",
      "Loss: 1.317 | Acc: 52.973% (1390/2624)\n",
      "Loss: 1.317 | Acc: 52.939% (1423/2688)\n",
      "Loss: 1.315 | Acc: 53.089% (1461/2752)\n",
      "Loss: 1.314 | Acc: 53.054% (1494/2816)\n",
      "Loss: 1.313 | Acc: 52.986% (1526/2880)\n",
      "Loss: 1.311 | Acc: 53.091% (1563/2944)\n",
      "Loss: 1.308 | Acc: 53.258% (1602/3008)\n",
      "Loss: 1.308 | Acc: 53.190% (1634/3072)\n",
      "Loss: 1.306 | Acc: 53.284% (1671/3136)\n",
      "Loss: 1.306 | Acc: 53.281% (1705/3200)\n",
      "Loss: 1.303 | Acc: 53.309% (1740/3264)\n",
      "Loss: 1.304 | Acc: 53.305% (1774/3328)\n",
      "Loss: 1.303 | Acc: 53.361% (1810/3392)\n",
      "Loss: 1.303 | Acc: 53.299% (1842/3456)\n",
      "Loss: 1.305 | Acc: 53.239% (1874/3520)\n",
      "Loss: 1.304 | Acc: 53.265% (1909/3584)\n",
      "Loss: 1.306 | Acc: 53.235% (1942/3648)\n",
      "Loss: 1.302 | Acc: 53.448% (1984/3712)\n",
      "Loss: 1.301 | Acc: 53.469% (2019/3776)\n",
      "Loss: 1.299 | Acc: 53.672% (2061/3840)\n",
      "Loss: 1.298 | Acc: 53.663% (2095/3904)\n",
      "Loss: 1.296 | Acc: 53.780% (2134/3968)\n",
      "Loss: 1.297 | Acc: 53.819% (2170/4032)\n",
      "Loss: 1.298 | Acc: 53.735% (2201/4096)\n",
      "Loss: 1.303 | Acc: 53.678% (2233/4160)\n",
      "Loss: 1.301 | Acc: 53.717% (2269/4224)\n",
      "Loss: 1.302 | Acc: 53.521% (2295/4288)\n",
      "Loss: 1.302 | Acc: 53.470% (2327/4352)\n",
      "Loss: 1.300 | Acc: 53.578% (2366/4416)\n",
      "Loss: 1.297 | Acc: 53.705% (2406/4480)\n",
      "Loss: 1.296 | Acc: 53.741% (2442/4544)\n",
      "Loss: 1.296 | Acc: 53.776% (2478/4608)\n",
      "Loss: 1.294 | Acc: 53.853% (2516/4672)\n",
      "Loss: 1.294 | Acc: 53.843% (2550/4736)\n",
      "Loss: 1.294 | Acc: 53.792% (2582/4800)\n",
      "Loss: 1.293 | Acc: 53.865% (2620/4864)\n",
      "Loss: 1.294 | Acc: 53.856% (2654/4928)\n",
      "Loss: 1.294 | Acc: 53.886% (2690/4992)\n",
      "Loss: 1.293 | Acc: 53.896% (2725/5056)\n",
      "Loss: 1.296 | Acc: 53.867% (2758/5120)\n",
      "Loss: 1.294 | Acc: 53.993% (2799/5184)\n",
      "Loss: 1.295 | Acc: 53.849% (2826/5248)\n",
      "Loss: 1.294 | Acc: 53.803% (2858/5312)\n",
      "Loss: 1.296 | Acc: 53.795% (2892/5376)\n",
      "Loss: 1.296 | Acc: 53.787% (2926/5440)\n",
      "Loss: 1.297 | Acc: 53.797% (2961/5504)\n",
      "Loss: 1.299 | Acc: 53.772% (2994/5568)\n",
      "Loss: 1.301 | Acc: 53.729% (3026/5632)\n",
      "Loss: 1.301 | Acc: 53.704% (3059/5696)\n",
      "Loss: 1.299 | Acc: 53.802% (3099/5760)\n",
      "Loss: 1.299 | Acc: 53.863% (3137/5824)\n",
      "Loss: 1.301 | Acc: 53.770% (3166/5888)\n",
      "Loss: 1.302 | Acc: 53.747% (3199/5952)\n",
      "Loss: 1.302 | Acc: 53.723% (3232/6016)\n",
      "Loss: 1.302 | Acc: 53.635% (3261/6080)\n",
      "Loss: 1.304 | Acc: 53.499% (3287/6144)\n",
      "Loss: 1.305 | Acc: 53.431% (3317/6208)\n",
      "Loss: 1.308 | Acc: 53.348% (3346/6272)\n",
      "Loss: 1.307 | Acc: 53.425% (3385/6336)\n",
      "Loss: 1.306 | Acc: 53.406% (3418/6400)\n",
      "Loss: 1.308 | Acc: 53.326% (3447/6464)\n",
      "Loss: 1.307 | Acc: 53.324% (3481/6528)\n",
      "Loss: 1.309 | Acc: 53.262% (3511/6592)\n",
      "Loss: 1.309 | Acc: 53.245% (3544/6656)\n",
      "Loss: 1.311 | Acc: 53.095% (3568/6720)\n",
      "Loss: 1.310 | Acc: 53.037% (3598/6784)\n",
      "Loss: 1.309 | Acc: 53.081% (3635/6848)\n",
      "Loss: 1.311 | Acc: 53.038% (3666/6912)\n",
      "Loss: 1.312 | Acc: 52.996% (3697/6976)\n",
      "Loss: 1.313 | Acc: 52.955% (3728/7040)\n",
      "Loss: 1.313 | Acc: 52.998% (3765/7104)\n",
      "Loss: 1.314 | Acc: 52.985% (3798/7168)\n",
      "Loss: 1.314 | Acc: 52.904% (3826/7232)\n",
      "Loss: 1.313 | Acc: 52.906% (3860/7296)\n",
      "Loss: 1.310 | Acc: 53.043% (3904/7360)\n",
      "Loss: 1.312 | Acc: 52.963% (3932/7424)\n",
      "Loss: 1.310 | Acc: 53.032% (3971/7488)\n",
      "Loss: 1.309 | Acc: 53.072% (4008/7552)\n",
      "Loss: 1.310 | Acc: 53.046% (4040/7616)\n",
      "Loss: 1.309 | Acc: 53.073% (4076/7680)\n",
      "Loss: 1.308 | Acc: 53.073% (4110/7744)\n",
      "Loss: 1.307 | Acc: 53.138% (4149/7808)\n",
      "Loss: 1.308 | Acc: 53.087% (4179/7872)\n",
      "Loss: 1.309 | Acc: 53.075% (4212/7936)\n",
      "Loss: 1.311 | Acc: 53.100% (4248/8000)\n",
      "Loss: 1.311 | Acc: 53.100% (4282/8064)\n",
      "Loss: 1.312 | Acc: 53.063% (4313/8128)\n",
      "Loss: 1.311 | Acc: 53.101% (4350/8192)\n",
      "Loss: 1.311 | Acc: 53.052% (4380/8256)\n",
      "Loss: 1.314 | Acc: 52.933% (4404/8320)\n",
      "Loss: 1.313 | Acc: 53.006% (4444/8384)\n",
      "Loss: 1.312 | Acc: 53.007% (4478/8448)\n",
      "Loss: 1.312 | Acc: 53.043% (4515/8512)\n",
      "Loss: 1.312 | Acc: 52.997% (4545/8576)\n",
      "Loss: 1.313 | Acc: 52.963% (4576/8640)\n",
      "Loss: 1.315 | Acc: 52.872% (4602/8704)\n",
      "Loss: 1.314 | Acc: 52.920% (4640/8768)\n",
      "Loss: 1.314 | Acc: 52.921% (4674/8832)\n",
      "Loss: 1.312 | Acc: 52.990% (4714/8896)\n",
      "Loss: 1.312 | Acc: 52.991% (4748/8960)\n",
      "Loss: 1.311 | Acc: 53.059% (4788/9024)\n",
      "Loss: 1.311 | Acc: 52.993% (4816/9088)\n",
      "Loss: 1.311 | Acc: 52.950% (4846/9152)\n",
      "Loss: 1.310 | Acc: 53.006% (4885/9216)\n",
      "Loss: 1.309 | Acc: 53.028% (4921/9280)\n",
      "Loss: 1.311 | Acc: 52.954% (4948/9344)\n",
      "Loss: 1.312 | Acc: 52.902% (4977/9408)\n",
      "Loss: 1.312 | Acc: 52.946% (5015/9472)\n",
      "Loss: 1.311 | Acc: 52.968% (5051/9536)\n",
      "Loss: 1.310 | Acc: 52.979% (5086/9600)\n",
      "Loss: 1.311 | Acc: 52.949% (5117/9664)\n",
      "Loss: 1.310 | Acc: 52.971% (5153/9728)\n",
      "Loss: 1.312 | Acc: 52.911% (5181/9792)\n",
      "Loss: 1.312 | Acc: 52.881% (5212/9856)\n",
      "Loss: 1.313 | Acc: 52.843% (5242/9920)\n",
      "Loss: 1.313 | Acc: 52.784% (5270/9984)\n",
      "Loss: 1.314 | Acc: 52.740% (5274/10000)\n",
      "Evaluation of Epoch 1 is completed, Test accuracy for this epoch is 52.74\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 0.898 | Acc: 65.625% (42/64)\n",
      "Loss: 1.126 | Acc: 60.156% (77/128)\n",
      "Loss: 1.113 | Acc: 61.979% (119/192)\n",
      "Loss: 1.170 | Acc: 59.766% (153/256)\n",
      "Loss: 1.210 | Acc: 58.125% (186/320)\n",
      "Loss: 1.205 | Acc: 58.073% (223/384)\n",
      "Loss: 1.230 | Acc: 58.259% (261/448)\n",
      "Loss: 1.231 | Acc: 58.008% (297/512)\n",
      "Loss: 1.228 | Acc: 58.160% (335/576)\n",
      "Loss: 1.260 | Acc: 57.656% (369/640)\n",
      "Loss: 1.256 | Acc: 57.386% (404/704)\n",
      "Loss: 1.249 | Acc: 57.161% (439/768)\n",
      "Loss: 1.245 | Acc: 57.332% (477/832)\n",
      "Loss: 1.246 | Acc: 57.031% (511/896)\n",
      "Loss: 1.247 | Acc: 56.667% (544/960)\n",
      "Loss: 1.254 | Acc: 56.445% (578/1024)\n",
      "Loss: 1.249 | Acc: 56.710% (617/1088)\n",
      "Loss: 1.235 | Acc: 57.292% (660/1152)\n",
      "Loss: 1.233 | Acc: 57.401% (698/1216)\n",
      "Loss: 1.233 | Acc: 57.578% (737/1280)\n",
      "Loss: 1.229 | Acc: 57.440% (772/1344)\n",
      "Loss: 1.233 | Acc: 57.102% (804/1408)\n",
      "Loss: 1.222 | Acc: 57.269% (843/1472)\n",
      "Loss: 1.229 | Acc: 56.966% (875/1536)\n",
      "Loss: 1.236 | Acc: 56.562% (905/1600)\n",
      "Loss: 1.232 | Acc: 56.671% (943/1664)\n",
      "Loss: 1.234 | Acc: 56.829% (982/1728)\n",
      "Loss: 1.231 | Acc: 56.975% (1021/1792)\n",
      "Loss: 1.233 | Acc: 56.897% (1056/1856)\n",
      "Loss: 1.240 | Acc: 56.771% (1090/1920)\n",
      "Loss: 1.241 | Acc: 56.452% (1120/1984)\n",
      "Loss: 1.238 | Acc: 56.641% (1160/2048)\n",
      "Loss: 1.245 | Acc: 56.297% (1189/2112)\n",
      "Loss: 1.239 | Acc: 56.526% (1230/2176)\n",
      "Loss: 1.245 | Acc: 56.161% (1258/2240)\n",
      "Loss: 1.238 | Acc: 56.467% (1301/2304)\n",
      "Loss: 1.240 | Acc: 56.588% (1340/2368)\n",
      "Loss: 1.242 | Acc: 56.456% (1373/2432)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.240 | Acc: 56.530% (1411/2496)\n",
      "Loss: 1.240 | Acc: 56.445% (1445/2560)\n",
      "Loss: 1.240 | Acc: 56.441% (1481/2624)\n",
      "Loss: 1.238 | Acc: 56.473% (1518/2688)\n",
      "Loss: 1.239 | Acc: 56.395% (1552/2752)\n",
      "Loss: 1.239 | Acc: 56.357% (1587/2816)\n",
      "Loss: 1.240 | Acc: 56.285% (1621/2880)\n",
      "Loss: 1.238 | Acc: 56.284% (1657/2944)\n",
      "Loss: 1.241 | Acc: 56.084% (1687/3008)\n",
      "Loss: 1.241 | Acc: 56.087% (1723/3072)\n",
      "Loss: 1.241 | Acc: 55.995% (1756/3136)\n",
      "Loss: 1.237 | Acc: 56.188% (1798/3200)\n",
      "Loss: 1.238 | Acc: 56.189% (1834/3264)\n",
      "Loss: 1.240 | Acc: 56.070% (1866/3328)\n",
      "Loss: 1.238 | Acc: 56.191% (1906/3392)\n",
      "Loss: 1.237 | Acc: 56.076% (1938/3456)\n",
      "Loss: 1.241 | Acc: 55.852% (1966/3520)\n",
      "Loss: 1.240 | Acc: 55.831% (2001/3584)\n",
      "Loss: 1.244 | Acc: 55.592% (2028/3648)\n",
      "Loss: 1.242 | Acc: 55.738% (2069/3712)\n",
      "Loss: 1.238 | Acc: 55.932% (2112/3776)\n",
      "Loss: 1.240 | Acc: 55.911% (2147/3840)\n",
      "Loss: 1.242 | Acc: 55.738% (2176/3904)\n",
      "Loss: 1.240 | Acc: 55.771% (2213/3968)\n",
      "Loss: 1.242 | Acc: 55.754% (2248/4032)\n",
      "Loss: 1.244 | Acc: 55.640% (2279/4096)\n",
      "Loss: 1.244 | Acc: 55.553% (2311/4160)\n",
      "Loss: 1.246 | Acc: 55.492% (2344/4224)\n",
      "Loss: 1.248 | Acc: 55.340% (2373/4288)\n",
      "Loss: 1.243 | Acc: 55.538% (2417/4352)\n",
      "Loss: 1.241 | Acc: 55.571% (2454/4416)\n",
      "Loss: 1.242 | Acc: 55.558% (2489/4480)\n",
      "Loss: 1.239 | Acc: 55.590% (2526/4544)\n",
      "Loss: 1.239 | Acc: 55.556% (2560/4608)\n",
      "Loss: 1.237 | Acc: 55.608% (2598/4672)\n",
      "Loss: 1.235 | Acc: 55.722% (2639/4736)\n",
      "Loss: 1.234 | Acc: 55.750% (2676/4800)\n",
      "Loss: 1.232 | Acc: 55.921% (2720/4864)\n",
      "Loss: 1.231 | Acc: 55.905% (2755/4928)\n",
      "Loss: 1.235 | Acc: 55.749% (2783/4992)\n",
      "Loss: 1.237 | Acc: 55.617% (2812/5056)\n",
      "Loss: 1.237 | Acc: 55.605% (2847/5120)\n",
      "Loss: 1.239 | Acc: 55.536% (2879/5184)\n",
      "Loss: 1.238 | Acc: 55.602% (2918/5248)\n",
      "Loss: 1.239 | Acc: 55.535% (2950/5312)\n",
      "Loss: 1.239 | Acc: 55.543% (2986/5376)\n",
      "Loss: 1.240 | Acc: 55.496% (3019/5440)\n",
      "Loss: 1.239 | Acc: 55.632% (3062/5504)\n",
      "Loss: 1.240 | Acc: 55.532% (3092/5568)\n",
      "Loss: 1.242 | Acc: 55.504% (3126/5632)\n",
      "Loss: 1.243 | Acc: 55.425% (3157/5696)\n",
      "Loss: 1.244 | Acc: 55.451% (3194/5760)\n",
      "Loss: 1.245 | Acc: 55.391% (3226/5824)\n",
      "Loss: 1.244 | Acc: 55.367% (3260/5888)\n",
      "Loss: 1.244 | Acc: 55.309% (3292/5952)\n",
      "Loss: 1.243 | Acc: 55.352% (3330/6016)\n",
      "Loss: 1.242 | Acc: 55.493% (3374/6080)\n",
      "Loss: 1.243 | Acc: 55.469% (3408/6144)\n",
      "Loss: 1.243 | Acc: 55.461% (3443/6208)\n",
      "Loss: 1.244 | Acc: 55.421% (3476/6272)\n",
      "Loss: 1.245 | Acc: 55.398% (3510/6336)\n",
      "Loss: 1.245 | Acc: 55.500% (3552/6400)\n",
      "Loss: 1.246 | Acc: 55.507% (3588/6464)\n",
      "Loss: 1.248 | Acc: 55.377% (3615/6528)\n",
      "Loss: 1.249 | Acc: 55.294% (3645/6592)\n",
      "Loss: 1.250 | Acc: 55.258% (3678/6656)\n",
      "Loss: 1.250 | Acc: 55.298% (3716/6720)\n",
      "Loss: 1.251 | Acc: 55.321% (3753/6784)\n",
      "Loss: 1.253 | Acc: 55.272% (3785/6848)\n",
      "Loss: 1.251 | Acc: 55.324% (3824/6912)\n",
      "Loss: 1.250 | Acc: 55.404% (3865/6976)\n",
      "Loss: 1.250 | Acc: 55.440% (3903/7040)\n",
      "Loss: 1.250 | Acc: 55.476% (3941/7104)\n",
      "Loss: 1.250 | Acc: 55.469% (3976/7168)\n",
      "Loss: 1.250 | Acc: 55.407% (4007/7232)\n",
      "Loss: 1.252 | Acc: 55.332% (4037/7296)\n",
      "Loss: 1.252 | Acc: 55.367% (4075/7360)\n",
      "Loss: 1.250 | Acc: 55.415% (4114/7424)\n",
      "Loss: 1.250 | Acc: 55.382% (4147/7488)\n",
      "Loss: 1.250 | Acc: 55.403% (4184/7552)\n",
      "Loss: 1.252 | Acc: 55.331% (4214/7616)\n",
      "Loss: 1.252 | Acc: 55.339% (4250/7680)\n",
      "Loss: 1.253 | Acc: 55.191% (4274/7744)\n",
      "Loss: 1.254 | Acc: 55.174% (4308/7808)\n",
      "Loss: 1.252 | Acc: 55.234% (4348/7872)\n",
      "Loss: 1.249 | Acc: 55.330% (4391/7936)\n",
      "Loss: 1.250 | Acc: 55.237% (4419/8000)\n",
      "Loss: 1.249 | Acc: 55.295% (4459/8064)\n",
      "Loss: 1.250 | Acc: 55.266% (4492/8128)\n",
      "Loss: 1.250 | Acc: 55.273% (4528/8192)\n",
      "Loss: 1.248 | Acc: 55.354% (4570/8256)\n",
      "Loss: 1.247 | Acc: 55.397% (4609/8320)\n",
      "Loss: 1.248 | Acc: 55.367% (4642/8384)\n",
      "Loss: 1.249 | Acc: 55.303% (4672/8448)\n",
      "Loss: 1.250 | Acc: 55.334% (4710/8512)\n",
      "Loss: 1.250 | Acc: 55.259% (4739/8576)\n",
      "Loss: 1.248 | Acc: 55.336% (4781/8640)\n",
      "Loss: 1.248 | Acc: 55.308% (4814/8704)\n",
      "Loss: 1.248 | Acc: 55.303% (4849/8768)\n",
      "Loss: 1.250 | Acc: 55.276% (4882/8832)\n",
      "Loss: 1.250 | Acc: 55.283% (4918/8896)\n",
      "Loss: 1.250 | Acc: 55.246% (4950/8960)\n",
      "Loss: 1.252 | Acc: 55.219% (4983/9024)\n",
      "Loss: 1.254 | Acc: 55.161% (5013/9088)\n",
      "Loss: 1.254 | Acc: 55.168% (5049/9152)\n",
      "Loss: 1.254 | Acc: 55.165% (5084/9216)\n",
      "Loss: 1.253 | Acc: 55.216% (5124/9280)\n",
      "Loss: 1.253 | Acc: 55.169% (5155/9344)\n",
      "Loss: 1.253 | Acc: 55.198% (5193/9408)\n",
      "Loss: 1.254 | Acc: 55.205% (5229/9472)\n",
      "Loss: 1.252 | Acc: 55.233% (5267/9536)\n",
      "Loss: 1.251 | Acc: 55.302% (5309/9600)\n",
      "Loss: 1.251 | Acc: 55.288% (5343/9664)\n",
      "Loss: 1.252 | Acc: 55.253% (5375/9728)\n",
      "Loss: 1.253 | Acc: 55.249% (5410/9792)\n",
      "Loss: 1.254 | Acc: 55.235% (5444/9856)\n",
      "Loss: 1.254 | Acc: 55.212% (5477/9920)\n",
      "Loss: 1.252 | Acc: 55.218% (5513/9984)\n",
      "Loss: 1.253 | Acc: 55.165% (5543/10048)\n",
      "Loss: 1.254 | Acc: 55.123% (5574/10112)\n",
      "Loss: 1.254 | Acc: 55.130% (5610/10176)\n",
      "Loss: 1.254 | Acc: 55.127% (5645/10240)\n",
      "Loss: 1.254 | Acc: 55.105% (5678/10304)\n",
      "Loss: 1.254 | Acc: 55.073% (5710/10368)\n",
      "Loss: 1.255 | Acc: 55.052% (5743/10432)\n",
      "Loss: 1.254 | Acc: 55.097% (5783/10496)\n",
      "Loss: 1.254 | Acc: 55.123% (5821/10560)\n",
      "Loss: 1.254 | Acc: 55.017% (5845/10624)\n",
      "Loss: 1.254 | Acc: 54.987% (5877/10688)\n",
      "Loss: 1.257 | Acc: 54.920% (5905/10752)\n",
      "Loss: 1.256 | Acc: 54.928% (5941/10816)\n",
      "Loss: 1.255 | Acc: 54.926% (5976/10880)\n",
      "Loss: 1.255 | Acc: 54.971% (6016/10944)\n",
      "Loss: 1.255 | Acc: 54.951% (6049/11008)\n",
      "Loss: 1.256 | Acc: 54.977% (6087/11072)\n",
      "Loss: 1.257 | Acc: 54.957% (6120/11136)\n",
      "Loss: 1.257 | Acc: 54.929% (6152/11200)\n",
      "Loss: 1.258 | Acc: 54.892% (6183/11264)\n",
      "Loss: 1.257 | Acc: 54.891% (6218/11328)\n",
      "Loss: 1.257 | Acc: 54.907% (6255/11392)\n",
      "Loss: 1.256 | Acc: 54.923% (6292/11456)\n",
      "Loss: 1.256 | Acc: 54.931% (6328/11520)\n",
      "Loss: 1.256 | Acc: 54.929% (6363/11584)\n",
      "Loss: 1.257 | Acc: 54.885% (6393/11648)\n",
      "Loss: 1.258 | Acc: 54.892% (6429/11712)\n",
      "Loss: 1.257 | Acc: 54.874% (6462/11776)\n",
      "Loss: 1.258 | Acc: 54.856% (6495/11840)\n",
      "Loss: 1.257 | Acc: 54.872% (6532/11904)\n",
      "Loss: 1.256 | Acc: 54.921% (6573/11968)\n",
      "Loss: 1.258 | Acc: 54.845% (6599/12032)\n",
      "Loss: 1.259 | Acc: 54.795% (6628/12096)\n",
      "Loss: 1.260 | Acc: 54.770% (6660/12160)\n",
      "Loss: 1.262 | Acc: 54.712% (6688/12224)\n",
      "Loss: 1.260 | Acc: 54.785% (6732/12288)\n",
      "Loss: 1.261 | Acc: 54.793% (6768/12352)\n",
      "Loss: 1.261 | Acc: 54.808% (6805/12416)\n",
      "Loss: 1.263 | Acc: 54.720% (6829/12480)\n",
      "Loss: 1.264 | Acc: 54.664% (6857/12544)\n",
      "Loss: 1.264 | Acc: 54.695% (6896/12608)\n",
      "Loss: 1.264 | Acc: 54.664% (6927/12672)\n",
      "Loss: 1.265 | Acc: 54.585% (6952/12736)\n",
      "Loss: 1.265 | Acc: 54.625% (6992/12800)\n",
      "Loss: 1.263 | Acc: 54.672% (7033/12864)\n",
      "Loss: 1.264 | Acc: 54.664% (7067/12928)\n",
      "Loss: 1.264 | Acc: 54.695% (7106/12992)\n",
      "Loss: 1.264 | Acc: 54.695% (7141/13056)\n",
      "Loss: 1.265 | Acc: 54.665% (7172/13120)\n",
      "Loss: 1.266 | Acc: 54.589% (7197/13184)\n",
      "Loss: 1.266 | Acc: 54.627% (7237/13248)\n",
      "Loss: 1.265 | Acc: 54.695% (7281/13312)\n",
      "Loss: 1.266 | Acc: 54.620% (7306/13376)\n",
      "Loss: 1.265 | Acc: 54.628% (7342/13440)\n",
      "Loss: 1.265 | Acc: 54.650% (7380/13504)\n",
      "Loss: 1.263 | Acc: 54.702% (7422/13568)\n",
      "Loss: 1.264 | Acc: 54.651% (7450/13632)\n",
      "Loss: 1.264 | Acc: 54.658% (7486/13696)\n",
      "Loss: 1.264 | Acc: 54.629% (7517/13760)\n",
      "Loss: 1.264 | Acc: 54.630% (7552/13824)\n",
      "Loss: 1.264 | Acc: 54.615% (7585/13888)\n",
      "Loss: 1.264 | Acc: 54.601% (7618/13952)\n",
      "Loss: 1.264 | Acc: 54.595% (7652/14016)\n",
      "Loss: 1.264 | Acc: 54.588% (7686/14080)\n",
      "Loss: 1.263 | Acc: 54.603% (7723/14144)\n",
      "Loss: 1.263 | Acc: 54.617% (7760/14208)\n",
      "Loss: 1.263 | Acc: 54.617% (7795/14272)\n",
      "Loss: 1.262 | Acc: 54.632% (7832/14336)\n",
      "Loss: 1.262 | Acc: 54.639% (7868/14400)\n",
      "Loss: 1.262 | Acc: 54.646% (7904/14464)\n",
      "Loss: 1.262 | Acc: 54.619% (7935/14528)\n",
      "Loss: 1.261 | Acc: 54.612% (7969/14592)\n",
      "Loss: 1.262 | Acc: 54.551% (7995/14656)\n",
      "Loss: 1.261 | Acc: 54.592% (8036/14720)\n",
      "Loss: 1.260 | Acc: 54.620% (8075/14784)\n",
      "Loss: 1.262 | Acc: 54.580% (8104/14848)\n",
      "Loss: 1.261 | Acc: 54.607% (8143/14912)\n",
      "Loss: 1.261 | Acc: 54.621% (8180/14976)\n",
      "Loss: 1.262 | Acc: 54.568% (8207/15040)\n",
      "Loss: 1.262 | Acc: 54.575% (8243/15104)\n",
      "Loss: 1.262 | Acc: 54.589% (8280/15168)\n",
      "Loss: 1.260 | Acc: 54.655% (8325/15232)\n",
      "Loss: 1.259 | Acc: 54.655% (8360/15296)\n",
      "Loss: 1.259 | Acc: 54.655% (8395/15360)\n",
      "Loss: 1.259 | Acc: 54.675% (8433/15424)\n",
      "Loss: 1.259 | Acc: 54.707% (8473/15488)\n",
      "Loss: 1.260 | Acc: 54.694% (8506/15552)\n",
      "Loss: 1.260 | Acc: 54.675% (8538/15616)\n",
      "Loss: 1.260 | Acc: 54.668% (8572/15680)\n",
      "Loss: 1.259 | Acc: 54.668% (8607/15744)\n",
      "Loss: 1.259 | Acc: 54.719% (8650/15808)\n",
      "Loss: 1.259 | Acc: 54.713% (8684/15872)\n",
      "Loss: 1.258 | Acc: 54.731% (8722/15936)\n",
      "Loss: 1.258 | Acc: 54.737% (8758/16000)\n",
      "Loss: 1.258 | Acc: 54.725% (8791/16064)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.258 | Acc: 54.756% (8831/16128)\n",
      "Loss: 1.257 | Acc: 54.768% (8868/16192)\n",
      "Loss: 1.257 | Acc: 54.792% (8907/16256)\n",
      "Loss: 1.257 | Acc: 54.767% (8938/16320)\n",
      "Loss: 1.257 | Acc: 54.773% (8974/16384)\n",
      "Loss: 1.257 | Acc: 54.760% (9007/16448)\n",
      "Loss: 1.257 | Acc: 54.742% (9039/16512)\n",
      "Loss: 1.258 | Acc: 54.748% (9075/16576)\n",
      "Loss: 1.258 | Acc: 54.718% (9105/16640)\n",
      "Loss: 1.258 | Acc: 54.717% (9140/16704)\n",
      "Loss: 1.258 | Acc: 54.735% (9178/16768)\n",
      "Loss: 1.258 | Acc: 54.735% (9213/16832)\n",
      "Loss: 1.258 | Acc: 54.717% (9245/16896)\n",
      "Loss: 1.259 | Acc: 54.711% (9279/16960)\n",
      "Loss: 1.259 | Acc: 54.699% (9312/17024)\n",
      "Loss: 1.259 | Acc: 54.688% (9345/17088)\n",
      "Loss: 1.259 | Acc: 54.705% (9383/17152)\n",
      "Loss: 1.259 | Acc: 54.699% (9417/17216)\n",
      "Loss: 1.259 | Acc: 54.699% (9452/17280)\n",
      "Loss: 1.260 | Acc: 54.653% (9479/17344)\n",
      "Loss: 1.259 | Acc: 54.665% (9516/17408)\n",
      "Loss: 1.259 | Acc: 54.665% (9551/17472)\n",
      "Loss: 1.259 | Acc: 54.688% (9590/17536)\n",
      "Loss: 1.258 | Acc: 54.693% (9626/17600)\n",
      "Loss: 1.259 | Acc: 54.671% (9657/17664)\n",
      "Loss: 1.258 | Acc: 54.710% (9699/17728)\n",
      "Loss: 1.258 | Acc: 54.727% (9737/17792)\n",
      "Loss: 1.258 | Acc: 54.716% (9770/17856)\n",
      "Loss: 1.258 | Acc: 54.682% (9799/17920)\n",
      "Loss: 1.258 | Acc: 54.676% (9833/17984)\n",
      "Loss: 1.258 | Acc: 54.660% (9865/18048)\n",
      "Loss: 1.258 | Acc: 54.643% (9897/18112)\n",
      "Loss: 1.258 | Acc: 54.627% (9929/18176)\n",
      "Loss: 1.258 | Acc: 54.627% (9964/18240)\n",
      "Loss: 1.258 | Acc: 54.633% (10000/18304)\n",
      "Loss: 1.258 | Acc: 54.617% (10032/18368)\n",
      "Loss: 1.258 | Acc: 54.617% (10067/18432)\n",
      "Loss: 1.258 | Acc: 54.579% (10095/18496)\n",
      "Loss: 1.258 | Acc: 54.564% (10127/18560)\n",
      "Loss: 1.258 | Acc: 54.537% (10157/18624)\n",
      "Loss: 1.258 | Acc: 54.548% (10194/18688)\n",
      "Loss: 1.258 | Acc: 54.560% (10231/18752)\n",
      "Loss: 1.258 | Acc: 54.539% (10262/18816)\n",
      "Loss: 1.259 | Acc: 54.507% (10291/18880)\n",
      "Loss: 1.260 | Acc: 54.461% (10317/18944)\n",
      "Loss: 1.259 | Acc: 54.488% (10357/19008)\n",
      "Loss: 1.259 | Acc: 54.493% (10393/19072)\n",
      "Loss: 1.260 | Acc: 54.489% (10427/19136)\n",
      "Loss: 1.259 | Acc: 54.490% (10462/19200)\n",
      "Loss: 1.260 | Acc: 54.485% (10496/19264)\n",
      "Loss: 1.260 | Acc: 54.470% (10528/19328)\n",
      "Loss: 1.260 | Acc: 54.461% (10561/19392)\n",
      "Loss: 1.261 | Acc: 54.466% (10597/19456)\n",
      "Loss: 1.261 | Acc: 54.452% (10629/19520)\n",
      "Loss: 1.261 | Acc: 54.448% (10663/19584)\n",
      "Loss: 1.261 | Acc: 54.433% (10695/19648)\n",
      "Loss: 1.261 | Acc: 54.444% (10732/19712)\n",
      "Loss: 1.261 | Acc: 54.419% (10762/19776)\n",
      "Loss: 1.261 | Acc: 54.405% (10794/19840)\n",
      "Loss: 1.262 | Acc: 54.416% (10831/19904)\n",
      "Loss: 1.262 | Acc: 54.397% (10862/19968)\n",
      "Loss: 1.262 | Acc: 54.408% (10899/20032)\n",
      "Loss: 1.262 | Acc: 54.399% (10932/20096)\n",
      "Loss: 1.262 | Acc: 54.410% (10969/20160)\n",
      "Loss: 1.261 | Acc: 54.440% (11010/20224)\n",
      "Loss: 1.261 | Acc: 54.451% (11047/20288)\n",
      "Loss: 1.262 | Acc: 54.412% (11074/20352)\n",
      "Loss: 1.262 | Acc: 54.408% (11108/20416)\n",
      "Loss: 1.262 | Acc: 54.404% (11142/20480)\n",
      "Loss: 1.262 | Acc: 54.395% (11175/20544)\n",
      "Loss: 1.262 | Acc: 54.387% (11208/20608)\n",
      "Loss: 1.261 | Acc: 54.412% (11248/20672)\n",
      "Loss: 1.261 | Acc: 54.408% (11282/20736)\n",
      "Loss: 1.261 | Acc: 54.418% (11319/20800)\n",
      "Loss: 1.261 | Acc: 54.438% (11358/20864)\n",
      "Loss: 1.260 | Acc: 54.444% (11394/20928)\n",
      "Loss: 1.260 | Acc: 54.445% (11429/20992)\n",
      "Loss: 1.260 | Acc: 54.450% (11465/21056)\n",
      "Loss: 1.260 | Acc: 54.465% (11503/21120)\n",
      "Loss: 1.260 | Acc: 54.451% (11535/21184)\n",
      "Loss: 1.261 | Acc: 54.462% (11572/21248)\n",
      "Loss: 1.261 | Acc: 54.439% (11602/21312)\n",
      "Loss: 1.261 | Acc: 54.435% (11636/21376)\n",
      "Loss: 1.261 | Acc: 54.436% (11671/21440)\n",
      "Loss: 1.260 | Acc: 54.469% (11713/21504)\n",
      "Loss: 1.260 | Acc: 54.483% (11751/21568)\n",
      "Loss: 1.260 | Acc: 54.466% (11782/21632)\n",
      "Loss: 1.259 | Acc: 54.508% (11826/21696)\n",
      "Loss: 1.259 | Acc: 54.527% (11865/21760)\n",
      "Loss: 1.259 | Acc: 54.523% (11899/21824)\n",
      "Loss: 1.259 | Acc: 54.505% (11930/21888)\n",
      "Loss: 1.258 | Acc: 54.524% (11969/21952)\n",
      "Loss: 1.258 | Acc: 54.524% (12004/22016)\n",
      "Loss: 1.257 | Acc: 54.534% (12041/22080)\n",
      "Loss: 1.257 | Acc: 54.566% (12083/22144)\n",
      "Loss: 1.257 | Acc: 54.566% (12118/22208)\n",
      "Loss: 1.256 | Acc: 54.593% (12159/22272)\n",
      "Loss: 1.257 | Acc: 54.589% (12193/22336)\n",
      "Loss: 1.256 | Acc: 54.621% (12235/22400)\n",
      "Loss: 1.256 | Acc: 54.603% (12266/22464)\n",
      "Loss: 1.256 | Acc: 54.612% (12303/22528)\n",
      "Loss: 1.256 | Acc: 54.630% (12342/22592)\n",
      "Loss: 1.255 | Acc: 54.648% (12381/22656)\n",
      "Loss: 1.255 | Acc: 54.643% (12415/22720)\n",
      "Loss: 1.255 | Acc: 54.679% (12458/22784)\n",
      "Loss: 1.255 | Acc: 54.670% (12491/22848)\n",
      "Loss: 1.255 | Acc: 54.653% (12522/22912)\n",
      "Loss: 1.256 | Acc: 54.627% (12551/22976)\n",
      "Loss: 1.256 | Acc: 54.605% (12581/23040)\n",
      "Loss: 1.256 | Acc: 54.601% (12615/23104)\n",
      "Loss: 1.256 | Acc: 54.614% (12653/23168)\n",
      "Loss: 1.256 | Acc: 54.597% (12684/23232)\n",
      "Loss: 1.256 | Acc: 54.580% (12715/23296)\n",
      "Loss: 1.256 | Acc: 54.593% (12753/23360)\n",
      "Loss: 1.256 | Acc: 54.577% (12784/23424)\n",
      "Loss: 1.256 | Acc: 54.585% (12821/23488)\n",
      "Loss: 1.256 | Acc: 54.607% (12861/23552)\n",
      "Loss: 1.256 | Acc: 54.607% (12896/23616)\n",
      "Loss: 1.256 | Acc: 54.603% (12930/23680)\n",
      "Loss: 1.256 | Acc: 54.586% (12961/23744)\n",
      "Loss: 1.256 | Acc: 54.591% (12997/23808)\n",
      "Loss: 1.256 | Acc: 54.583% (13030/23872)\n",
      "Loss: 1.255 | Acc: 54.591% (13067/23936)\n",
      "Loss: 1.255 | Acc: 54.600% (13104/24000)\n",
      "Loss: 1.256 | Acc: 54.592% (13137/24064)\n",
      "Loss: 1.257 | Acc: 54.584% (13170/24128)\n",
      "Loss: 1.257 | Acc: 54.572% (13202/24192)\n",
      "Loss: 1.257 | Acc: 54.572% (13237/24256)\n",
      "Loss: 1.257 | Acc: 54.564% (13270/24320)\n",
      "Loss: 1.257 | Acc: 54.564% (13305/24384)\n",
      "Loss: 1.258 | Acc: 54.528% (13331/24448)\n",
      "Loss: 1.257 | Acc: 54.557% (13373/24512)\n",
      "Loss: 1.257 | Acc: 54.561% (13409/24576)\n",
      "Loss: 1.257 | Acc: 54.537% (13438/24640)\n",
      "Loss: 1.258 | Acc: 54.517% (13468/24704)\n",
      "Loss: 1.258 | Acc: 54.530% (13506/24768)\n",
      "Loss: 1.258 | Acc: 54.534% (13542/24832)\n",
      "Loss: 1.258 | Acc: 54.527% (13575/24896)\n",
      "Loss: 1.258 | Acc: 54.539% (13613/24960)\n",
      "Loss: 1.258 | Acc: 54.548% (13650/25024)\n",
      "Loss: 1.257 | Acc: 54.564% (13689/25088)\n",
      "Loss: 1.257 | Acc: 54.580% (13728/25152)\n",
      "Loss: 1.257 | Acc: 54.565% (13759/25216)\n",
      "Loss: 1.258 | Acc: 54.529% (13785/25280)\n",
      "Loss: 1.257 | Acc: 54.553% (13826/25344)\n",
      "Loss: 1.257 | Acc: 54.562% (13863/25408)\n",
      "Loss: 1.256 | Acc: 54.601% (13908/25472)\n",
      "Loss: 1.256 | Acc: 54.621% (13948/25536)\n",
      "Loss: 1.256 | Acc: 54.613% (13981/25600)\n",
      "Loss: 1.257 | Acc: 54.606% (14014/25664)\n",
      "Loss: 1.256 | Acc: 54.618% (14052/25728)\n",
      "Loss: 1.256 | Acc: 54.614% (14086/25792)\n",
      "Loss: 1.256 | Acc: 54.599% (14117/25856)\n",
      "Loss: 1.256 | Acc: 54.591% (14150/25920)\n",
      "Loss: 1.257 | Acc: 54.568% (14179/25984)\n",
      "Loss: 1.256 | Acc: 54.572% (14215/26048)\n",
      "Loss: 1.256 | Acc: 54.576% (14251/26112)\n",
      "Loss: 1.257 | Acc: 54.565% (14283/26176)\n",
      "Loss: 1.257 | Acc: 54.558% (14316/26240)\n",
      "Loss: 1.256 | Acc: 54.570% (14354/26304)\n",
      "Loss: 1.256 | Acc: 54.596% (14396/26368)\n",
      "Loss: 1.256 | Acc: 54.616% (14436/26432)\n",
      "Loss: 1.256 | Acc: 54.627% (14474/26496)\n",
      "Loss: 1.256 | Acc: 54.627% (14509/26560)\n",
      "Loss: 1.255 | Acc: 54.665% (14554/26624)\n",
      "Loss: 1.255 | Acc: 54.669% (14590/26688)\n",
      "Loss: 1.255 | Acc: 54.658% (14622/26752)\n",
      "Loss: 1.255 | Acc: 54.661% (14658/26816)\n",
      "Loss: 1.255 | Acc: 54.650% (14690/26880)\n",
      "Loss: 1.255 | Acc: 54.650% (14725/26944)\n",
      "Loss: 1.255 | Acc: 54.632% (14755/27008)\n",
      "Loss: 1.256 | Acc: 54.639% (14792/27072)\n",
      "Loss: 1.255 | Acc: 54.665% (14834/27136)\n",
      "Loss: 1.255 | Acc: 54.676% (14872/27200)\n",
      "Loss: 1.255 | Acc: 54.680% (14908/27264)\n",
      "Loss: 1.255 | Acc: 54.677% (14942/27328)\n",
      "Loss: 1.255 | Acc: 54.680% (14978/27392)\n",
      "Loss: 1.255 | Acc: 54.691% (15016/27456)\n",
      "Loss: 1.254 | Acc: 54.709% (15056/27520)\n",
      "Loss: 1.254 | Acc: 54.691% (15086/27584)\n",
      "Loss: 1.254 | Acc: 54.695% (15122/27648)\n",
      "Loss: 1.254 | Acc: 54.720% (15164/27712)\n",
      "Loss: 1.254 | Acc: 54.734% (15203/27776)\n",
      "Loss: 1.254 | Acc: 54.745% (15241/27840)\n",
      "Loss: 1.254 | Acc: 54.727% (15271/27904)\n",
      "Loss: 1.254 | Acc: 54.727% (15306/27968)\n",
      "Loss: 1.254 | Acc: 54.720% (15339/28032)\n",
      "Loss: 1.254 | Acc: 54.709% (15371/28096)\n",
      "Loss: 1.254 | Acc: 54.695% (15402/28160)\n",
      "Loss: 1.253 | Acc: 54.712% (15442/28224)\n",
      "Loss: 1.254 | Acc: 54.695% (15472/28288)\n",
      "Loss: 1.254 | Acc: 54.705% (15510/28352)\n",
      "Loss: 1.254 | Acc: 54.684% (15539/28416)\n",
      "Loss: 1.254 | Acc: 54.684% (15574/28480)\n",
      "Loss: 1.254 | Acc: 54.684% (15609/28544)\n",
      "Loss: 1.254 | Acc: 54.708% (15651/28608)\n",
      "Loss: 1.253 | Acc: 54.726% (15691/28672)\n",
      "Loss: 1.253 | Acc: 54.733% (15728/28736)\n",
      "Loss: 1.254 | Acc: 54.729% (15762/28800)\n",
      "Loss: 1.254 | Acc: 54.726% (15796/28864)\n",
      "Loss: 1.253 | Acc: 54.760% (15841/28928)\n",
      "Loss: 1.253 | Acc: 54.760% (15876/28992)\n",
      "Loss: 1.253 | Acc: 54.784% (15918/29056)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.252 | Acc: 54.797% (15957/29120)\n",
      "Loss: 1.252 | Acc: 54.807% (15995/29184)\n",
      "Loss: 1.252 | Acc: 54.797% (16027/29248)\n",
      "Loss: 1.252 | Acc: 54.807% (16065/29312)\n",
      "Loss: 1.253 | Acc: 54.783% (16093/29376)\n",
      "Loss: 1.253 | Acc: 54.779% (16127/29440)\n",
      "Loss: 1.253 | Acc: 54.769% (16159/29504)\n",
      "Loss: 1.253 | Acc: 54.772% (16195/29568)\n",
      "Loss: 1.254 | Acc: 54.772% (16230/29632)\n",
      "Loss: 1.253 | Acc: 54.785% (16269/29696)\n",
      "Loss: 1.253 | Acc: 54.775% (16301/29760)\n",
      "Loss: 1.252 | Acc: 54.802% (16344/29824)\n",
      "Loss: 1.252 | Acc: 54.831% (16388/29888)\n",
      "Loss: 1.252 | Acc: 54.831% (16423/29952)\n",
      "Loss: 1.252 | Acc: 54.831% (16458/30016)\n",
      "Loss: 1.252 | Acc: 54.824% (16491/30080)\n",
      "Loss: 1.252 | Acc: 54.814% (16523/30144)\n",
      "Loss: 1.252 | Acc: 54.820% (16560/30208)\n",
      "Loss: 1.252 | Acc: 54.803% (16590/30272)\n",
      "Loss: 1.252 | Acc: 54.800% (16624/30336)\n",
      "Loss: 1.252 | Acc: 54.812% (16663/30400)\n",
      "Loss: 1.252 | Acc: 54.806% (16696/30464)\n",
      "Loss: 1.252 | Acc: 54.815% (16734/30528)\n",
      "Loss: 1.252 | Acc: 54.808% (16767/30592)\n",
      "Loss: 1.253 | Acc: 54.802% (16800/30656)\n",
      "Loss: 1.253 | Acc: 54.769% (16825/30720)\n",
      "Loss: 1.253 | Acc: 54.752% (16855/30784)\n",
      "Loss: 1.253 | Acc: 54.756% (16891/30848)\n",
      "Loss: 1.253 | Acc: 54.772% (16931/30912)\n",
      "Loss: 1.253 | Acc: 54.778% (16968/30976)\n",
      "Loss: 1.253 | Acc: 54.784% (17005/31040)\n",
      "Loss: 1.253 | Acc: 54.774% (17037/31104)\n",
      "Loss: 1.253 | Acc: 54.765% (17069/31168)\n",
      "Loss: 1.253 | Acc: 54.771% (17106/31232)\n",
      "Loss: 1.253 | Acc: 54.755% (17136/31296)\n",
      "Loss: 1.254 | Acc: 54.745% (17168/31360)\n",
      "Loss: 1.253 | Acc: 54.745% (17203/31424)\n",
      "Loss: 1.253 | Acc: 54.767% (17245/31488)\n",
      "Loss: 1.254 | Acc: 54.757% (17277/31552)\n",
      "Loss: 1.254 | Acc: 54.751% (17310/31616)\n",
      "Loss: 1.253 | Acc: 54.763% (17349/31680)\n",
      "Loss: 1.253 | Acc: 54.763% (17384/31744)\n",
      "Loss: 1.253 | Acc: 54.782% (17425/31808)\n",
      "Loss: 1.253 | Acc: 54.769% (17456/31872)\n",
      "Loss: 1.253 | Acc: 54.781% (17495/31936)\n",
      "Loss: 1.253 | Acc: 54.784% (17531/32000)\n",
      "Loss: 1.252 | Acc: 54.794% (17569/32064)\n",
      "Loss: 1.252 | Acc: 54.821% (17613/32128)\n",
      "Loss: 1.252 | Acc: 54.827% (17650/32192)\n",
      "Loss: 1.251 | Acc: 54.833% (17687/32256)\n",
      "Loss: 1.252 | Acc: 54.814% (17716/32320)\n",
      "Loss: 1.251 | Acc: 54.836% (17758/32384)\n",
      "Loss: 1.251 | Acc: 54.826% (17790/32448)\n",
      "Loss: 1.251 | Acc: 54.838% (17829/32512)\n",
      "Loss: 1.252 | Acc: 54.832% (17862/32576)\n",
      "Loss: 1.251 | Acc: 54.816% (17892/32640)\n",
      "Loss: 1.251 | Acc: 54.831% (17932/32704)\n",
      "Loss: 1.251 | Acc: 54.837% (17969/32768)\n",
      "Loss: 1.251 | Acc: 54.855% (18010/32832)\n",
      "Loss: 1.251 | Acc: 54.864% (18048/32896)\n",
      "Loss: 1.250 | Acc: 54.882% (18089/32960)\n",
      "Loss: 1.251 | Acc: 54.875% (18122/33024)\n",
      "Loss: 1.251 | Acc: 54.878% (18158/33088)\n",
      "Loss: 1.250 | Acc: 54.893% (18198/33152)\n",
      "Loss: 1.250 | Acc: 54.904% (18237/33216)\n",
      "Loss: 1.250 | Acc: 54.910% (18274/33280)\n",
      "Loss: 1.250 | Acc: 54.927% (18315/33344)\n",
      "Loss: 1.250 | Acc: 54.921% (18348/33408)\n",
      "Loss: 1.250 | Acc: 54.927% (18385/33472)\n",
      "Loss: 1.250 | Acc: 54.929% (18421/33536)\n",
      "Loss: 1.250 | Acc: 54.935% (18458/33600)\n",
      "Loss: 1.250 | Acc: 54.943% (18496/33664)\n",
      "Loss: 1.250 | Acc: 54.940% (18530/33728)\n",
      "Loss: 1.250 | Acc: 54.951% (18569/33792)\n",
      "Loss: 1.250 | Acc: 54.953% (18605/33856)\n",
      "Loss: 1.249 | Acc: 54.965% (18644/33920)\n",
      "Loss: 1.250 | Acc: 54.958% (18677/33984)\n",
      "Loss: 1.250 | Acc: 54.937% (18705/34048)\n",
      "Loss: 1.250 | Acc: 54.945% (18743/34112)\n",
      "Loss: 1.250 | Acc: 54.930% (18773/34176)\n",
      "Loss: 1.250 | Acc: 54.921% (18805/34240)\n",
      "Loss: 1.250 | Acc: 54.900% (18833/34304)\n",
      "Loss: 1.251 | Acc: 54.885% (18863/34368)\n",
      "Loss: 1.251 | Acc: 54.891% (18900/34432)\n",
      "Loss: 1.251 | Acc: 54.896% (18937/34496)\n",
      "Loss: 1.251 | Acc: 54.884% (18968/34560)\n",
      "Loss: 1.251 | Acc: 54.893% (19006/34624)\n",
      "Loss: 1.251 | Acc: 54.875% (19035/34688)\n",
      "Loss: 1.251 | Acc: 54.877% (19071/34752)\n",
      "Loss: 1.251 | Acc: 54.874% (19105/34816)\n",
      "Loss: 1.251 | Acc: 54.871% (19139/34880)\n",
      "Loss: 1.251 | Acc: 54.876% (19176/34944)\n",
      "Loss: 1.251 | Acc: 54.870% (19209/35008)\n",
      "Loss: 1.251 | Acc: 54.870% (19244/35072)\n",
      "Loss: 1.251 | Acc: 54.870% (19279/35136)\n",
      "Loss: 1.251 | Acc: 54.866% (19313/35200)\n",
      "Loss: 1.251 | Acc: 54.858% (19345/35264)\n",
      "Loss: 1.251 | Acc: 54.852% (19378/35328)\n",
      "Loss: 1.251 | Acc: 54.849% (19412/35392)\n",
      "Loss: 1.251 | Acc: 54.845% (19446/35456)\n",
      "Loss: 1.252 | Acc: 54.825% (19474/35520)\n",
      "Loss: 1.252 | Acc: 54.836% (19513/35584)\n",
      "Loss: 1.252 | Acc: 54.805% (19537/35648)\n",
      "Loss: 1.252 | Acc: 54.777% (19562/35712)\n",
      "Loss: 1.253 | Acc: 54.752% (19588/35776)\n",
      "Loss: 1.252 | Acc: 54.763% (19627/35840)\n",
      "Loss: 1.252 | Acc: 54.777% (19667/35904)\n",
      "Loss: 1.253 | Acc: 54.765% (19698/35968)\n",
      "Loss: 1.252 | Acc: 54.785% (19740/36032)\n",
      "Loss: 1.252 | Acc: 54.779% (19773/36096)\n",
      "Loss: 1.252 | Acc: 54.784% (19810/36160)\n",
      "Loss: 1.252 | Acc: 54.795% (19849/36224)\n",
      "Loss: 1.252 | Acc: 54.803% (19887/36288)\n",
      "Loss: 1.252 | Acc: 54.798% (19920/36352)\n",
      "Loss: 1.252 | Acc: 54.789% (19952/36416)\n",
      "Loss: 1.252 | Acc: 54.805% (19993/36480)\n",
      "Loss: 1.253 | Acc: 54.789% (20022/36544)\n",
      "Loss: 1.253 | Acc: 54.780% (20054/36608)\n",
      "Loss: 1.253 | Acc: 54.772% (20086/36672)\n",
      "Loss: 1.253 | Acc: 54.775% (20122/36736)\n",
      "Loss: 1.253 | Acc: 54.780% (20159/36800)\n",
      "Loss: 1.253 | Acc: 54.785% (20196/36864)\n",
      "Loss: 1.253 | Acc: 54.780% (20229/36928)\n",
      "Loss: 1.253 | Acc: 54.779% (20264/36992)\n",
      "Loss: 1.252 | Acc: 54.801% (20307/37056)\n",
      "Loss: 1.252 | Acc: 54.817% (20348/37120)\n",
      "Loss: 1.252 | Acc: 54.817% (20383/37184)\n",
      "Loss: 1.252 | Acc: 54.806% (20414/37248)\n",
      "Loss: 1.253 | Acc: 54.784% (20441/37312)\n",
      "Loss: 1.253 | Acc: 54.773% (20472/37376)\n",
      "Loss: 1.253 | Acc: 54.781% (20510/37440)\n",
      "Loss: 1.253 | Acc: 54.759% (20537/37504)\n",
      "Loss: 1.253 | Acc: 54.735% (20563/37568)\n",
      "Loss: 1.254 | Acc: 54.730% (20596/37632)\n",
      "Loss: 1.253 | Acc: 54.746% (20637/37696)\n",
      "Loss: 1.253 | Acc: 54.730% (20666/37760)\n",
      "Loss: 1.254 | Acc: 54.738% (20704/37824)\n",
      "Loss: 1.253 | Acc: 54.746% (20742/37888)\n",
      "Loss: 1.254 | Acc: 54.735% (20773/37952)\n",
      "Loss: 1.254 | Acc: 54.735% (20808/38016)\n",
      "Loss: 1.253 | Acc: 54.743% (20846/38080)\n",
      "Loss: 1.253 | Acc: 54.756% (20886/38144)\n",
      "Loss: 1.253 | Acc: 54.766% (20925/38208)\n",
      "Loss: 1.253 | Acc: 54.768% (20961/38272)\n",
      "Loss: 1.254 | Acc: 54.776% (20999/38336)\n",
      "Loss: 1.253 | Acc: 54.781% (21036/38400)\n",
      "Loss: 1.253 | Acc: 54.791% (21075/38464)\n",
      "Loss: 1.253 | Acc: 54.786% (21108/38528)\n",
      "Loss: 1.253 | Acc: 54.783% (21142/38592)\n",
      "Loss: 1.254 | Acc: 54.783% (21177/38656)\n",
      "Loss: 1.254 | Acc: 54.780% (21211/38720)\n",
      "Loss: 1.253 | Acc: 54.783% (21247/38784)\n",
      "Loss: 1.253 | Acc: 54.803% (21290/38848)\n",
      "Loss: 1.253 | Acc: 54.798% (21323/38912)\n",
      "Loss: 1.253 | Acc: 54.811% (21363/38976)\n",
      "Loss: 1.253 | Acc: 54.816% (21400/39040)\n",
      "Loss: 1.252 | Acc: 54.815% (21435/39104)\n",
      "Loss: 1.252 | Acc: 54.807% (21467/39168)\n",
      "Loss: 1.252 | Acc: 54.797% (21498/39232)\n",
      "Loss: 1.253 | Acc: 54.787% (21529/39296)\n",
      "Loss: 1.253 | Acc: 54.782% (21562/39360)\n",
      "Loss: 1.253 | Acc: 54.789% (21600/39424)\n",
      "Loss: 1.253 | Acc: 54.784% (21633/39488)\n",
      "Loss: 1.253 | Acc: 54.779% (21666/39552)\n",
      "Loss: 1.253 | Acc: 54.778% (21701/39616)\n",
      "Loss: 1.253 | Acc: 54.806% (21747/39680)\n",
      "Loss: 1.252 | Acc: 54.821% (21788/39744)\n",
      "Loss: 1.252 | Acc: 54.821% (21823/39808)\n",
      "Loss: 1.252 | Acc: 54.825% (21860/39872)\n",
      "Loss: 1.252 | Acc: 54.825% (21895/39936)\n",
      "Loss: 1.252 | Acc: 54.833% (21933/40000)\n",
      "Loss: 1.252 | Acc: 54.812% (21960/40064)\n",
      "Loss: 1.252 | Acc: 54.812% (21995/40128)\n",
      "Loss: 1.253 | Acc: 54.797% (22024/40192)\n",
      "Loss: 1.253 | Acc: 54.802% (22061/40256)\n",
      "Loss: 1.253 | Acc: 54.812% (22100/40320)\n",
      "Loss: 1.253 | Acc: 54.811% (22135/40384)\n",
      "Loss: 1.253 | Acc: 54.806% (22168/40448)\n",
      "Loss: 1.253 | Acc: 54.808% (22204/40512)\n",
      "Loss: 1.253 | Acc: 54.826% (22246/40576)\n",
      "Loss: 1.253 | Acc: 54.828% (22282/40640)\n",
      "Loss: 1.253 | Acc: 54.805% (22308/40704)\n",
      "Loss: 1.254 | Acc: 54.805% (22343/40768)\n",
      "Loss: 1.254 | Acc: 54.805% (22378/40832)\n",
      "Loss: 1.254 | Acc: 54.800% (22411/40896)\n",
      "Loss: 1.254 | Acc: 54.790% (22442/40960)\n",
      "Loss: 1.254 | Acc: 54.790% (22477/41024)\n",
      "Loss: 1.254 | Acc: 54.792% (22513/41088)\n",
      "Loss: 1.254 | Acc: 54.797% (22550/41152)\n",
      "Loss: 1.254 | Acc: 54.789% (22582/41216)\n",
      "Loss: 1.254 | Acc: 54.787% (22616/41280)\n",
      "Loss: 1.254 | Acc: 54.772% (22645/41344)\n",
      "Loss: 1.254 | Acc: 54.772% (22680/41408)\n",
      "Loss: 1.254 | Acc: 54.782% (22719/41472)\n",
      "Loss: 1.255 | Acc: 54.774% (22751/41536)\n",
      "Loss: 1.255 | Acc: 54.774% (22786/41600)\n",
      "Loss: 1.255 | Acc: 54.772% (22820/41664)\n",
      "Loss: 1.255 | Acc: 54.791% (22863/41728)\n",
      "Loss: 1.255 | Acc: 54.783% (22895/41792)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.255 | Acc: 54.776% (22927/41856)\n",
      "Loss: 1.255 | Acc: 54.769% (22959/41920)\n",
      "Loss: 1.255 | Acc: 54.773% (22996/41984)\n",
      "Loss: 1.255 | Acc: 54.778% (23033/42048)\n",
      "Loss: 1.255 | Acc: 54.785% (23071/42112)\n",
      "Loss: 1.255 | Acc: 54.775% (23102/42176)\n",
      "Loss: 1.255 | Acc: 54.773% (23136/42240)\n",
      "Loss: 1.255 | Acc: 54.768% (23169/42304)\n",
      "Loss: 1.255 | Acc: 54.761% (23201/42368)\n",
      "Loss: 1.255 | Acc: 54.775% (23242/42432)\n",
      "Loss: 1.255 | Acc: 54.775% (23277/42496)\n",
      "Loss: 1.255 | Acc: 54.760% (23306/42560)\n",
      "Loss: 1.255 | Acc: 54.770% (23345/42624)\n",
      "Loss: 1.255 | Acc: 54.767% (23379/42688)\n",
      "Loss: 1.255 | Acc: 54.760% (23411/42752)\n",
      "Loss: 1.255 | Acc: 54.760% (23446/42816)\n",
      "Loss: 1.255 | Acc: 54.762% (23482/42880)\n",
      "Loss: 1.255 | Acc: 54.774% (23522/42944)\n",
      "Loss: 1.255 | Acc: 54.783% (23561/43008)\n",
      "Loss: 1.255 | Acc: 54.794% (23601/43072)\n",
      "Loss: 1.255 | Acc: 54.787% (23633/43136)\n",
      "Loss: 1.255 | Acc: 54.792% (23670/43200)\n",
      "Loss: 1.255 | Acc: 54.794% (23706/43264)\n",
      "Loss: 1.255 | Acc: 54.782% (23736/43328)\n",
      "Loss: 1.255 | Acc: 54.773% (23767/43392)\n",
      "Loss: 1.255 | Acc: 54.768% (23800/43456)\n",
      "Loss: 1.255 | Acc: 54.775% (23838/43520)\n",
      "Loss: 1.255 | Acc: 54.777% (23874/43584)\n",
      "Loss: 1.255 | Acc: 54.788% (23914/43648)\n",
      "Loss: 1.255 | Acc: 54.784% (23947/43712)\n",
      "Loss: 1.255 | Acc: 54.781% (23981/43776)\n",
      "Loss: 1.255 | Acc: 54.779% (24015/43840)\n",
      "Loss: 1.255 | Acc: 54.795% (24057/43904)\n",
      "Loss: 1.254 | Acc: 54.813% (24100/43968)\n",
      "Loss: 1.254 | Acc: 54.821% (24139/44032)\n",
      "Loss: 1.254 | Acc: 54.819% (24173/44096)\n",
      "Loss: 1.254 | Acc: 54.819% (24208/44160)\n",
      "Loss: 1.255 | Acc: 54.814% (24241/44224)\n",
      "Loss: 1.254 | Acc: 54.818% (24278/44288)\n",
      "Loss: 1.254 | Acc: 54.823% (24315/44352)\n",
      "Loss: 1.255 | Acc: 54.811% (24345/44416)\n",
      "Loss: 1.255 | Acc: 54.813% (24381/44480)\n",
      "Loss: 1.255 | Acc: 54.820% (24419/44544)\n",
      "Loss: 1.255 | Acc: 54.824% (24456/44608)\n",
      "Loss: 1.255 | Acc: 54.817% (24488/44672)\n",
      "Loss: 1.254 | Acc: 54.828% (24528/44736)\n",
      "Loss: 1.254 | Acc: 54.830% (24564/44800)\n",
      "Loss: 1.254 | Acc: 54.830% (24599/44864)\n",
      "Loss: 1.254 | Acc: 54.848% (24642/44928)\n",
      "Loss: 1.254 | Acc: 54.859% (24682/44992)\n",
      "Loss: 1.254 | Acc: 54.861% (24718/45056)\n",
      "Loss: 1.254 | Acc: 54.858% (24752/45120)\n",
      "Loss: 1.254 | Acc: 54.858% (24787/45184)\n",
      "Loss: 1.254 | Acc: 54.844% (24816/45248)\n",
      "Loss: 1.254 | Acc: 54.827% (24843/45312)\n",
      "Loss: 1.254 | Acc: 54.829% (24879/45376)\n",
      "Loss: 1.254 | Acc: 54.846% (24922/45440)\n",
      "Loss: 1.254 | Acc: 54.857% (24962/45504)\n",
      "Loss: 1.254 | Acc: 54.848% (24993/45568)\n",
      "Loss: 1.254 | Acc: 54.850% (25029/45632)\n",
      "Loss: 1.254 | Acc: 54.843% (25061/45696)\n",
      "Loss: 1.255 | Acc: 54.832% (25091/45760)\n",
      "Loss: 1.255 | Acc: 54.825% (25123/45824)\n",
      "Loss: 1.255 | Acc: 54.842% (25166/45888)\n",
      "Loss: 1.254 | Acc: 54.853% (25206/45952)\n",
      "Loss: 1.254 | Acc: 54.846% (25238/46016)\n",
      "Loss: 1.254 | Acc: 54.852% (25276/46080)\n",
      "Loss: 1.254 | Acc: 54.848% (25309/46144)\n",
      "Loss: 1.255 | Acc: 54.839% (25340/46208)\n",
      "Loss: 1.255 | Acc: 54.832% (25372/46272)\n",
      "Loss: 1.255 | Acc: 54.830% (25406/46336)\n",
      "Loss: 1.255 | Acc: 54.825% (25439/46400)\n",
      "Loss: 1.255 | Acc: 54.810% (25467/46464)\n",
      "Loss: 1.255 | Acc: 54.814% (25504/46528)\n",
      "Loss: 1.255 | Acc: 54.810% (25537/46592)\n",
      "Loss: 1.255 | Acc: 54.808% (25571/46656)\n",
      "Loss: 1.255 | Acc: 54.810% (25607/46720)\n",
      "Loss: 1.254 | Acc: 54.814% (25644/46784)\n",
      "Loss: 1.254 | Acc: 54.828% (25686/46848)\n",
      "Loss: 1.255 | Acc: 54.818% (25716/46912)\n",
      "Loss: 1.254 | Acc: 54.817% (25751/46976)\n",
      "Loss: 1.254 | Acc: 54.819% (25787/47040)\n",
      "Loss: 1.254 | Acc: 54.828% (25826/47104)\n",
      "Loss: 1.254 | Acc: 54.827% (25861/47168)\n",
      "Loss: 1.254 | Acc: 54.840% (25902/47232)\n",
      "Loss: 1.254 | Acc: 54.848% (25941/47296)\n",
      "Loss: 1.254 | Acc: 54.854% (25979/47360)\n",
      "Loss: 1.254 | Acc: 54.860% (26017/47424)\n",
      "Loss: 1.254 | Acc: 54.856% (26050/47488)\n",
      "Loss: 1.253 | Acc: 54.860% (26087/47552)\n",
      "Loss: 1.253 | Acc: 54.864% (26124/47616)\n",
      "Loss: 1.254 | Acc: 54.855% (26155/47680)\n",
      "Loss: 1.253 | Acc: 54.855% (26190/47744)\n",
      "Loss: 1.254 | Acc: 54.840% (26218/47808)\n",
      "Loss: 1.254 | Acc: 54.836% (26251/47872)\n",
      "Loss: 1.254 | Acc: 54.840% (26288/47936)\n",
      "Loss: 1.254 | Acc: 54.837% (26322/48000)\n",
      "Loss: 1.254 | Acc: 54.837% (26357/48064)\n",
      "Loss: 1.254 | Acc: 54.841% (26394/48128)\n",
      "Loss: 1.254 | Acc: 54.843% (26430/48192)\n",
      "Loss: 1.254 | Acc: 54.845% (26466/48256)\n",
      "Loss: 1.254 | Acc: 54.841% (26499/48320)\n",
      "Loss: 1.254 | Acc: 54.840% (26534/48384)\n",
      "Loss: 1.254 | Acc: 54.836% (26567/48448)\n",
      "Loss: 1.254 | Acc: 54.819% (26594/48512)\n",
      "Loss: 1.254 | Acc: 54.834% (26636/48576)\n",
      "Loss: 1.254 | Acc: 54.815% (26662/48640)\n",
      "Loss: 1.254 | Acc: 54.817% (26698/48704)\n",
      "Loss: 1.254 | Acc: 54.839% (26744/48768)\n",
      "Loss: 1.254 | Acc: 54.857% (26788/48832)\n",
      "Loss: 1.254 | Acc: 54.861% (26825/48896)\n",
      "Loss: 1.254 | Acc: 54.865% (26862/48960)\n",
      "Loss: 1.253 | Acc: 54.876% (26889/49000)\n",
      "Epoch 2 of training is completed, Training accuracy for this epoch is 54.875510204081635\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.157 | Acc: 53.125% (34/64)\n",
      "Loss: 1.139 | Acc: 53.906% (69/128)\n",
      "Loss: 1.239 | Acc: 52.604% (101/192)\n",
      "Loss: 1.341 | Acc: 49.609% (127/256)\n",
      "Loss: 1.329 | Acc: 50.625% (162/320)\n",
      "Loss: 1.339 | Acc: 50.000% (192/384)\n",
      "Loss: 1.357 | Acc: 49.107% (220/448)\n",
      "Loss: 1.328 | Acc: 49.805% (255/512)\n",
      "Loss: 1.290 | Acc: 51.215% (295/576)\n",
      "Loss: 1.285 | Acc: 51.250% (328/640)\n",
      "Loss: 1.288 | Acc: 51.705% (364/704)\n",
      "Loss: 1.286 | Acc: 52.083% (400/768)\n",
      "Loss: 1.288 | Acc: 52.284% (435/832)\n",
      "Loss: 1.282 | Acc: 53.125% (476/896)\n",
      "Loss: 1.283 | Acc: 53.229% (511/960)\n",
      "Loss: 1.269 | Acc: 53.711% (550/1024)\n",
      "Loss: 1.275 | Acc: 53.585% (583/1088)\n",
      "Loss: 1.262 | Acc: 54.167% (624/1152)\n",
      "Loss: 1.257 | Acc: 54.276% (660/1216)\n",
      "Loss: 1.269 | Acc: 54.141% (693/1280)\n",
      "Loss: 1.272 | Acc: 53.943% (725/1344)\n",
      "Loss: 1.271 | Acc: 54.119% (762/1408)\n",
      "Loss: 1.263 | Acc: 54.348% (800/1472)\n",
      "Loss: 1.265 | Acc: 54.297% (834/1536)\n",
      "Loss: 1.267 | Acc: 54.250% (868/1600)\n",
      "Loss: 1.276 | Acc: 54.267% (903/1664)\n",
      "Loss: 1.282 | Acc: 54.051% (934/1728)\n",
      "Loss: 1.283 | Acc: 53.627% (961/1792)\n",
      "Loss: 1.283 | Acc: 53.664% (996/1856)\n",
      "Loss: 1.288 | Acc: 53.802% (1033/1920)\n",
      "Loss: 1.286 | Acc: 53.881% (1069/1984)\n",
      "Loss: 1.289 | Acc: 53.857% (1103/2048)\n",
      "Loss: 1.283 | Acc: 54.119% (1143/2112)\n",
      "Loss: 1.285 | Acc: 53.998% (1175/2176)\n",
      "Loss: 1.284 | Acc: 53.973% (1209/2240)\n",
      "Loss: 1.283 | Acc: 54.036% (1245/2304)\n",
      "Loss: 1.284 | Acc: 53.927% (1277/2368)\n",
      "Loss: 1.285 | Acc: 53.947% (1312/2432)\n",
      "Loss: 1.285 | Acc: 54.087% (1350/2496)\n",
      "Loss: 1.298 | Acc: 53.906% (1380/2560)\n",
      "Loss: 1.299 | Acc: 53.925% (1415/2624)\n",
      "Loss: 1.297 | Acc: 53.869% (1448/2688)\n",
      "Loss: 1.296 | Acc: 53.706% (1478/2752)\n",
      "Loss: 1.291 | Acc: 53.729% (1513/2816)\n",
      "Loss: 1.291 | Acc: 53.681% (1546/2880)\n",
      "Loss: 1.288 | Acc: 53.872% (1586/2944)\n",
      "Loss: 1.283 | Acc: 54.056% (1626/3008)\n",
      "Loss: 1.282 | Acc: 54.069% (1661/3072)\n",
      "Loss: 1.280 | Acc: 54.177% (1699/3136)\n",
      "Loss: 1.280 | Acc: 54.188% (1734/3200)\n",
      "Loss: 1.280 | Acc: 54.167% (1768/3264)\n",
      "Loss: 1.277 | Acc: 54.297% (1807/3328)\n",
      "Loss: 1.276 | Acc: 54.216% (1839/3392)\n",
      "Loss: 1.275 | Acc: 54.225% (1874/3456)\n",
      "Loss: 1.276 | Acc: 54.261% (1910/3520)\n",
      "Loss: 1.275 | Acc: 54.353% (1948/3584)\n",
      "Loss: 1.277 | Acc: 54.276% (1980/3648)\n",
      "Loss: 1.275 | Acc: 54.283% (2015/3712)\n",
      "Loss: 1.273 | Acc: 54.317% (2051/3776)\n",
      "Loss: 1.271 | Acc: 54.453% (2091/3840)\n",
      "Loss: 1.269 | Acc: 54.406% (2124/3904)\n",
      "Loss: 1.269 | Acc: 54.410% (2159/3968)\n",
      "Loss: 1.269 | Acc: 54.439% (2195/4032)\n",
      "Loss: 1.269 | Acc: 54.492% (2232/4096)\n",
      "Loss: 1.270 | Acc: 54.423% (2264/4160)\n",
      "Loss: 1.270 | Acc: 54.522% (2303/4224)\n",
      "Loss: 1.272 | Acc: 54.431% (2334/4288)\n",
      "Loss: 1.271 | Acc: 54.504% (2372/4352)\n",
      "Loss: 1.269 | Acc: 54.665% (2414/4416)\n",
      "Loss: 1.268 | Acc: 54.799% (2455/4480)\n",
      "Loss: 1.267 | Acc: 54.908% (2495/4544)\n",
      "Loss: 1.269 | Acc: 54.839% (2527/4608)\n",
      "Loss: 1.267 | Acc: 54.880% (2564/4672)\n",
      "Loss: 1.266 | Acc: 55.046% (2607/4736)\n",
      "Loss: 1.266 | Acc: 54.958% (2638/4800)\n",
      "Loss: 1.263 | Acc: 55.078% (2679/4864)\n",
      "Loss: 1.264 | Acc: 55.053% (2713/4928)\n",
      "Loss: 1.264 | Acc: 54.968% (2744/4992)\n",
      "Loss: 1.261 | Acc: 55.142% (2788/5056)\n",
      "Loss: 1.264 | Acc: 54.980% (2815/5120)\n",
      "Loss: 1.264 | Acc: 54.977% (2850/5184)\n",
      "Loss: 1.265 | Acc: 54.916% (2882/5248)\n",
      "Loss: 1.264 | Acc: 54.951% (2919/5312)\n",
      "Loss: 1.266 | Acc: 54.929% (2953/5376)\n",
      "Loss: 1.267 | Acc: 54.908% (2987/5440)\n",
      "Loss: 1.267 | Acc: 54.942% (3024/5504)\n",
      "Loss: 1.269 | Acc: 54.867% (3055/5568)\n",
      "Loss: 1.271 | Acc: 54.883% (3091/5632)\n",
      "Loss: 1.271 | Acc: 54.863% (3125/5696)\n",
      "Loss: 1.269 | Acc: 54.896% (3162/5760)\n",
      "Loss: 1.268 | Acc: 54.894% (3197/5824)\n",
      "Loss: 1.271 | Acc: 54.738% (3223/5888)\n",
      "Loss: 1.272 | Acc: 54.738% (3258/5952)\n",
      "Loss: 1.273 | Acc: 54.688% (3290/6016)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.274 | Acc: 54.572% (3318/6080)\n",
      "Loss: 1.273 | Acc: 54.541% (3351/6144)\n",
      "Loss: 1.275 | Acc: 54.494% (3383/6208)\n",
      "Loss: 1.276 | Acc: 54.448% (3415/6272)\n",
      "Loss: 1.274 | Acc: 54.482% (3452/6336)\n",
      "Loss: 1.275 | Acc: 54.375% (3480/6400)\n",
      "Loss: 1.277 | Acc: 54.270% (3508/6464)\n",
      "Loss: 1.276 | Acc: 54.320% (3546/6528)\n",
      "Loss: 1.277 | Acc: 54.308% (3580/6592)\n",
      "Loss: 1.276 | Acc: 54.312% (3615/6656)\n",
      "Loss: 1.277 | Acc: 54.241% (3645/6720)\n",
      "Loss: 1.277 | Acc: 54.260% (3681/6784)\n",
      "Loss: 1.275 | Acc: 54.322% (3720/6848)\n",
      "Loss: 1.278 | Acc: 54.225% (3748/6912)\n",
      "Loss: 1.280 | Acc: 54.114% (3775/6976)\n",
      "Loss: 1.282 | Acc: 54.048% (3805/7040)\n",
      "Loss: 1.280 | Acc: 54.110% (3844/7104)\n",
      "Loss: 1.282 | Acc: 54.032% (3873/7168)\n",
      "Loss: 1.284 | Acc: 53.982% (3904/7232)\n",
      "Loss: 1.283 | Acc: 53.975% (3938/7296)\n",
      "Loss: 1.281 | Acc: 54.062% (3979/7360)\n",
      "Loss: 1.282 | Acc: 53.987% (4008/7424)\n",
      "Loss: 1.281 | Acc: 53.953% (4040/7488)\n",
      "Loss: 1.282 | Acc: 53.906% (4071/7552)\n",
      "Loss: 1.284 | Acc: 53.782% (4096/7616)\n",
      "Loss: 1.283 | Acc: 53.841% (4135/7680)\n",
      "Loss: 1.281 | Acc: 53.887% (4173/7744)\n",
      "Loss: 1.280 | Acc: 53.932% (4211/7808)\n",
      "Loss: 1.280 | Acc: 53.836% (4238/7872)\n",
      "Loss: 1.280 | Acc: 53.868% (4275/7936)\n",
      "Loss: 1.280 | Acc: 53.913% (4313/8000)\n",
      "Loss: 1.281 | Acc: 53.881% (4345/8064)\n",
      "Loss: 1.282 | Acc: 53.839% (4376/8128)\n",
      "Loss: 1.281 | Acc: 53.870% (4413/8192)\n",
      "Loss: 1.282 | Acc: 53.840% (4445/8256)\n",
      "Loss: 1.285 | Acc: 53.762% (4473/8320)\n",
      "Loss: 1.284 | Acc: 53.805% (4511/8384)\n",
      "Loss: 1.285 | Acc: 53.717% (4538/8448)\n",
      "Loss: 1.286 | Acc: 53.759% (4576/8512)\n",
      "Loss: 1.285 | Acc: 53.766% (4611/8576)\n",
      "Loss: 1.286 | Acc: 53.738% (4643/8640)\n",
      "Loss: 1.287 | Acc: 53.699% (4674/8704)\n",
      "Loss: 1.287 | Acc: 53.718% (4710/8768)\n",
      "Loss: 1.285 | Acc: 53.770% (4749/8832)\n",
      "Loss: 1.285 | Acc: 53.777% (4784/8896)\n",
      "Loss: 1.285 | Acc: 53.739% (4815/8960)\n",
      "Loss: 1.285 | Acc: 53.746% (4850/9024)\n",
      "Loss: 1.286 | Acc: 53.686% (4879/9088)\n",
      "Loss: 1.286 | Acc: 53.660% (4911/9152)\n",
      "Loss: 1.284 | Acc: 53.743% (4953/9216)\n",
      "Loss: 1.283 | Acc: 53.772% (4990/9280)\n",
      "Loss: 1.284 | Acc: 53.756% (5023/9344)\n",
      "Loss: 1.284 | Acc: 53.773% (5059/9408)\n",
      "Loss: 1.285 | Acc: 53.811% (5097/9472)\n",
      "Loss: 1.284 | Acc: 53.891% (5139/9536)\n",
      "Loss: 1.283 | Acc: 53.969% (5181/9600)\n",
      "Loss: 1.283 | Acc: 53.974% (5216/9664)\n",
      "Loss: 1.284 | Acc: 53.937% (5247/9728)\n",
      "Loss: 1.285 | Acc: 53.871% (5275/9792)\n",
      "Loss: 1.284 | Acc: 53.886% (5311/9856)\n",
      "Loss: 1.286 | Acc: 53.821% (5339/9920)\n",
      "Loss: 1.286 | Acc: 53.776% (5369/9984)\n",
      "Loss: 1.286 | Acc: 53.750% (5375/10000)\n",
      "Evaluation of Epoch 2 is completed, Test accuracy for this epoch is 53.75\n",
      "\n",
      "Final train set accuracy is 54.875510204081635\n",
      "Final test set accuracy is 53.75\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "input_dims = 3\n",
    "hidden_dims = 128\n",
    "output_dims=10\n",
    "num_trans_layers = 4\n",
    "num_heads=4\n",
    "image_k=32\n",
    "patch_k=4\n",
    "\n",
    "network = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "network = ViT(hidden_dims, input_dims, output_dims, num_trans_layers, num_heads, image_k, patch_k)\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "tr_accs=[]\n",
    "test_accs=[]\n",
    "for epoch in range(3):\n",
    "    tr_acc = train(network, optimizer, loader_train)\n",
    "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
    "              .format(epoch, tr_acc))  \n",
    "    \n",
    "    test_acc = evaluate(network, loader_test)\n",
    "    print('Evaluation of Epoch {} is completed, Test accuracy for this epoch is {}'\\\n",
    "              .format(epoch, test_acc))  \n",
    "    \n",
    "    tr_accs.append(tr_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
    "print(\"Final test set accuracy is {}\".format(test_accs[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
